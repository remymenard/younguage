---
- :url: https://netflixtechblog.com/notebook-innovation-591ee3221233?source=search_post---------0
  :title: 'Beyond Interactive: Notebook Innovation at Netflix'
  :content: '<p>By Michelle Ufford, M Pacer, Matthew Seal, and Kyle Kelley</p><p>Notebooks
    have rapidly grown in popularity among data scientists to become the de facto
    standard for quick prototyping and exploratory analysis. At Netflix, we’re pushing
    the boundaries even further, reimagining what a notebook can be, who can use it,
    and what they can do with it. And we’re making big investments to help make this
    vision a reality.</p><p>In this post, we’ll share our motivations and why we find
    Jupyter notebooks so compelling. We’ll also introduce components of our notebook
    infrastructure and explore some of the novel ways we’re using notebooks at Netflix.</p><p>If
    you’re short on time, we suggest jumping down to the Use Cases section.</p><h2>Motivations</h2><p>Data
    powers Netflix. It permeates our thoughts, informs our <a href="https://medium.com/netflix-techblog/its-all-a-bout-testing-the-netflix-experimentation-platform-4e1ca458c15">decisions</a>,
    and challenges our <a href="https://medium.com/netflix-techblog/selecting-the-best-artwork-for-videos-through-a-b-testing-f6155c4595f6">assumptions</a>.
    It fuels <a href="https://medium.com/netflix-techblog/a-b-testing-and-beyond-improving-the-netflix-streaming-experience-with-experimentation-and-data-5b0ae9295bdf">experimentation</a>
    and <a href="https://medium.com/netflix-techblog/growth-engineering-at-netflix-accelerating-innovation-90eb8e70ce59">innovation</a>
    at <a href="https://www.popularmechanics.com/technology/a20138/netflix-interview-wired-traffic-exceeds-internet-capacity/">unprecedented
    scale</a>. Data helps us discover <a href="https://medium.com/netflix-techblog/studio-production-data-science-646ee2cc21a1">fantastic
    content</a> and deliver <a href="https://medium.com/netflix-techblog/artwork-personalization-c589f074ad76">personalized
    experiences</a> for our 130 million members <a href="https://medium.com/netflix-techblog/using-machine-learning-to-improve-streaming-quality-at-netflix-9651263ef09f">around
    the world</a>.</p><p>Making this possible is no small feat; it requires extensive
    engineering and infrastructure support. Every day more than 1 trillion events
    are written into a streaming ingestion pipeline, which is processed and written
    to a 100PB cloud-native data warehouse. And every day, our users run more than
    150,000 jobs against this data, spanning everything from reporting and analysis
    to machine learning and recommendation algorithms. To support these use cases
    at such scale, we’ve built an industry-leading Data Platform which is flexible,
    powerful, and complex (by necessity). We’ve also built a rich ecosystem of complementary
    tools and services, such as <a href="https://medium.com/netflix-techblog/evolving-the-netflix-data-platform-with-genie-3-598021604dda">Genie</a>,
    a federated job execution service, and <a href="https://medium.com/netflix-techblog/metacat-making-big-data-discoverable-and-meaningful-at-netflix-56fb36a53520">Metacat</a>,
    a federated metastore. These tools simplify the complexity, making it possible
    to support a broader set of users across the company.</p><p><img src="https://cdn-images-1.medium.com/max/3122/1*Yzh8GbLOuY3-_rh2bMu9Bw.jpeg"
    alt=""></p><p>User diversity is exciting, but it comes at a cost: the Netflix
    Data Platform — and its ecosystem of tools and services — must scale to support
    additional use cases, languages, access patterns, and more. To better understand
    this problem, consider 3 common roles: analytics engineer, data engineer, and
    data scientist.</p><p><img src="https://cdn-images-1.medium.com/max/3234/1*NRoFl1l4lIVQAAvmBOKd4A.jpeg"
    alt="Example of how tooling &amp; language preferences may vary across roles"><em>Example
    of how tooling &amp; language preferences may vary across roles</em></p><p>Generally,
    each role relies on a different set of tools and languages. For example, a data
    engineer might create a new aggregate of a dataset containing trillions of streaming
    events — using Scala in IntelliJ. An analytics engineer might use that aggregate
    in a new report on global streaming quality — using SQL and Tableau. And that
    report might lead to a data scientist building a new streaming compression model
    — using R and RStudio. On the surface, these seem like disparate, albeit complementary,
    workflows. But if we delve deeper, we see that each of these workflows has multiple
    overlapping tasks:</p><p>*<em>data exploration — *</em>occurs early in a project;
    may include viewing sample data, running queries for statistical profiling and
    exploratory analysis, and visualizing data</p><p>*<em>data preparation *</em>—
    iterative task; may include cleaning, standardizing, transforming, denormalizing,
    and aggregating data; typically the most time-intensive task of a project</p><p>*<em>data
    validation *</em>— recurring task; may include viewing sample data, running queries
    for statistical profiling and aggregate analysis, and visualizing data; typically
    occurs as part of data exploration, data preparation, development, pre-deployment,
    and post-deployment phases</p><p><strong>productionalization</strong> — occurs
    late in a project; may include deploying code to production, backfilling datasets,
    training models, validating data, and scheduling workflows</p><p>To help our users
    scale, we want to make these tasks as effortless as possible. To help our platform
    scale, we want to minimize the number of tools we need to support. But how? No
    single tool could span all of these tasks; what’s more, a single task often requires
    multiple tools. When we add another layer of abstraction, however, a common pattern
    emerges across tools and languages: run code, explore data, present results.</p><p>As
    it happens, an open source project was designed to do precisely that: <a href="http://jupyter.org/">Project
    Jupyter</a>.</p><h2>Jupyter Notebooks</h2><p><img src="https://cdn-images-1.medium.com/max/2984/0*cgQX-JHRytjbeDNx"
    alt="Jupyter notebook rendered in nteract desktop featuring [Vega](https://vega.github.io/)
    and [Altair](https://altair-viz.github.io/)"><em>Jupyter notebook rendered in
    nteract desktop featuring <a href="https://vega.github.io/">Vega</a> and <a href="https://altair-viz.github.io/">Altair</a></em></p><p>Project
    Jupyter began in 2014 with a goal of creating a consistent set of open-source
    tools for scientific research, reproducible workflows, computational narratives,
    and data analytics. Those tools translated well to industry, and today Jupyter
    notebooks have become an essential part of the data scientist toolkit. To give
    you a sense of its impact, Jupyter was awarded the <a href="https://blog.jupyter.org/jupyter-receives-the-acm-software-system-award-d433b0dfe3a2">2017
    ACM Software Systems Award</a> — a prestigious honor it shares with Java, Unix,
    and the Web.</p><p>To understand why the Jupyter notebook is so compelling for
    us, consider the core functionality it provides:</p><ul><li><p>a messaging protocol
    for introspecting and executing code which is language agnostic</p></li><li><p>an
    editable file format for describing and capturing code, code output, and markdown
    notes</p></li><li><p>a web-based UI for interactively writing and running code
    as well as visualizing outputs</p></li></ul><p>The Jupyter protocol provides a
    standard messaging API to communicate with kernels that act as computational engines.
    The protocol enables a composable architecture that separates where content is
    written (the UI) and where code is executed (the kernel). By isolating the runtime
    from the interface, notebooks can span multiple languages while maintaining flexibility
    in how the execution environment is configured. If a kernel exists for a language
    that knows how to communicate using the Jupyter protocol, notebooks can run code
    by sending messages back and forth with that kernel.</p><p>Backing all this is
    a file format that stores both code and results together. This means results can
    be accessed later without needing to rerun the code. In addition, the notebook
    stores rich prose to give context to what’s happening within the notebook. This
    makes it an ideal format for communicating business context, documenting assumptions,
    annotating code, describing conclusions, and more.</p><h2>Use Cases</h2><p>Of
    our many use cases, the most common ways we’re using notebooks today are: data
    access, notebook templates, and scheduling notebooks.</p><h3><strong>Data Access</strong></h3><p>Notebooks
    were first introduced at Netflix to support data science workflows. As their adoption
    grew among data scientists, we saw an opportunity to scale our tooling efforts.
    We realized we could leverage the versatility and architecture of Jupyter notebooks
    and extend it for general data access. In Q3 2017 we began this work in earnest,
    elevating notebooks from a niche tool to a first-class citizen of the Netflix
    Data Platform.</p><p>From our users’ perspective, notebooks offer a convenient
    interface for iteratively running code, exploring output, and visualizing data
    — all from a single cloud-based development environment. We also maintain a Python
    library that consolidates access to platform APIs. This means users have programmatic
    access to virtually the entire platform from within a notebook. Because of this
    combination of versatility, power, and ease of use, we’ve seen rapid organic adoption
    for all user types across our entire platform.</p><p>Today, notebooks are the
    most popular tool for working with data at Netflix.</p><h3><strong>Notebook Templates</strong></h3><p>As
    we expanded platform support for notebooks, we began to introduce new capabilities
    to meet new use cases. From this work emerged parameterized notebooks. A parameterized
    notebook is exactly what it sounds like: a notebook which allows you to specify
    parameters in your code and accept input values at runtime. This provides an excellent
    mechanism for users to define notebooks as reusable templates.</p><p>Our users
    have found a surprising number of uses for these templates. Some of the most common
    ones are:</p><ul><li><p>*<em>Data Scientist: *</em>run an experiment with different
    coefficients and summarize the results</p></li><li><p><strong>Data Engineer:</strong>
    execute a collection of data quality audits as part of the deployment process</p></li><li><p><strong>Data
    Analyst:</strong> share prepared queries and visualizations to enable a stakeholder
    to explore more deeply than Tableau allows</p></li><li><p><strong>Software Engineer:</strong>
    email the results of a troubleshooting script each time there’s a failure</p></li></ul><h3><strong>Scheduling
    Notebooks</strong></h3><p>One of the more novel ways we’re leveraging notebooks
    is as a unifying layer for scheduling workflows.</p><p>Since each notebook can
    run against an arbitrary kernel, we can support any execution environment a user
    has defined. And because notebooks describe a linear flow of execution, broken
    up by cells, we can map failure to particular cells. This allows users to describe
    a short narrative of execution and visualizations that we can accurately report
    against when running at a later point in time.</p><p>This paradigm means we can
    use notebooks for interactive work and smoothly move to scheduling that work to
    run recurrently. For users, this is very convenient. Many users construct an entire
    workflow in a notebook, only to have to copy/paste it into separate files for
    scheduling when they’re ready to deploy it. By treating notebooks as a logical
    workflow, we can easily schedule it the same as any other workflow.</p><p>We can
    schedule other types of work through notebooks, too. When a Spark or Presto job
    executes from the scheduler, the source code is injected into a newly-created
    notebook and executed. That notebook then becomes an immutable historical record,
    containing all related artifacts — including source code, parameters, runtime
    config, execution logs, error messages, and so on. When troubleshooting failures,
    this offers a quick entry point for investigation, as all relevant information
    is colocated and the notebook can be launched for interactive debugging.</p><h2>Notebook
    Infrastructure</h2><p>Supporting these use cases at Netflix scale requires extensive
    supporting infrastructure. Let’s briefly introduce some of the projects we’ll
    be talking about.</p><p><a href="https://github.com/nteract">**nteract</a>** is
    a next-gen React-based UI for Jupyter notebooks. It provides a simple, intuitive
    interface and offers several improvements over the classic Jupyter UI, such as
    inline cell toolbars, drag and droppable cells, and a built-in data explorer.</p><p><a
    href="https://github.com/nteract/papermill">**Papermill</a>** is a library for
    parameterizing, executing, and analyzing Jupyter notebooks. With it, you can spawn
    multiple notebooks with different parameter sets and execute them concurrently.
    Papermill can also help collect and summarize metrics from a collection of notebooks.</p><p><a
    href="https://github.com/nteract/nteract/blob/master/applications/commuter/README.md">**Commuter</a>**
    is a lightweight, vertically-scalable service for viewing and sharing notebooks.
    It provides a Jupyter-compatible version of the contents API and makes it trivial
    to read notebooks stored locally or on Amazon S3. It also offers a directory explorer
    for finding and sharing notebooks.</p><p><a href="https://netflix.github.io/titus/">**Titus</a>**
    is a container management platform that provides scalable and reliable container
    execution and cloud-native integration with Amazon AWS. Titus was built internally
    at Netflix and is used in production to power Netflix streaming, recommendation,
    and content systems.</p><p>We explore this architecture in our follow-up blog
    post, <a href="https://medium.com/@NetflixTechBlog/scheduling-notebooks-348e6c14cfd6">Scheduling
    Notebooks at Netflix</a>. For the purposes of this post, we’ll just introduce
    three of its fundamental components: storage, compute, and interface.</p><p><img
    src="https://cdn-images-1.medium.com/max/3840/1*WOEEJizYnO8ibtU2l9jWbA.jpeg" alt="Notebook
    Infrastructure at Netflix"><em>Notebook Infrastructure at Netflix</em></p><h3><strong>Storage</strong></h3><p>The
    Netflix Data Platform relies on Amazon S3 and EFS for cloud storage, which notebooks
    treat as virtual filesystems. This means each user has a home directory on EFS,
    which contains a personal workspace for notebooks. This workspace is where we
    store any notebook created or uploaded by a user. This is also where all reading
    and writing activity occurs when a user launches a notebook interactively. We
    rely on a combination of [workspace + filename] to form the notebook’s <em>namespace</em>,
    e.g. /efs/users/kylek/notebooks/MySparkJob.ipynb. We use this namespace for viewing,
    sharing, and scheduling notebooks. This convention prevents collisions and makes
    it easy to identify both the user and the location of the notebook in the EFS
    volume.</p><p>We can rely on the workspace path to abstract away the complexity
    of cloud-based storage from users. For example, only the filename of a notebook
    is displayed in directory listings, e.g. MySparkJob.ipynb. This same file is accessible
    at ~/notebooks/MySparkJob.ipynb from a terminal.</p><p><img src="https://cdn-images-1.medium.com/max/2000/1*pjjOIx1g7g4XoTxWBv0pIw.png"
    alt="Notebook storage vs. notebook access"><em>Notebook storage vs. notebook access</em></p><p>When
    the user schedules a notebook, the scheduler copies the user’s notebook from EFS
    to a common directory on S3. The notebook on S3 becomes the source of truth for
    the scheduler, or <em>source notebook</em>. Each time the scheduler runs a notebook,
    it instantiates a new notebook from the source notebook. This new notebook is
    what actually executes and becomes an immutable record of that execution, containing
    the code, output, and logs from each cell. We refer to this as the <em>output
    notebook</em>.</p><p>Collaboration is fundamental to how we work at Netflix. It
    came as no surprise then when users started sharing notebook URLs. As this practice
    grew, we ran into frequent problems with accidental overwrites caused by multiple
    people concurrently accessing the same notebook . Our users wanted a way to share
    their active notebook in a read-only state. This led to the creation of <a href="https://github.com/nteract/nteract/tree/master/applications/commuter">**Commuter</a>**.
    Behind the scenes, Commuter surfaces the Jupyter APIs for /files and /api/contents
    to list directories, view file contents, and access file metadata. This means
    users can safely view notebooks without affecting production jobs or live-running
    notebooks.</p><h3><strong>Compute</strong></h3><p>Managing compute resources is
    one of the most challenging parts of working with data. This is especially true
    at Netflix, where we employ a highly-scalable containerized architecture on AWS.
    All jobs on the Data Platform run on containers — including queries, pipelines,
    and notebooks. Naturally, we wanted to abstract away as much of this complexity
    as possible.</p><p>A container is provisioned when a user launches a notebook
    server. We provide reasonable defaults for container resources, which works for
    ~87.3% of execution patterns. When that’s not enough, users can request more resources
    using a simple interface.</p><p><img src="https://cdn-images-1.medium.com/max/2360/0*FUdkFk_m2tyeloJa"
    alt="Users can select as much or as little compute + memory as they need"><em>Users
    can select as much or as little compute + memory as they need</em></p><p>We also
    provide a unified execution environment with a prepared container image. The image
    has common libraries and an array of default kernels preinstalled. Not everything
    in the image is static — our kernels pull the most recent versions of Spark and
    the latest cluster configurations for our platform. This reduces the friction
    and setup time for new notebooks and generally keeps us to a single execution
    environment.</p><p>Under the hood we’re managing the orchestration and environments
    with <a href="https://medium.com/netflix-techblog/titus-the-netflix-container-management-platform-is-now-open-source-f868c9fb5436">Titus</a>,
    our Docker container management service. We further wrap that service by managing
    the user’s particular server configuration and image. The image also includes
    user security groups and roles, as well as common environment variables for identity
    within included libraries. This means our users can spend less time on infrastructure
    and more time on data.</p><h3><strong>Interface</strong></h3><p>Earlier we described
    our vision for notebooks to become the tool of choice for working with data. But
    this presents an interesting challenge: how can a single interface support all
    users? We don’t fully know the answer yet, but we have some ideas.</p><p>We know
    we want to lean into simplicity. This means an intuitive UI with a minimalistic
    aesthetic, and it also requires a thoughtful UX that makes it easy to do the hard
    things. This philosophy aligns well with the goals of <a href="https://github.com/nteract/nteract">**nteract</a>**,
    a React-based frontend for Jupyter notebooks. It emphasizes simplicity and <a
    href="https://components.nteract.io/">composability</a> as core design principles,
    which makes it an ideal building block for the work we want to do.</p><p>One of
    the most frequent complaints we heard from users is the lack of native data visualization
    across language boundaries, especially for non-Python languages. nteract’s <a
    href="https://blog.nteract.io/designing-the-nteract-data-explorer-f4476d53f897">Data
    Explorer</a> is a good example of how we can make the hard things simpler by providing
    a language-agnostic way to explore data quickly.</p><p>You can see Data Explorer
    in action in this <a href="https://mybinder.org/v2/gh/nteract/examples/master?urlpath=%2Fnteract%2Fedit%2Fpython%2Fhappiness.ipynb">sample
    notebook</a> on MyBinder. <em>(please note: it may take a minute to load)</em></p><p><img
    src="https://cdn-images-1.medium.com/max/5652/1*uW5qwTm4FQ3OauOWu_MCcA.gif" alt="Visualizing
    the World Happiness Report dataset with nteract’s Data Explorer"><em>Visualizing
    the World Happiness Report dataset with nteract’s Data Explorer</em></p><p>We’re
    also introducing native support for parametrization, which makes it easier to
    schedule notebooks and create reusable templates.</p><p><img src="https://cdn-images-1.medium.com/max/3838/1*UjwesaxwjH3d1glkP41J4w.gif"
    alt="Native support for parameterized notebooks in nteract"><em>Native support
    for parameterized notebooks in nteract</em></p><p>Although notebooks are already
    offering a lot of value at Netflix, we’ve just begun. We know we need to <a href="https://jobs.netflix.com/search?q=notebooks">make
    investments</a> in both the frontend and backend to improve the overall notebook
    experience. Our work over the next 12 months is focused on improving reliability,
    visibility, and collaboration. Context is paramount for users, which is why we’re
    increasing visibility into cluster status, kernel state, job history, and more.
    We’re also working on automatic version control, native in-app scheduling, better
    support for visualizing Spark DataFrames, and greater stability for our Scala
    kernel. We’ll go into more detail on this work in a future blog post.</p><h2>Open
    Source Projects</h2><p>Netflix has long been a proponent of open source. We value
    the energy, open standards, and exchange of ideas that emerge from open source
    collaborations. Many of the applications we developed for the Netflix Data Platform
    have already been open sourced through <a href="https://netflix.github.io/">Netflix
    OSS</a>. We are also intentional about not creating one-off solutions or succumbing
    to “Not Invented Here” mentality. Whenever possible, we leverage and contribute
    to existing open source projects, such as <a href="https://github.com/apache/spark">Spark</a>,
    <a href="https://github.com/jupyter">Jupyter</a>, and <a href="https://github.com/pandas-dev/pandas">pandas</a>.</p><p>The
    infrastructure we’ve described relies heavily on the Project Jupyter ecosystem,
    but there are some places where we diverge. Most notably, we have chosen <a href="https://github.com/nteract">**nteract</a>**
    as the notebook UI for Netflix. We made this decision for many reasons, including
    alignment with our technology stack and design philosophies. As we push the limits
    of what a notebook can do, we will likely create new tools, libraries, and services.
    These projects will also be open sourced as part of the nteract ecosystem.</p><p>We
    recognize that what makes sense for Netflix does not necessarily make sense for
    everyone. We have designed these projects with modularity in mind. This makes
    it possible to pick and choose only the components that make sense for your environment,
    e.g. Papermill, without requiring a commitment to the entire ecosystem.</p><h2>What’s
    Next</h2><p>As a platform team, our responsibility is to enable Netflixers to
    do amazing things with data. Notebooks are already having a dramatic impact at
    Netflix. With the significant investments we’re making in this space, we’re excited
    to see this impact grow. If you’d like to be a part of it, check out <a href="https://jobs.netflix.com/search?q=notebooks">our
    job openings</a>.</p><p>Phew! Thanks for sticking with us through this long post.
    We’ve just scratched the surface of what we’re doing with notebooks. This post
    is part one in a series on notebooks at Netflix we’ll be releasing over the coming
    weeks. You can follow us on Medium for more from Netflix and check out the currently
    released articles below:</p><ul><li><p>Part I: <a href="https://medium.com/netflix-techblog/notebook-innovation-591ee3221233">Notebook
    Innovation</a> (this post)</p></li><li><p>Part II: <a href="https://medium.com/@NetflixTechBlog/scheduling-notebooks-348e6c14cfd6">Scheduling
    Notebooks</a></p></li></ul><p>We’re thrilled to sponsor this year’s <a href="https://conferences.oreilly.com/jupyter/jup-ny">JupyterCon</a>.
    If you’re attending, check out one of the 5 talks by our engineers, or swing by
    our booth to talk about Jupyter, nteract, or data with us.</p><ul><li><p>8/22
    1:30 PM — <a href="https://conferences.oreilly.com/jupyter/jup-ny/public/schedule/detail/68413">How
    to Build on top of Jupyter’s Protocols</a>, <em>Kyle Kelley</em></p></li><li><p>8/23
    1:50 PM — <a href="https://conferences.oreilly.com/jupyter/jup-ny/public/schedule/detail/68348">Scheduled
    Notebooks: Manageable and traceable code execution</a>, <em>Matthew Seal</em></p></li><li><p>8/23
    2:40 PM — <a href="https://conferences.oreilly.com/jupyter/jup-ny/public/schedule/detail/71601">Notebooks
    @ Netflix: From Analytics to Engineering</a>, <em>Michelle Ufford, Kyle Kelley</em></p></li><li><p>8/23
    5:00 PM — <a href="https://conferences.oreilly.com/jupyter/jup-ny/public/schedule/detail/71220">Making
    beautiful objects with Jupyter</a>, <em>M Pacer</em></p></li><li><p>8/24 2:40
    PM — <a href="https://conferences.oreilly.com/jupyter/jup-ny/public/schedule/detail/71220">Jupyter’s
    configuration system</a>, <em>M Pacer</em> et. al.</p></li><li><p>8/25 9AM — 5PM
    <a href="https://blog.jupyter.org/jupytercon-2018-registration-open-3b52abba9cce#a707">JupyterCon
    Community Sprint Day</a></p></li></ul><p>There are more ways to learn from Netflix
    Data and we’re happy to share:</p><ul><li><p><a href="http://twitter.com/NetflixData">@NetflixData</a>
    on Twitter</p></li><li><p><a href="https://www.youtube.com/channel/UC00QATOrSH4K2uOljTnnaKw/featured">Netflix
    Data</a> talks on YouTube</p></li><li><p><a href="https://research.netflix.com/">Netflix
    Research</a> website</p></li></ul><p>You can also stay up to date with nteract
    via their <a href="http://bit.ly/nteract-monthly">mailing list</a> and <a href="https://blog.nteract.io/">blog</a>!</p>'
  :author: '<p>By Michelle Ufford, M Pacer, Matthew Seal, and Kyle Kelley</p><p>Notebooks
    have rapidly grown in popularity among data scientists to become the de facto
    standard for quick prototyping and exploratory analysis. At Netflix, we’re pushing
    the boundaries even further, reimagining what a notebook can be, who can use it,
    and what they can do with it. And we’re making big investments to help make this
    vision a reality.</p><p>In this post, we’ll share our motivations and why we find
    Jupyter notebooks so compelling. We’ll also introduce components of our notebook
    infrastructure and explore some of the novel ways we’re using notebooks at Netflix.</p><p>If
    you’re short on time, we suggest jumping down to the Use Cases section.</p><h2>Motivations</h2><p>Data
    powers Netflix. It permeates our thoughts, informs our <a href="https://medium.com/netflix-techblog/its-all-a-bout-testing-the-netflix-experimentation-platform-4e1ca458c15">decisions</a>,
    and challenges our <a href="https://medium.com/netflix-techblog/selecting-the-best-artwork-for-videos-through-a-b-testing-f6155c4595f6">assumptions</a>.
    It fuels <a href="https://medium.com/netflix-techblog/a-b-testing-and-beyond-improving-the-netflix-streaming-experience-with-experimentation-and-data-5b0ae9295bdf">experimentation</a>
    and <a href="https://medium.com/netflix-techblog/growth-engineering-at-netflix-accelerating-innovation-90eb8e70ce59">innovation</a>
    at <a href="https://www.popularmechanics.com/technology/a20138/netflix-interview-wired-traffic-exceeds-internet-capacity/">unprecedented
    scale</a>. Data helps us discover <a href="https://medium.com/netflix-techblog/studio-production-data-science-646ee2cc21a1">fantastic
    content</a> and deliver <a href="https://medium.com/netflix-techblog/artwork-personalization-c589f074ad76">personalized
    experiences</a> for our 130 million members <a href="https://medium.com/netflix-techblog/using-machine-learning-to-improve-streaming-quality-at-netflix-9651263ef09f">around
    the world</a>.</p><p>Making this possible is no small feat; it requires extensive
    engineering and infrastructure support. Every day more than 1 trillion events
    are written into a streaming ingestion pipeline, which is processed and written
    to a 100PB cloud-native data warehouse. And every day, our users run more than
    150,000 jobs against this data, spanning everything from reporting and analysis
    to machine learning and recommendation algorithms. To support these use cases
    at such scale, we’ve built an industry-leading Data Platform which is flexible,
    powerful, and complex (by necessity). We’ve also built a rich ecosystem of complementary
    tools and services, such as <a href="https://medium.com/netflix-techblog/evolving-the-netflix-data-platform-with-genie-3-598021604dda">Genie</a>,
    a federated job execution service, and <a href="https://medium.com/netflix-techblog/metacat-making-big-data-discoverable-and-meaningful-at-netflix-56fb36a53520">Metacat</a>,
    a federated metastore. These tools simplify the complexity, making it possible
    to support a broader set of users across the company.</p><p><img src="https://cdn-images-1.medium.com/max/3122/1*Yzh8GbLOuY3-_rh2bMu9Bw.jpeg"
    alt=""></p><p>User diversity is exciting, but it comes at a cost: the Netflix
    Data Platform — and its ecosystem of tools and services — must scale to support
    additional use cases, languages, access patterns, and more. To better understand
    this problem, consider 3 common roles: analytics engineer, data engineer, and
    data scientist.</p><p><img src="https://cdn-images-1.medium.com/max/3234/1*NRoFl1l4lIVQAAvmBOKd4A.jpeg"
    alt="Example of how tooling &amp; language preferences may vary across roles"><em>Example
    of how tooling &amp; language preferences may vary across roles</em></p><p>Generally,
    each role relies on a different set of tools and languages. For example, a data
    engineer might create a new aggregate of a dataset containing trillions of streaming
    events — using Scala in IntelliJ. An analytics engineer might use that aggregate
    in a new report on global streaming quality — using SQL and Tableau. And that
    report might lead to a data scientist building a new streaming compression model
    — using R and RStudio. On the surface, these seem like disparate, albeit complementary,
    workflows. But if we delve deeper, we see that each of these workflows has multiple
    overlapping tasks:</p><p>*<em>data exploration — *</em>occurs early in a project;
    may include viewing sample data, running queries for statistical profiling and
    exploratory analysis, and visualizing data</p><p>*<em>data preparation *</em>—
    iterative task; may include cleaning, standardizing, transforming, denormalizing,
    and aggregating data; typically the most time-intensive task of a project</p><p>*<em>data
    validation *</em>— recurring task; may include viewing sample data, running queries
    for statistical profiling and aggregate analysis, and visualizing data; typically
    occurs as part of data exploration, data preparation, development, pre-deployment,
    and post-deployment phases</p><p><strong>productionalization</strong> — occurs
    late in a project; may include deploying code to production, backfilling datasets,
    training models, validating data, and scheduling workflows</p><p>To help our users
    scale, we want to make these tasks as effortless as possible. To help our platform
    scale, we want to minimize the number of tools we need to support. But how? No
    single tool could span all of these tasks; what’s more, a single task often requires
    multiple tools. When we add another layer of abstraction, however, a common pattern
    emerges across tools and languages: run code, explore data, present results.</p><p>As
    it happens, an open source project was designed to do precisely that: <a href="http://jupyter.org/">Project
    Jupyter</a>.</p><h2>Jupyter Notebooks</h2><p><img src="https://cdn-images-1.medium.com/max/2984/0*cgQX-JHRytjbeDNx"
    alt="Jupyter notebook rendered in nteract desktop featuring [Vega](https://vega.github.io/)
    and [Altair](https://altair-viz.github.io/)"><em>Jupyter notebook rendered in
    nteract desktop featuring <a href="https://vega.github.io/">Vega</a> and <a href="https://altair-viz.github.io/">Altair</a></em></p><p>Project
    Jupyter began in 2014 with a goal of creating a consistent set of open-source
    tools for scientific research, reproducible workflows, computational narratives,
    and data analytics. Those tools translated well to industry, and today Jupyter
    notebooks have become an essential part of the data scientist toolkit. To give
    you a sense of its impact, Jupyter was awarded the <a href="https://blog.jupyter.org/jupyter-receives-the-acm-software-system-award-d433b0dfe3a2">2017
    ACM Software Systems Award</a> — a prestigious honor it shares with Java, Unix,
    and the Web.</p><p>To understand why the Jupyter notebook is so compelling for
    us, consider the core functionality it provides:</p><ul><li><p>a messaging protocol
    for introspecting and executing code which is language agnostic</p></li><li><p>an
    editable file format for describing and capturing code, code output, and markdown
    notes</p></li><li><p>a web-based UI for interactively writing and running code
    as well as visualizing outputs</p></li></ul><p>The Jupyter protocol provides a
    standard messaging API to communicate with kernels that act as computational engines.
    The protocol enables a composable architecture that separates where content is
    written (the UI) and where code is executed (the kernel). By isolating the runtime
    from the interface, notebooks can span multiple languages while maintaining flexibility
    in how the execution environment is configured. If a kernel exists for a language
    that knows how to communicate using the Jupyter protocol, notebooks can run code
    by sending messages back and forth with that kernel.</p><p>Backing all this is
    a file format that stores both code and results together. This means results can
    be accessed later without needing to rerun the code. In addition, the notebook
    stores rich prose to give context to what’s happening within the notebook. This
    makes it an ideal format for communicating business context, documenting assumptions,
    annotating code, describing conclusions, and more.</p><h2>Use Cases</h2><p>Of
    our many use cases, the most common ways we’re using notebooks today are: data
    access, notebook templates, and scheduling notebooks.</p><h3><strong>Data Access</strong></h3><p>Notebooks
    were first introduced at Netflix to support data science workflows. As their adoption
    grew among data scientists, we saw an opportunity to scale our tooling efforts.
    We realized we could leverage the versatility and architecture of Jupyter notebooks
    and extend it for general data access. In Q3 2017 we began this work in earnest,
    elevating notebooks from a niche tool to a first-class citizen of the Netflix
    Data Platform.</p><p>From our users’ perspective, notebooks offer a convenient
    interface for iteratively running code, exploring output, and visualizing data
    — all from a single cloud-based development environment. We also maintain a Python
    library that consolidates access to platform APIs. This means users have programmatic
    access to virtually the entire platform from within a notebook. Because of this
    combination of versatility, power, and ease of use, we’ve seen rapid organic adoption
    for all user types across our entire platform.</p><p>Today, notebooks are the
    most popular tool for working with data at Netflix.</p><h3><strong>Notebook Templates</strong></h3><p>As
    we expanded platform support for notebooks, we began to introduce new capabilities
    to meet new use cases. From this work emerged parameterized notebooks. A parameterized
    notebook is exactly what it sounds like: a notebook which allows you to specify
    parameters in your code and accept input values at runtime. This provides an excellent
    mechanism for users to define notebooks as reusable templates.</p><p>Our users
    have found a surprising number of uses for these templates. Some of the most common
    ones are:</p><ul><li><p>*<em>Data Scientist: *</em>run an experiment with different
    coefficients and summarize the results</p></li><li><p><strong>Data Engineer:</strong>
    execute a collection of data quality audits as part of the deployment process</p></li><li><p><strong>Data
    Analyst:</strong> share prepared queries and visualizations to enable a stakeholder
    to explore more deeply than Tableau allows</p></li><li><p><strong>Software Engineer:</strong>
    email the results of a troubleshooting script each time there’s a failure</p></li></ul><h3><strong>Scheduling
    Notebooks</strong></h3><p>One of the more novel ways we’re leveraging notebooks
    is as a unifying layer for scheduling workflows.</p><p>Since each notebook can
    run against an arbitrary kernel, we can support any execution environment a user
    has defined. And because notebooks describe a linear flow of execution, broken
    up by cells, we can map failure to particular cells. This allows users to describe
    a short narrative of execution and visualizations that we can accurately report
    against when running at a later point in time.</p><p>This paradigm means we can
    use notebooks for interactive work and smoothly move to scheduling that work to
    run recurrently. For users, this is very convenient. Many users construct an entire
    workflow in a notebook, only to have to copy/paste it into separate files for
    scheduling when they’re ready to deploy it. By treating notebooks as a logical
    workflow, we can easily schedule it the same as any other workflow.</p><p>We can
    schedule other types of work through notebooks, too. When a Spark or Presto job
    executes from the scheduler, the source code is injected into a newly-created
    notebook and executed. That notebook then becomes an immutable historical record,
    containing all related artifacts — including source code, parameters, runtime
    config, execution logs, error messages, and so on. When troubleshooting failures,
    this offers a quick entry point for investigation, as all relevant information
    is colocated and the notebook can be launched for interactive debugging.</p><h2>Notebook
    Infrastructure</h2><p>Supporting these use cases at Netflix scale requires extensive
    supporting infrastructure. Let’s briefly introduce some of the projects we’ll
    be talking about.</p><p><a href="https://github.com/nteract">**nteract</a>** is
    a next-gen React-based UI for Jupyter notebooks. It provides a simple, intuitive
    interface and offers several improvements over the classic Jupyter UI, such as
    inline cell toolbars, drag and droppable cells, and a built-in data explorer.</p><p><a
    href="https://github.com/nteract/papermill">**Papermill</a>** is a library for
    parameterizing, executing, and analyzing Jupyter notebooks. With it, you can spawn
    multiple notebooks with different parameter sets and execute them concurrently.
    Papermill can also help collect and summarize metrics from a collection of notebooks.</p><p><a
    href="https://github.com/nteract/nteract/blob/master/applications/commuter/README.md">**Commuter</a>**
    is a lightweight, vertically-scalable service for viewing and sharing notebooks.
    It provides a Jupyter-compatible version of the contents API and makes it trivial
    to read notebooks stored locally or on Amazon S3. It also offers a directory explorer
    for finding and sharing notebooks.</p><p><a href="https://netflix.github.io/titus/">**Titus</a>**
    is a container management platform that provides scalable and reliable container
    execution and cloud-native integration with Amazon AWS. Titus was built internally
    at Netflix and is used in production to power Netflix streaming, recommendation,
    and content systems.</p><p>We explore this architecture in our follow-up blog
    post, <a href="https://medium.com/@NetflixTechBlog/scheduling-notebooks-348e6c14cfd6">Scheduling
    Notebooks at Netflix</a>. For the purposes of this post, we’ll just introduce
    three of its fundamental components: storage, compute, and interface.</p><p><img
    src="https://cdn-images-1.medium.com/max/3840/1*WOEEJizYnO8ibtU2l9jWbA.jpeg" alt="Notebook
    Infrastructure at Netflix"><em>Notebook Infrastructure at Netflix</em></p><h3><strong>Storage</strong></h3><p>The
    Netflix Data Platform relies on Amazon S3 and EFS for cloud storage, which notebooks
    treat as virtual filesystems. This means each user has a home directory on EFS,
    which contains a personal workspace for notebooks. This workspace is where we
    store any notebook created or uploaded by a user. This is also where all reading
    and writing activity occurs when a user launches a notebook interactively. We
    rely on a combination of [workspace + filename] to form the notebook’s <em>namespace</em>,
    e.g. /efs/users/kylek/notebooks/MySparkJob.ipynb. We use this namespace for viewing,
    sharing, and scheduling notebooks. This convention prevents collisions and makes
    it easy to identify both the user and the location of the notebook in the EFS
    volume.</p><p>We can rely on the workspace path to abstract away the complexity
    of cloud-based storage from users. For example, only the filename of a notebook
    is displayed in directory listings, e.g. MySparkJob.ipynb. This same file is accessible
    at ~/notebooks/MySparkJob.ipynb from a terminal.</p><p><img src="https://cdn-images-1.medium.com/max/2000/1*pjjOIx1g7g4XoTxWBv0pIw.png"
    alt="Notebook storage vs. notebook access"><em>Notebook storage vs. notebook access</em></p><p>When
    the user schedules a notebook, the scheduler copies the user’s notebook from EFS
    to a common directory on S3. The notebook on S3 becomes the source of truth for
    the scheduler, or <em>source notebook</em>. Each time the scheduler runs a notebook,
    it instantiates a new notebook from the source notebook. This new notebook is
    what actually executes and becomes an immutable record of that execution, containing
    the code, output, and logs from each cell. We refer to this as the <em>output
    notebook</em>.</p><p>Collaboration is fundamental to how we work at Netflix. It
    came as no surprise then when users started sharing notebook URLs. As this practice
    grew, we ran into frequent problems with accidental overwrites caused by multiple
    people concurrently accessing the same notebook . Our users wanted a way to share
    their active notebook in a read-only state. This led to the creation of <a href="https://github.com/nteract/nteract/tree/master/applications/commuter">**Commuter</a>**.
    Behind the scenes, Commuter surfaces the Jupyter APIs for /files and /api/contents
    to list directories, view file contents, and access file metadata. This means
    users can safely view notebooks without affecting production jobs or live-running
    notebooks.</p><h3><strong>Compute</strong></h3><p>Managing compute resources is
    one of the most challenging parts of working with data. This is especially true
    at Netflix, where we employ a highly-scalable containerized architecture on AWS.
    All jobs on the Data Platform run on containers — including queries, pipelines,
    and notebooks. Naturally, we wanted to abstract away as much of this complexity
    as possible.</p><p>A container is provisioned when a user launches a notebook
    server. We provide reasonable defaults for container resources, which works for
    ~87.3% of execution patterns. When that’s not enough, users can request more resources
    using a simple interface.</p><p><img src="https://cdn-images-1.medium.com/max/2360/0*FUdkFk_m2tyeloJa"
    alt="Users can select as much or as little compute + memory as they need"><em>Users
    can select as much or as little compute + memory as they need</em></p><p>We also
    provide a unified execution environment with a prepared container image. The image
    has common libraries and an array of default kernels preinstalled. Not everything
    in the image is static — our kernels pull the most recent versions of Spark and
    the latest cluster configurations for our platform. This reduces the friction
    and setup time for new notebooks and generally keeps us to a single execution
    environment.</p><p>Under the hood we’re managing the orchestration and environments
    with <a href="https://medium.com/netflix-techblog/titus-the-netflix-container-management-platform-is-now-open-source-f868c9fb5436">Titus</a>,
    our Docker container management service. We further wrap that service by managing
    the user’s particular server configuration and image. The image also includes
    user security groups and roles, as well as common environment variables for identity
    within included libraries. This means our users can spend less time on infrastructure
    and more time on data.</p><h3><strong>Interface</strong></h3><p>Earlier we described
    our vision for notebooks to become the tool of choice for working with data. But
    this presents an interesting challenge: how can a single interface support all
    users? We don’t fully know the answer yet, but we have some ideas.</p><p>We know
    we want to lean into simplicity. This means an intuitive UI with a minimalistic
    aesthetic, and it also requires a thoughtful UX that makes it easy to do the hard
    things. This philosophy aligns well with the goals of <a href="https://github.com/nteract/nteract">**nteract</a>**,
    a React-based frontend for Jupyter notebooks. It emphasizes simplicity and <a
    href="https://components.nteract.io/">composability</a> as core design principles,
    which makes it an ideal building block for the work we want to do.</p><p>One of
    the most frequent complaints we heard from users is the lack of native data visualization
    across language boundaries, especially for non-Python languages. nteract’s <a
    href="https://blog.nteract.io/designing-the-nteract-data-explorer-f4476d53f897">Data
    Explorer</a> is a good example of how we can make the hard things simpler by providing
    a language-agnostic way to explore data quickly.</p><p>You can see Data Explorer
    in action in this <a href="https://mybinder.org/v2/gh/nteract/examples/master?urlpath=%2Fnteract%2Fedit%2Fpython%2Fhappiness.ipynb">sample
    notebook</a> on MyBinder. <em>(please note: it may take a minute to load)</em></p><p><img
    src="https://cdn-images-1.medium.com/max/5652/1*uW5qwTm4FQ3OauOWu_MCcA.gif" alt="Visualizing
    the World Happiness Report dataset with nteract’s Data Explorer"><em>Visualizing
    the World Happiness Report dataset with nteract’s Data Explorer</em></p><p>We’re
    also introducing native support for parametrization, which makes it easier to
    schedule notebooks and create reusable templates.</p><p><img src="https://cdn-images-1.medium.com/max/3838/1*UjwesaxwjH3d1glkP41J4w.gif"
    alt="Native support for parameterized notebooks in nteract"><em>Native support
    for parameterized notebooks in nteract</em></p><p>Although notebooks are already
    offering a lot of value at Netflix, we’ve just begun. We know we need to <a href="https://jobs.netflix.com/search?q=notebooks">make
    investments</a> in both the frontend and backend to improve the overall notebook
    experience. Our work over the next 12 months is focused on improving reliability,
    visibility, and collaboration. Context is paramount for users, which is why we’re
    increasing visibility into cluster status, kernel state, job history, and more.
    We’re also working on automatic version control, native in-app scheduling, better
    support for visualizing Spark DataFrames, and greater stability for our Scala
    kernel. We’ll go into more detail on this work in a future blog post.</p><h2>Open
    Source Projects</h2><p>Netflix has long been a proponent of open source. We value
    the energy, open standards, and exchange of ideas that emerge from open source
    collaborations. Many of the applications we developed for the Netflix Data Platform
    have already been open sourced through <a href="https://netflix.github.io/">Netflix
    OSS</a>. We are also intentional about not creating one-off solutions or succumbing
    to “Not Invented Here” mentality. Whenever possible, we leverage and contribute
    to existing open source projects, such as <a href="https://github.com/apache/spark">Spark</a>,
    <a href="https://github.com/jupyter">Jupyter</a>, and <a href="https://github.com/pandas-dev/pandas">pandas</a>.</p><p>The
    infrastructure we’ve described relies heavily on the Project Jupyter ecosystem,
    but there are some places where we diverge. Most notably, we have chosen <a href="https://github.com/nteract">**nteract</a>**
    as the notebook UI for Netflix. We made this decision for many reasons, including
    alignment with our technology stack and design philosophies. As we push the limits
    of what a notebook can do, we will likely create new tools, libraries, and services.
    These projects will also be open sourced as part of the nteract ecosystem.</p><p>We
    recognize that what makes sense for Netflix does not necessarily make sense for
    everyone. We have designed these projects with modularity in mind. This makes
    it possible to pick and choose only the components that make sense for your environment,
    e.g. Papermill, without requiring a commitment to the entire ecosystem.</p><h2>What’s
    Next</h2><p>As a platform team, our responsibility is to enable Netflixers to
    do amazing things with data. Notebooks are already having a dramatic impact at
    Netflix. With the significant investments we’re making in this space, we’re excited
    to see this impact grow. If you’d like to be a part of it, check out <a href="https://jobs.netflix.com/search?q=notebooks">our
    job openings</a>.</p><p>Phew! Thanks for sticking with us through this long post.
    We’ve just scratched the surface of what we’re doing with notebooks. This post
    is part one in a series on notebooks at Netflix we’ll be releasing over the coming
    weeks. You can follow us on Medium for more from Netflix and check out the currently
    released articles below:</p><ul><li><p>Part I: <a href="https://medium.com/netflix-techblog/notebook-innovation-591ee3221233">Notebook
    Innovation</a> (this post)</p></li><li><p>Part II: <a href="https://medium.com/@NetflixTechBlog/scheduling-notebooks-348e6c14cfd6">Scheduling
    Notebooks</a></p></li></ul><p>We’re thrilled to sponsor this year’s <a href="https://conferences.oreilly.com/jupyter/jup-ny">JupyterCon</a>.
    If you’re attending, check out one of the 5 talks by our engineers, or swing by
    our booth to talk about Jupyter, nteract, or data with us.</p><ul><li><p>8/22
    1:30 PM — <a href="https://conferences.oreilly.com/jupyter/jup-ny/public/schedule/detail/68413">How
    to Build on top of Jupyter’s Protocols</a>, <em>Kyle Kelley</em></p></li><li><p>8/23
    1:50 PM — <a href="https://conferences.oreilly.com/jupyter/jup-ny/public/schedule/detail/68348">Scheduled
    Notebooks: Manageable and traceable code execution</a>, <em>Matthew Seal</em></p></li><li><p>8/23
    2:40 PM — <a href="https://conferences.oreilly.com/jupyter/jup-ny/public/schedule/detail/71601">Notebooks
    @ Netflix: From Analytics to Engineering</a>, <em>Michelle Ufford, Kyle Kelley</em></p></li><li><p>8/23
    5:00 PM — <a href="https://conferences.oreilly.com/jupyter/jup-ny/public/schedule/detail/71220">Making
    beautiful objects with Jupyter</a>, <em>M Pacer</em></p></li><li><p>8/24 2:40
    PM — <a href="https://conferences.oreilly.com/jupyter/jup-ny/public/schedule/detail/71220">Jupyter’s
    configuration system</a>, <em>M Pacer</em> et. al.</p></li><li><p>8/25 9AM — 5PM
    <a href="https://blog.jupyter.org/jupytercon-2018-registration-open-3b52abba9cce#a707">JupyterCon
    Community Sprint Day</a></p></li></ul><p>There are more ways to learn from Netflix
    Data and we’re happy to share:</p><ul><li><p><a href="http://twitter.com/NetflixData">@NetflixData</a>
    on Twitter</p></li><li><p><a href="https://www.youtube.com/channel/UC00QATOrSH4K2uOljTnnaKw/featured">Netflix
    Data</a> talks on YouTube</p></li><li><p><a href="https://research.netflix.com/">Netflix
    Research</a> website</p></li></ul><p>You can also stay up to date with nteract
    via their <a href="http://bit.ly/nteract-monthly">mailing list</a> and <a href="https://blog.nteract.io/">blog</a>!</p>'
  :topic_id: 628
- :url: https://onezero.medium.com/apple-is-trying-to-kill-web-technology-a274237c174d?source=search_post---------1
  :title: Apple Is Trying to Kill Web Technology
  :content: '<p>The company has made it extremely difficult to use web-based technology
    on its platforms, and it hopes developers won’t bother</p><p><img src="https://cdn-images-1.medium.com/max/10000/1*zqENj8mfvANOuM2PEcFPxw.jpeg"
    alt="Credit: SOPA Images/Getty Images"><em>Credit: SOPA Images/Getty Images</em></p><p>The
    programming languages used to build the web often find their way into apps, too.
    That’s largely due to software that allows developers to “reuse” the code they
    write for the web in products they build to run on operating systems like Linux,
    Android, Windows, and macOS.</p><p>But Apple has a reason not to like this recycling
    of web technology. It wants its Mac App Store to be filled with apps that you
    can’t find anywhere else, not apps that are available on every platform. <a href="https://9to5mac.com/2019/11/04/electron-app-rejections/">With
    a recent policy change</a>, the company has made it a little more difficult for
    developers to submit apps containing web code.</p><p>The Mac App Store has quietly
    started rejecting apps made with a popular tool called Electron that allows developers
    to base all of their apps on the web-based code. Some of the most popular apps
    in the App Store, like Slack, Spotify, Discord, and WhatsApp, fall into this category.</p><p><a
    href="https://github.com/electron/electron/issues/20027">In a discussion</a> on
    the programming community Github, several developers say rejections for apps that
    they built using Electron — which would were approved in the past — came with
    an explanation that these apps “attempt to hide the use of private APIs,” which
    are APIs built for Apple’s internal usage, rather than for third-party developers.
    Using private APIs to build public-facing apps is commonly frowned upon because
    they may change or break over time, and Apple <a href="https://developer.apple.com/app-store/review/guidelines/#software-requirements">bans</a>
    apps that use them.</p><p>Electron has used these private APIs for years without
    issue. These private APIs allow developers to, for instance, <a href="https://mozillagfx.wordpress.com/2019/10/22/dramatically-reduced-power-usage-in-firefox-70-on-macos-with-core-animation/">drastically
    improve power usage</a> whereas Apple’s sanctioned tools make the user experience
    worse. In the majority of these cases, Apple doesn’t provide real alternatives
    for developers who want to access these private API features.</p><p>Now it’s unlikely
    that the thousands of developers who have built their apps using Electron can
    release updates to them unless the Electron framework releases a major change
    to its implementation.</p><p>Developers could distribute their apps from their
    own websites, asking users to download them directly. But that means abandoning
    features like Apple’s auto-update mechanism from the Mac App Store and iCloud
    sync. And this direct-to-consumer method could soon be locked down, too, with
    Apple’s controversial <a href="https://developer.apple.com/documentation/security/notarizing_your_app_before_distribution">notarization
    requirements</a> potentially requiring their review.</p><p>Apple has a history
    of stunting the web’s progress on its platforms. On iOS, <a href="https://www.howtogeek.com/184283/why-third-party-browsers-will-always-be-inferior-to-safari-on-iphone-and-ipad/">Apple
    doesn’t allow fully independent third-party browsers</a>, requiring all apps to
    leverage its Safari browser when rendering web-based content. While browsers like
    Chrome and Opera are available in the App Store, they must use Apple’s Safari
    browser behind the scenes to render web pages, rather than their own. That means
    Apple has a monopoly on how iPhone and iPad users access the web. To push developers
    toward building native apps on iOS rather than using web technologies, Apple ignores
    popular parts of the <a href="https://www.w3.org/TR/">open web specification</a>
    that other browsers implement, to its own benefit.</p><blockquote><h1>Apple’s
    subtle, anti-competitive practices don’t look terrible in isolation, but together
    they form a clear strategy.</h1></blockquote><p>A technology called WebRTC, for
    example, allows video calling in a web browser without additional software. It
    powers tools like Google Meet. But Apple was incredibly slow to <a href="https://webkit.org/blog/8672/on-the-road-to-webrtc-1-0-including-vp8/">implement
    the specification</a>, leaving out key pieces of functionality, and the technology
    didn’t work <a href="https://bugs.webkit.org/show_bug.cgi?id=183201">when embedded
    inside apps</a>.</p><p>Apple also handicapped an emerging standard called Progressive
    Web Apps (PWAs) — which, like Electron, allows developers to build native-like
    apps for both desktop and mobile — by <a href="https://caniuse.com/#feat=web-app-manifest">partially</a>
    <a href="https://medium.com/@firt/whats-new-on-ios-12-2-for-progressive-web-apps-75c348f8e945">implementing</a>
    it in a way that makes it too inconsistent to rely on. PWA doesn’t have the same
    problem if users open apps in Chrome or Firefox, but iPhone and iPad users can’t
    install third-party browsers, which makes PWA-based technology a non-starter.</p><p>Developers
    use technologies like Electron and PWA because they allow for faster updates across
    platforms without an array of different codebases. Some argue that this results
    in lower quality apps, but I’d argue the alternative is no app at all or apps
    that are rarely updated because maintaining unique Windows, Mac, and web-based
    products is complex and expensive. Apple recently launched a <a href="https://onezero.medium.com/to-revive-the-mac-apple-wants-to-kill-electron-154873336e78">competing
    framework called Catalyst</a>, which allows developers with iPad apps to bring
    them to macOS quickly — a great tool for developers exclusively targeting Apple
    users, but not those building cross-platform apps.</p><p>Apple’s subtle, anti-competitive
    practices don’t look terrible in isolation, but together they form a clear strategy:
    Make it so painful to build with web-based technology on Apple platforms that
    developers won’t bother. Now that the App Store is not accepting apps built using
    Electron, developers will likely find creative ways to work around it, but Apple
    is setting up for a continual cat-and-mouse game as it <a href="https://www.macrumors.com/2019/09/03/apple-macos-catalina-notarization-mac-apps/">plans
    to exert more control</a> over which apps can run on the platform in the future.</p><p>These
    types of changes may be made in the name of privacy or security, but the reality
    is that the argument looks weak when both users and developers simply don’t have
    a choice because Apple controls the platform, browser engine, and the distribution
    method. Regardless of your opinion of Electron app quality, choice is important.</p><p>Apple’s
    control over its app ecosystem is a new type of monopoly that’s hard to understand
    for lawmakers, and difficult for us to fight back against — because there simply
    isn’t a way out of these restrictions when the company controls both the distribution
    method and the platform itself.</p>'
  :author: '<p>The company has made it extremely difficult to use web-based technology
    on its platforms, and it hopes developers won’t bother</p><p><img src="https://cdn-images-1.medium.com/max/10000/1*zqENj8mfvANOuM2PEcFPxw.jpeg"
    alt="Credit: SOPA Images/Getty Images"><em>Credit: SOPA Images/Getty Images</em></p><p>The
    programming languages used to build the web often find their way into apps, too.
    That’s largely due to software that allows developers to “reuse” the code they
    write for the web in products they build to run on operating systems like Linux,
    Android, Windows, and macOS.</p><p>But Apple has a reason not to like this recycling
    of web technology. It wants its Mac App Store to be filled with apps that you
    can’t find anywhere else, not apps that are available on every platform. <a href="https://9to5mac.com/2019/11/04/electron-app-rejections/">With
    a recent policy change</a>, the company has made it a little more difficult for
    developers to submit apps containing web code.</p><p>The Mac App Store has quietly
    started rejecting apps made with a popular tool called Electron that allows developers
    to base all of their apps on the web-based code. Some of the most popular apps
    in the App Store, like Slack, Spotify, Discord, and WhatsApp, fall into this category.</p><p><a
    href="https://github.com/electron/electron/issues/20027">In a discussion</a> on
    the programming community Github, several developers say rejections for apps that
    they built using Electron — which would were approved in the past — came with
    an explanation that these apps “attempt to hide the use of private APIs,” which
    are APIs built for Apple’s internal usage, rather than for third-party developers.
    Using private APIs to build public-facing apps is commonly frowned upon because
    they may change or break over time, and Apple <a href="https://developer.apple.com/app-store/review/guidelines/#software-requirements">bans</a>
    apps that use them.</p><p>Electron has used these private APIs for years without
    issue. These private APIs allow developers to, for instance, <a href="https://mozillagfx.wordpress.com/2019/10/22/dramatically-reduced-power-usage-in-firefox-70-on-macos-with-core-animation/">drastically
    improve power usage</a> whereas Apple’s sanctioned tools make the user experience
    worse. In the majority of these cases, Apple doesn’t provide real alternatives
    for developers who want to access these private API features.</p><p>Now it’s unlikely
    that the thousands of developers who have built their apps using Electron can
    release updates to them unless the Electron framework releases a major change
    to its implementation.</p><p>Developers could distribute their apps from their
    own websites, asking users to download them directly. But that means abandoning
    features like Apple’s auto-update mechanism from the Mac App Store and iCloud
    sync. And this direct-to-consumer method could soon be locked down, too, with
    Apple’s controversial <a href="https://developer.apple.com/documentation/security/notarizing_your_app_before_distribution">notarization
    requirements</a> potentially requiring their review.</p><p>Apple has a history
    of stunting the web’s progress on its platforms. On iOS, <a href="https://www.howtogeek.com/184283/why-third-party-browsers-will-always-be-inferior-to-safari-on-iphone-and-ipad/">Apple
    doesn’t allow fully independent third-party browsers</a>, requiring all apps to
    leverage its Safari browser when rendering web-based content. While browsers like
    Chrome and Opera are available in the App Store, they must use Apple’s Safari
    browser behind the scenes to render web pages, rather than their own. That means
    Apple has a monopoly on how iPhone and iPad users access the web. To push developers
    toward building native apps on iOS rather than using web technologies, Apple ignores
    popular parts of the <a href="https://www.w3.org/TR/">open web specification</a>
    that other browsers implement, to its own benefit.</p><blockquote><h1>Apple’s
    subtle, anti-competitive practices don’t look terrible in isolation, but together
    they form a clear strategy.</h1></blockquote><p>A technology called WebRTC, for
    example, allows video calling in a web browser without additional software. It
    powers tools like Google Meet. But Apple was incredibly slow to <a href="https://webkit.org/blog/8672/on-the-road-to-webrtc-1-0-including-vp8/">implement
    the specification</a>, leaving out key pieces of functionality, and the technology
    didn’t work <a href="https://bugs.webkit.org/show_bug.cgi?id=183201">when embedded
    inside apps</a>.</p><p>Apple also handicapped an emerging standard called Progressive
    Web Apps (PWAs) — which, like Electron, allows developers to build native-like
    apps for both desktop and mobile — by <a href="https://caniuse.com/#feat=web-app-manifest">partially</a>
    <a href="https://medium.com/@firt/whats-new-on-ios-12-2-for-progressive-web-apps-75c348f8e945">implementing</a>
    it in a way that makes it too inconsistent to rely on. PWA doesn’t have the same
    problem if users open apps in Chrome or Firefox, but iPhone and iPad users can’t
    install third-party browsers, which makes PWA-based technology a non-starter.</p><p>Developers
    use technologies like Electron and PWA because they allow for faster updates across
    platforms without an array of different codebases. Some argue that this results
    in lower quality apps, but I’d argue the alternative is no app at all or apps
    that are rarely updated because maintaining unique Windows, Mac, and web-based
    products is complex and expensive. Apple recently launched a <a href="https://onezero.medium.com/to-revive-the-mac-apple-wants-to-kill-electron-154873336e78">competing
    framework called Catalyst</a>, which allows developers with iPad apps to bring
    them to macOS quickly — a great tool for developers exclusively targeting Apple
    users, but not those building cross-platform apps.</p><p>Apple’s subtle, anti-competitive
    practices don’t look terrible in isolation, but together they form a clear strategy:
    Make it so painful to build with web-based technology on Apple platforms that
    developers won’t bother. Now that the App Store is not accepting apps built using
    Electron, developers will likely find creative ways to work around it, but Apple
    is setting up for a continual cat-and-mouse game as it <a href="https://www.macrumors.com/2019/09/03/apple-macos-catalina-notarization-mac-apps/">plans
    to exert more control</a> over which apps can run on the platform in the future.</p><p>These
    types of changes may be made in the name of privacy or security, but the reality
    is that the argument looks weak when both users and developers simply don’t have
    a choice because Apple controls the platform, browser engine, and the distribution
    method. Regardless of your opinion of Electron app quality, choice is important.</p><p>Apple’s
    control over its app ecosystem is a new type of monopoly that’s hard to understand
    for lawmakers, and difficult for us to fight back against — because there simply
    isn’t a way out of these restrictions when the company controls both the distribution
    method and the platform itself.</p>'
  :topic_id: 628
- :url: https://netflixtechblog.com/our-learnings-from-adopting-graphql-f099de39ae5f?source=search_post---------3
  :title: Our learnings from adopting GraphQL
  :content: '<p>A Marketing Tech Campaign</p><p>by <a href="https://www.linkedin.com/in/shtatnov/">Artem
    Shtatnov</a> and <a href="https://twitter.com/ducktyped">Ravi Srinivas Ranganathan</a></p><p>In
    an <a href="https://medium.com/netflix-techblog/https-medium-com-netflixtechblog-engineering-to-improve-marketing-effectiveness-part-2-7dd933974f5e">earlier
    blog post</a>, we provided a high-level overview of some of the applications in
    the Marketing Technology team that we build to enable <em>scale and intelligence</em>
    in driving our global advertising, which reaches users on sites like The New York
    Times, Youtube, and thousands of others. In this post, we’ll share our journey
    in updating our front-end architecture and our learnings in introducing GraphQL
    into the Marketing Tech system.</p><p>Our primary application for managing the
    creation and assembly of ads that reach the external publishing platforms is internally
    dubbed <strong><em>Monet</em></strong>. It’s used to supercharge ad creation and
    automate management of marketing campaigns on external ad platforms. Monet helps
    drive incremental conversions, engagement with our product and in general, present
    a rich story about our content and the Netflix brand to users around the world.
    To do this, first, it helps scale up and automate ad production and manage millions
    of creative permutations. Secondly, we utilize various signals and aggregate data
    such as understanding of content popularity on Netflix to enable highly relevant
    ads. Our overall aim is to make our ads on all the external publishing channels
    resonate well with users and we are constantly experimenting to improve our effectiveness
    in doing that.</p><p><img src="https://cdn-images-1.medium.com/max/2166/0*CafLBZiEtz9uwO62"
    alt="Monet and the high-level *Marketing Technology* flow"><em>Monet and the high-level
    *Marketing Technology</em> flow*</p><p>When we started out, the React UI layer
    for Monet accessed traditional REST APIs powered by an Apache Tomcat server. Over
    time, as our application evolved, our use cases became more complex. Simple pages
    would need to draw in data from a wide variety of sources. To more effectively
    load this data onto the client application, we first attempted to denormalize
    data on the backend. Managing this denormalization became difficult to maintain
    since not all pages needed all the data. We quickly ran into network bandwidth
    bottlenecks. The browser would need to fetch much more denormalized data than
    it would ever use.</p><p>To winnow down the number of fields sent to the client,
    one approach is to build custom endpoints for every page; it was a fairly obvious
    non-starter. Instead of building these custom endpoints, we opted for GraphQL
    as the middle layer of the app. We also considered <a href="https://netflix.github.io/falcor/">Falcor</a>
    as a possible solution since it has delivered great results at Netflix in many
    core use cases and has a ton of usage, but a robust GraphQL ecosystem and powerful
    third party tooling made GraphQL the better option for our use case. Also, as
    our data structures have become increasingly graph-oriented, it ended up being
    a very natural fit. Not only did adding GraphQL solve the network bandwidth bottleneck,
    but it also provided numerous other benefits that helped us add features more
    quickly.</p><p><img src="https://cdn-images-1.medium.com/max/2000/1*pmh-VimZJJindIJUyZtyzg.png"
    alt="Architecture before and after GraphQL"><em>Architecture before and after
    GraphQL</em></p><h2>Benefits</h2><p>We have been running GraphQL on NodeJS for
    about 6 months, and it has proven to significantly increase our development velocity
    and overall page load performance. Here are some of the benefits that worked out
    well for us since we started using it.</p><p><strong>Redistributing load and payload
    optimization</strong></p><p>Often times, some machines are better suited for certain
    tasks than others. When we added the GraphQL middle layer, the GraphQL server
    still needed to call the same services and REST APIs as the client would have
    called directly. The difference now is that the majority of the data is flowing
    between servers within the same data center. These server to server calls are
    of very low latency and high bandwidth, which gives us about an 8x performance
    boost compared to direct network calls from the browser. The last mile of the
    data transfer from the GraphQL server to the client browser, although still a
    slow point, is now reduced to a single network call. Since GraphQL allows the
    client to select only the data it needs we end up fetching a significantly smaller
    payload. In our application, pages that were fetching 10MB of data before now
    receive about 200KB. Page loads became much faster, especially over data-constrained
    mobile networks, and our app uses much less memory. These changes did come at
    the cost of higher server utilization to perform data fetching and aggregation,
    but the few extra milliseconds of server time per request were greatly outweighed
    by the smaller client payloads.</p><p><strong>Reusable abstractions</strong></p><p>Software
    developers generally want to work with reusable abstractions instead of single-purpose
    methods. With GraphQL, we define each piece of data once and define how it relates
    to other data in our system. When the consumer application fetches data from multiple
    sources, it no longer needs to worry about the complex business logic associated
    with data join operations.</p><p>Consider the following example, we define entities
    in GraphQL exactly once: <em>catalogs, creatives, and comments</em>. We can now
    build the views for several pages from these definitions. One page on the client
    app (catalogView) declares that it wants all comments for all creatives in a catalog
    while another client page (creativeView) wants to know the associated catalog
    that a creative belongs to, along with all of its comments.</p><p><img src="https://cdn-images-1.medium.com/max/2000/1*Tr-cnrbTOPKkWkshYpQeIA.png"
    alt="The flexibility of the GraphQL data model to represent different views from
    the same underlying data"><em>The flexibility of the GraphQL data model to represent
    different views from the same underlying data</em></p><p>The same graph model
    can power both of these views without having to make any server side code changes.</p><p><strong>Chaining
    type systems</strong></p><p>Many people focus on type systems within a single
    service, but rarely across services. Once we defined the entities in our GraphQL
    server, we use auto codegen tools to generate TypeScript types for the client
    application. The props of our React components receive types to match the query
    that the component is making. Since these types and queries are also validated
    against the server schema, any breaking change by the server would be caught by
    clients consuming the data. Chaining multiple services together with GraphQL and
    hooking these checks into the build process allows us to catch many more issues
    before deploying bad code. Ideally, we would like to have type safety from the
    database layer all the way to the client browser.</p><p><img src="https://cdn-images-1.medium.com/max/2000/1*YLL0aFFgcGDXFEa-V9_LPA.png"
    alt="Type safety from database to backend to client code"><em>Type safety from
    database to backend to client code</em></p><p><strong>DI/DX — Simplifying development</strong></p><p>A
    common concern when creating client applications is the UI/UX, but the developer
    interface and developer experience is just as important for building maintainable
    apps. Before GraphQL, writing a new React container component required maintaining
    complex logic to make network requests for the data we need. The developer would
    need to consider how one piece of data relates to another, how the data should
    be cached, whether to make the calls in parallel or in sequence and where in Redux
    to store the data. With a GraphQL query wrapper, each React component only needs
    to describe the data it needs, and the wrapper takes care of all of these concerns.
    There is much less boilerplate code and a cleaner separation of concerns between
    the data and UI. This model of declarative data fetching makes the React components
    much easier to understand, and serves to partially document what the component
    is doing.</p><p><strong>Other benefits</strong></p><p>There are a few other smaller
    benefits that we noticed as well. First, if any resolver of the GraphQL query
    fails, the resolvers that succeeded still return data to the client to render
    as much of the page as possible. Second, the backend data model is greatly simplified
    since we are less concerned with modeling for the client and in most cases can
    simply provide a CRUD interface to raw entities. Finally, testing our components
    has also become easier since the GraphQL query is automatically translatable into
    stubs for our tests and we can test resolvers in isolation from the React components.</p><h2>Growing
    pains</h2><p>Our migration to GraphQL was a straightforward experience. Most of
    the infrastructure we built to make network requests and transform data was easily
    transferable from our React application to our NodeJS server without any code
    changes. We even ended up deleting more code than we added. But as with any migration
    to a new technology, there were a few obstacles we needed to overcome.</p><p><strong>Selfish
    resolvers</strong></p><p>Since resolvers in GraphQL are meant to run as isolated
    units that are not concerned with what other resolvers do, we found that they
    were making many duplicate network requests for the same or similar data. We got
    around this duplication by wrapping the data providers in a simple caching layer
    that stored network responses in memory until all resolvers finished. The caching
    layer also allowed us to aggregate multiple requests to a single service into
    a bulk request for all the data at once. Resolvers can now request any data they
    need without worrying about how to optimize the process of fetching it.</p><p><img
    src="https://cdn-images-1.medium.com/max/2038/1*FZCtNPL4bXS6jpgVZx0RYg.png" alt="Adding
    a cache to simplify data access from resolvers"><em>Adding a cache to simplify
    data access from resolvers</em></p><p><strong>What a tangled web we weave</strong></p><p>Abstractions
    are a great way to make developers more efficient… until something goes wrong.
    There will undoubtedly be bugs in our code and we didn’t want to obfuscate the
    root cause with a middle layer. GraphQL would orchestrate network calls to other
    services automatically, hiding the complexities from the user. Server logs provide
    a way to debug, but they are still one step removed from the natural approach
    of debugging via the browser’s network tab. To make debugging easier, we added
    logs directly to the GraphQL response payload that expose all of the network requests
    that the server is making. When the debug flag is enabled, you get the same data
    in the client browser as you would if the browser made the network call directly.</p><p><strong>Breaking
    down typing</strong></p><p>Passing around objects is what OOP is all about, but
    unfortunately, GraphQL throws a wrench into this paradigm. When we fetch partial
    objects, this data cannot be used in methods and components that require the full
    object. Of course, you can cast the object manually and hope for the best, but
    you lose many of the benefits of type systems. Luckily, TypeScript uses duck typing,
    so adjusting the methods to only require the object properties that they really
    need was the quickest fix. Defining these more precise types takes a bit more
    work, but gives greater type safety overall.</p><h2>What comes next</h2><p>We
    are still in the early stages in our exploration of GraphQL, but it’s been a positive
    experience so far and we’re happy to have embraced it. One of the key goals of
    this endeavor was to help us get increased development velocity as our systems
    become increasingly sophisticated. Instead of being bogged down with complex data
    structures, we hope for the investment in the graph data model to make our team
    more productive over time as more edges and nodes are added. Even over the last
    few months, we have found that our existing graph model has become sufficiently
    robust that we don’t need any graph changes to be able to build some features.
    It has certainly made us more productive.</p><p><img src="https://cdn-images-1.medium.com/max/2000/1*T3KO2GOY6EhoWUdQw8zuLQ.png"
    alt="Visualization of our GraphQL Schema"><em>Visualization of our GraphQL Schema</em></p><p>As
    GraphQL continues to thrive and mature, we look forward to learning from all the
    amazing things that the community can build and solve with it. On an implementation
    level, we are looking forward to using some cool concepts like schema stitching,
    which can make integrations with other services much more straightforward and
    save a great deal of developer time. Most crucially, it’s very exciting to see
    a lot more teams <a href="https://medium.com/netflix-techblog/the-new-netflix-stethoscope-native-app-f4e1d38aafcd">across
    our company</a> see GraphQL’s potential and start to adopt it.</p><p>If you’ve
    made this thus far and you’re also interested in joining the Netflix Marketing
    Technology team to help conquer our unique challenges, check out the <a href="https://sites.google.com/netflix.com/adtechjobs/ad-tech-engineering">open
    positions</a> listed on our page. <strong><em>We’re hiring!</em></strong></p>'
  :author: '<p>A Marketing Tech Campaign</p><p>by <a href="https://www.linkedin.com/in/shtatnov/">Artem
    Shtatnov</a> and <a href="https://twitter.com/ducktyped">Ravi Srinivas Ranganathan</a></p><p>In
    an <a href="https://medium.com/netflix-techblog/https-medium-com-netflixtechblog-engineering-to-improve-marketing-effectiveness-part-2-7dd933974f5e">earlier
    blog post</a>, we provided a high-level overview of some of the applications in
    the Marketing Technology team that we build to enable <em>scale and intelligence</em>
    in driving our global advertising, which reaches users on sites like The New York
    Times, Youtube, and thousands of others. In this post, we’ll share our journey
    in updating our front-end architecture and our learnings in introducing GraphQL
    into the Marketing Tech system.</p><p>Our primary application for managing the
    creation and assembly of ads that reach the external publishing platforms is internally
    dubbed <strong><em>Monet</em></strong>. It’s used to supercharge ad creation and
    automate management of marketing campaigns on external ad platforms. Monet helps
    drive incremental conversions, engagement with our product and in general, present
    a rich story about our content and the Netflix brand to users around the world.
    To do this, first, it helps scale up and automate ad production and manage millions
    of creative permutations. Secondly, we utilize various signals and aggregate data
    such as understanding of content popularity on Netflix to enable highly relevant
    ads. Our overall aim is to make our ads on all the external publishing channels
    resonate well with users and we are constantly experimenting to improve our effectiveness
    in doing that.</p><p><img src="https://cdn-images-1.medium.com/max/2166/0*CafLBZiEtz9uwO62"
    alt="Monet and the high-level *Marketing Technology* flow"><em>Monet and the high-level
    *Marketing Technology</em> flow*</p><p>When we started out, the React UI layer
    for Monet accessed traditional REST APIs powered by an Apache Tomcat server. Over
    time, as our application evolved, our use cases became more complex. Simple pages
    would need to draw in data from a wide variety of sources. To more effectively
    load this data onto the client application, we first attempted to denormalize
    data on the backend. Managing this denormalization became difficult to maintain
    since not all pages needed all the data. We quickly ran into network bandwidth
    bottlenecks. The browser would need to fetch much more denormalized data than
    it would ever use.</p><p>To winnow down the number of fields sent to the client,
    one approach is to build custom endpoints for every page; it was a fairly obvious
    non-starter. Instead of building these custom endpoints, we opted for GraphQL
    as the middle layer of the app. We also considered <a href="https://netflix.github.io/falcor/">Falcor</a>
    as a possible solution since it has delivered great results at Netflix in many
    core use cases and has a ton of usage, but a robust GraphQL ecosystem and powerful
    third party tooling made GraphQL the better option for our use case. Also, as
    our data structures have become increasingly graph-oriented, it ended up being
    a very natural fit. Not only did adding GraphQL solve the network bandwidth bottleneck,
    but it also provided numerous other benefits that helped us add features more
    quickly.</p><p><img src="https://cdn-images-1.medium.com/max/2000/1*pmh-VimZJJindIJUyZtyzg.png"
    alt="Architecture before and after GraphQL"><em>Architecture before and after
    GraphQL</em></p><h2>Benefits</h2><p>We have been running GraphQL on NodeJS for
    about 6 months, and it has proven to significantly increase our development velocity
    and overall page load performance. Here are some of the benefits that worked out
    well for us since we started using it.</p><p><strong>Redistributing load and payload
    optimization</strong></p><p>Often times, some machines are better suited for certain
    tasks than others. When we added the GraphQL middle layer, the GraphQL server
    still needed to call the same services and REST APIs as the client would have
    called directly. The difference now is that the majority of the data is flowing
    between servers within the same data center. These server to server calls are
    of very low latency and high bandwidth, which gives us about an 8x performance
    boost compared to direct network calls from the browser. The last mile of the
    data transfer from the GraphQL server to the client browser, although still a
    slow point, is now reduced to a single network call. Since GraphQL allows the
    client to select only the data it needs we end up fetching a significantly smaller
    payload. In our application, pages that were fetching 10MB of data before now
    receive about 200KB. Page loads became much faster, especially over data-constrained
    mobile networks, and our app uses much less memory. These changes did come at
    the cost of higher server utilization to perform data fetching and aggregation,
    but the few extra milliseconds of server time per request were greatly outweighed
    by the smaller client payloads.</p><p><strong>Reusable abstractions</strong></p><p>Software
    developers generally want to work with reusable abstractions instead of single-purpose
    methods. With GraphQL, we define each piece of data once and define how it relates
    to other data in our system. When the consumer application fetches data from multiple
    sources, it no longer needs to worry about the complex business logic associated
    with data join operations.</p><p>Consider the following example, we define entities
    in GraphQL exactly once: <em>catalogs, creatives, and comments</em>. We can now
    build the views for several pages from these definitions. One page on the client
    app (catalogView) declares that it wants all comments for all creatives in a catalog
    while another client page (creativeView) wants to know the associated catalog
    that a creative belongs to, along with all of its comments.</p><p><img src="https://cdn-images-1.medium.com/max/2000/1*Tr-cnrbTOPKkWkshYpQeIA.png"
    alt="The flexibility of the GraphQL data model to represent different views from
    the same underlying data"><em>The flexibility of the GraphQL data model to represent
    different views from the same underlying data</em></p><p>The same graph model
    can power both of these views without having to make any server side code changes.</p><p><strong>Chaining
    type systems</strong></p><p>Many people focus on type systems within a single
    service, but rarely across services. Once we defined the entities in our GraphQL
    server, we use auto codegen tools to generate TypeScript types for the client
    application. The props of our React components receive types to match the query
    that the component is making. Since these types and queries are also validated
    against the server schema, any breaking change by the server would be caught by
    clients consuming the data. Chaining multiple services together with GraphQL and
    hooking these checks into the build process allows us to catch many more issues
    before deploying bad code. Ideally, we would like to have type safety from the
    database layer all the way to the client browser.</p><p><img src="https://cdn-images-1.medium.com/max/2000/1*YLL0aFFgcGDXFEa-V9_LPA.png"
    alt="Type safety from database to backend to client code"><em>Type safety from
    database to backend to client code</em></p><p><strong>DI/DX — Simplifying development</strong></p><p>A
    common concern when creating client applications is the UI/UX, but the developer
    interface and developer experience is just as important for building maintainable
    apps. Before GraphQL, writing a new React container component required maintaining
    complex logic to make network requests for the data we need. The developer would
    need to consider how one piece of data relates to another, how the data should
    be cached, whether to make the calls in parallel or in sequence and where in Redux
    to store the data. With a GraphQL query wrapper, each React component only needs
    to describe the data it needs, and the wrapper takes care of all of these concerns.
    There is much less boilerplate code and a cleaner separation of concerns between
    the data and UI. This model of declarative data fetching makes the React components
    much easier to understand, and serves to partially document what the component
    is doing.</p><p><strong>Other benefits</strong></p><p>There are a few other smaller
    benefits that we noticed as well. First, if any resolver of the GraphQL query
    fails, the resolvers that succeeded still return data to the client to render
    as much of the page as possible. Second, the backend data model is greatly simplified
    since we are less concerned with modeling for the client and in most cases can
    simply provide a CRUD interface to raw entities. Finally, testing our components
    has also become easier since the GraphQL query is automatically translatable into
    stubs for our tests and we can test resolvers in isolation from the React components.</p><h2>Growing
    pains</h2><p>Our migration to GraphQL was a straightforward experience. Most of
    the infrastructure we built to make network requests and transform data was easily
    transferable from our React application to our NodeJS server without any code
    changes. We even ended up deleting more code than we added. But as with any migration
    to a new technology, there were a few obstacles we needed to overcome.</p><p><strong>Selfish
    resolvers</strong></p><p>Since resolvers in GraphQL are meant to run as isolated
    units that are not concerned with what other resolvers do, we found that they
    were making many duplicate network requests for the same or similar data. We got
    around this duplication by wrapping the data providers in a simple caching layer
    that stored network responses in memory until all resolvers finished. The caching
    layer also allowed us to aggregate multiple requests to a single service into
    a bulk request for all the data at once. Resolvers can now request any data they
    need without worrying about how to optimize the process of fetching it.</p><p><img
    src="https://cdn-images-1.medium.com/max/2038/1*FZCtNPL4bXS6jpgVZx0RYg.png" alt="Adding
    a cache to simplify data access from resolvers"><em>Adding a cache to simplify
    data access from resolvers</em></p><p><strong>What a tangled web we weave</strong></p><p>Abstractions
    are a great way to make developers more efficient… until something goes wrong.
    There will undoubtedly be bugs in our code and we didn’t want to obfuscate the
    root cause with a middle layer. GraphQL would orchestrate network calls to other
    services automatically, hiding the complexities from the user. Server logs provide
    a way to debug, but they are still one step removed from the natural approach
    of debugging via the browser’s network tab. To make debugging easier, we added
    logs directly to the GraphQL response payload that expose all of the network requests
    that the server is making. When the debug flag is enabled, you get the same data
    in the client browser as you would if the browser made the network call directly.</p><p><strong>Breaking
    down typing</strong></p><p>Passing around objects is what OOP is all about, but
    unfortunately, GraphQL throws a wrench into this paradigm. When we fetch partial
    objects, this data cannot be used in methods and components that require the full
    object. Of course, you can cast the object manually and hope for the best, but
    you lose many of the benefits of type systems. Luckily, TypeScript uses duck typing,
    so adjusting the methods to only require the object properties that they really
    need was the quickest fix. Defining these more precise types takes a bit more
    work, but gives greater type safety overall.</p><h2>What comes next</h2><p>We
    are still in the early stages in our exploration of GraphQL, but it’s been a positive
    experience so far and we’re happy to have embraced it. One of the key goals of
    this endeavor was to help us get increased development velocity as our systems
    become increasingly sophisticated. Instead of being bogged down with complex data
    structures, we hope for the investment in the graph data model to make our team
    more productive over time as more edges and nodes are added. Even over the last
    few months, we have found that our existing graph model has become sufficiently
    robust that we don’t need any graph changes to be able to build some features.
    It has certainly made us more productive.</p><p><img src="https://cdn-images-1.medium.com/max/2000/1*T3KO2GOY6EhoWUdQw8zuLQ.png"
    alt="Visualization of our GraphQL Schema"><em>Visualization of our GraphQL Schema</em></p><p>As
    GraphQL continues to thrive and mature, we look forward to learning from all the
    amazing things that the community can build and solve with it. On an implementation
    level, we are looking forward to using some cool concepts like schema stitching,
    which can make integrations with other services much more straightforward and
    save a great deal of developer time. Most crucially, it’s very exciting to see
    a lot more teams <a href="https://medium.com/netflix-techblog/the-new-netflix-stethoscope-native-app-f4e1d38aafcd">across
    our company</a> see GraphQL’s potential and start to adopt it.</p><p>If you’ve
    made this thus far and you’re also interested in joining the Netflix Marketing
    Technology team to help conquer our unique challenges, check out the <a href="https://sites.google.com/netflix.com/adtechjobs/ad-tech-engineering">open
    positions</a> listed on our page. <strong><em>We’re hiring!</em></strong></p>'
  :topic_id: 628
- :url: https://netflixtechblog.com/python-at-netflix-bba45dae649e?source=search_post---------4
  :title: Python at Netflix
  :content: '<p>By Pythonistas at Netflix, coordinated by Amjith Ramanujam and edited
    by Ellen Livengood</p><p><img src="https://cdn-images-1.medium.com/max/2000/1*PPIp7twJJUknfohZqtL8pQ.png"
    alt=""></p><p>As many of us prepare to go to PyCon, we wanted to share a sampling
    of how Python is used at Netflix. We use Python through the full content lifecycle,
    from deciding which content to fund all the way to operating the CDN that serves
    the final video to 148 million members. We use and contribute to many open-source
    Python packages, some of which are mentioned below. If any of this interests you,
    check out the <a href="https://jobs.netflix.com/search?q=python">jobs site</a>
    or find us at PyCon. We have donated a few Netflix Originals posters to the <a
    href="https://us.pycon.org/2019/events/auction/">PyLadies Auction</a> and look
    forward to seeing you all there.</p><h3>Open Connect</h3><p><a href="https://openconnect.netflix.com/en/">Open
    Connect</a> is Netflix’s content delivery network (CDN). An easy, though imprecise,
    way of thinking about Netflix infrastructure is that everything that happens before
    you press Play on your remote control (e.g., are you logged in? what plan do you
    have? what have you watched so we can recommend new titles to you? what do you
    want to watch?) takes place in Amazon Web Services (AWS), whereas everything that
    happens afterwards (i.e., video streaming) takes place in the Open Connect network.
    Content is placed on the network of servers in the Open Connect CDN as close to
    the end user as possible, improving the streaming experience for our customers
    and reducing costs for both Netflix and our Internet Service Provider (ISP) partners.</p><p>Various
    software systems are needed to design, build, and operate this CDN infrastructure,
    and a significant number of them are written in Python. The network devices that
    underlie a large portion of the CDN are mostly managed by Python applications.
    Such applications track the inventory of our network gear: what devices, of which
    models, with which hardware components, located in which sites. The configuration
    of these devices is controlled by several other systems including source of truth,
    application of configurations to devices, and back up. Device interaction for
    the collection of health and other operational data is yet another Python application.
    Python has long been a popular programming language in the networking space because
    it’s an intuitive language that allows engineers to quickly solve networking problems.
    Subsequently, many useful libraries get developed, making the language even more
    desirable to learn and use.</p><h3>Demand Engineering</h3><p><a href="https://www.linkedin.com/pulse/what-demand-engineering-aaron-blohowiak/">Demand
    Engineering</a> is responsible for <a href="https://opensource.com/article/18/4/how-netflix-does-failovers-7-minutes-flat">Regional
    Failovers</a>, Traffic Distribution, Capacity Operations, and Fleet Efficiency
    of the Netflix cloud. We are proud to say that our team’s tools are built primarily
    in Python. The service that orchestrates failover uses numpy and scipy to perform
    numerical analysis, boto3 to make changes to our AWS infrastructure, rq to run
    asynchronous workloads and we wrap it all up in a thin layer of Flask APIs. The
    ability to drop into a <a href="https://bpython-interpreter.org">bpython</a> shell
    and improvise has saved the day more than once.</p><p>We are heavy users of Jupyter
    Notebooks and <a href="https://nteract.io">nteract</a> to analyze operational
    data and prototype <a href="https://github.com/nteract/nteract/tree/master/packages/data-explorer">visualization
    tools </a>that help us detect capacity regressions.</p><h3>CORE</h3><p>The CORE
    team uses Python in our alerting and statistical analytical work. We lean on many
    of the statistical and mathematical libraries (numpy, scipy, ruptures, pandas)
    to help automate the analysis of 1000s of related signals when our alerting systems
    indicate problems. We’ve developed a time series correlation system used both
    inside and outside the team as well as a distributed worker system to parallelize
    large amounts of analytical work to deliver results quickly.</p><p>Python is also
    a tool we typically use for automation tasks, data exploration and cleaning, and
    as a convenient source for visualization work.</p><h3>Monitoring, alerting and
    auto-remediation</h3><p>The Insight Engineering team is responsible for building
    and operating the tools for operational insight, alerting, diagnostics, and auto-remediation.
    With the increased popularity of Python, the team now supports Python clients
    for most of their services. One example is the <a href="https://github.com/Netflix/spectator-py">Spectator</a>
    Python client library, a library for instrumenting code to record dimensional
    time series metrics. We build Python libraries to interact with other Netflix
    platform level services. In addition to libraries, the <a href="https://medium.com/netflix-techblog/introducing-winston-event-driven-diagnostic-and-remediation-platform-46ce39aa81cc">Winston</a>
    and <a href="https://medium.com/netflix-techblog/introducing-bolt-on-instance-diagnostic-and-remediation-platform-176651b55505">Bolt</a>
    products are also built using Python frameworks (Gunicorn + Flask + Flask-RESTPlus).</p><h3>Information
    Security</h3><p>The information security team uses Python to accomplish a number
    of high leverage goals for Netflix: security automation, risk classification,
    auto-remediation, and vulnerability identification to name a few. We’ve had a
    number of successful Python open sources, including <a href="https://github.com/Netflix/security_monkey">Security
    Monkey</a> (our team’s most active open source project). We leverage Python to
    protect our SSH resources using <a href="https://github.com/Netflix/bless">Bless</a>.
    Our Infrastructure Security team leverages Python to help with IAM permission
    tuning using <a href="https://github.com/Netflix/repokid">Repokid</a>. We use
    Python to help generate TLS certificates using <a href="https://github.com/Netflix/lemur">Lemur</a>.</p><p>Some
    of our more recent projects include Prism: a batch framework to help security
    engineers measure paved road adoption, risk factors, and identify vulnerabilities
    in source code. We currently provide Python and Ruby libraries for Prism. The
    <a href="https://medium.com/netflix-techblog/netflix-sirt-releases-diffy-a-differencing-engine-for-digital-forensics-in-the-cloud-37b71abd2698">Diffy</a>
    forensics triage tool is written entirely in Python. We also use Python to detect
    sensitive data using Lanius.</p><h3>Personalization Algorithms</h3><p>We use Python
    extensively within our broader <a href="https://www.slideshare.net/FaisalZakariaSiddiqi/ml-infra-for-netflix-recommendations-ai-nextcon-talk">Personalization
    Machine Learning Infrastructure</a> to train some of the Machine Learning models
    for key aspects of the Netflix experience: from our <a href="https://research.netflix.com/research-area/recommendations">recommendation
    algorithms</a> to <a href="https://medium.com/netflix-techblog/artwork-personalization-c589f074ad76">artwork
    personalization</a> to <a href="https://medium.com/netflix-techblog/engineering-to-scale-paid-media-campaigns-84ba018fb3fa">marketing
    algorithms</a>. For example, some algorithms use TensorFlow, Keras, and PyTorch
    to learn Deep Neural Networks, XGBoost and LightGBM to learn Gradient Boosted
    Decision Trees or the broader scientific stack in Python (e.g. numpy, scipy, sklearn,
    matplotlib, pandas, cvxpy). Because we’re constantly trying out new approaches,
    we use Jupyter Notebooks to drive many of our experiments. We have also developed
    a number of higher-level libraries to help integrate these with the rest of our
    <a href="https://www.slideshare.net/FaisalZakariaSiddiqi/ml-infra-for-netflix-recommendations-ai-nextcon-talk">ecosystem</a>
    (e.g. data access, fact logging and feature extraction, model evaluation, and
    publishing).</p><h3>Machine Learning Infrastructure</h3><p>Besides personalization,
    Netflix applies machine learning to hundreds of use cases across the company.
    Many of these applications are powered by <a href="https://www.youtube.com/watch?v=XV5VGddmP24">Metaflow</a>,
    a Python framework that makes it easy to execute ML projects from the prototype
    stage to production.</p><p>Metaflow pushes the limits of Python: We leverage well
    parallelized and optimized Python code to fetch data at 10Gbps, handle hundreds
    of millions of data points in memory, and orchestrate computation over tens of
    thousands of CPU cores.</p><h3>Notebooks</h3><p>We are avid users of Jupyter notebooks
    at Netflix, and we’ve written about the <a href="https://medium.com/netflix-techblog/notebook-innovation-591ee3221233">reasons
    and nature of this investment</a> before.</p><p>But Python plays a huge role in
    how we provide those services. Python is a primary language when we need to develop,
    debug, explore, and prototype different interactions with the Jupyter ecosystem.
    We use Python to build custom extensions to the Jupyter server that allows us
    to manage tasks like logging, archiving, publishing, and cloning notebooks on
    behalf of our users.We provide many flavors of Python to our users via different
    Jupyter kernels, and manage the deployment of those kernel specifications using
    Python.</p><h3>Orchestration</h3><p>The Big Data Orchestration team is responsible
    for providing all of the services and tooling to schedule and execute ETL and
    Adhoc pipelines.</p><p>Many of the components of the orchestration service are
    written in Python. Starting with our scheduler, which uses<a href="https://jupyter.org">
    Jupyter Notebooks</a> with <a href="https://papermill.readthedocs.io/en/latest/">papermill</a>
    to provide templatized job types (Spark, Presto, …). This allows our users to
    have a standardized and easy way to express work that needs to be executed. You
    can see some deeper details on the subject <a href="https://medium.com/netflix-techblog/scheduling-notebooks-348e6c14cfd6">here</a>.
    We have been using notebooks as real runbooks for situations where human intervention
    is required — for example: to restart everything that has failed in the last hour.</p><p>Internally,
    we also built an event-driven platform that is fully written in Python. We have
    created streams of events from a number of systems that get unified into a single
    tool. This allows us to define conditions to filter events, and actions to react
    or route them. As a result of this, we have been able to decouple microservices
    and get visibility into everything that happens on the data platform.</p><p>Our
    team also built the <a href="https://github.com/Netflix/pygenie">pygenie</a> client
    which interfaces with <a href="https://netflix.github.io/genie/">Genie</a>, a
    federated job execution service. Internally, we have additional extensions to
    this library that apply business conventions and integrate with the Netflix platform.
    These libraries are the primary way users interface programmatically with work
    in the Big Data platform.</p><p>Finally, it’s been our team’s commitment to contribute
    to <a href="https://papermill.readthedocs.io/en/latest/">papermill</a> and <a
    href="https://nteract-scrapbook.readthedocs.io/en/latest/">scrapbook</a> open
    source projects. Our work there has been both for our own and external use cases.
    These efforts have been gaining a lot of traction in the open source community
    and we’re glad to be able to contribute to these shared projects.</p><h3>Experimentation
    Platform</h3><p>The scientific computing team for experimentation is creating
    a platform for scientists and engineers to analyze AB tests and other experiments.
    Scientists and engineers can contribute new innovations on three fronts, data,
    statistics, and visualizations.</p><p>The Metrics Repo is a Python framework based
    on <a href="https://pypika.readthedocs.io/en/latest/">PyPika</a> that allows contributors
    to write reusable parameterized SQL queries. It serves as an entry point into
    any new analysis.</p><p>The Causal Models library is a Python &amp; R framework
    for scientists to contribute new models for causal inference. It leverages <a
    href="https://arrow.apache.org/docs/python/">PyArrow</a> and <a href="https://rpy2.readthedocs.io/en/version_2.8.x/">RPy2</a>
    so that statistics can be calculated seamlessly in either language.</p><p>The
    Visualizations library is based on <a href="https://plot.ly">Plotly</a>. Since
    Plotly is a widely adopted visualization spec, there are a variety of tools that
    allow contributors to produce an output that is consumable by our platforms.</p><h3>Partner
    Ecosystem</h3><p>The Partner Ecosystem group is expanding its use of Python for
    testing Netflix applications on devices. Python is forming the core of a new CI
    infrastructure, including controlling our orchestration servers, controlling Spinnaker,
    test case querying and filtering, and scheduling test runs on devices and containers.
    Additional post-run analysis is being done in Python using TensorFlow to determine
    which tests are most likely to show problems on which devices.</p><h3>Video Encoding
    and Media Cloud Engineering</h3><p>Our team takes care of encoding (and re-encoding)
    the Netflix catalog, as well as leveraging machine learning for insights into
    that catalog.We use Python for ~50 projects such as <a href="https://github.com/Netflix/vmaf/blob/master/resource/doc/references.md">vmaf</a>
    and <a href="https://medium.com/netflix-techblog/mezzfs-mounting-object-storage-in-netflixs-media-processing-platform-cda01c446ba">mezzfs</a>,
    we build <a href="https://medium.com/netflix-techblog/ava-the-art-and-science-of-image-discovery-at-netflix-a442f163af6">computer
    vision solutions</a> using a media map-reduce platform called <a href="https://medium.com/netflix-techblog/simplifying-media-innovation-at-netflix-with-archer-3f8cbb0e2bcb">Archer</a>,
    and we use Python for many internal projects.We have also open sourced a few tools
    to ease development/distribution of Python projects, like <a href="https://pypi.org/project/setupmeta/">setupmeta</a>
    and <a href="https://pypi.org/project/pickley/">pickley</a>.</p><h3>Netflix Animation
    and NVFX</h3><p>Python is the industry standard for all of the major applications
    we use to create Animated and VFX content, so it goes without saying that we are
    using it very heavily. All of our integrations with Maya and Nuke are in Python,
    and the bulk of our Shotgun tools are also in Python. We’re just getting started
    on getting our tooling in the cloud, and anticipate deploying many of our own
    custom Python AMIs/containers.</p><h3>Content Machine Learning, Science &amp;
    Analytics</h3><p>The Content Machine Learning team uses Python extensively for
    the development of machine learning models that are the core of forecasting audience
    size, viewership, and other demand metrics for all content.</p>'
  :author: '<p>By Pythonistas at Netflix, coordinated by Amjith Ramanujam and edited
    by Ellen Livengood</p><p><img src="https://cdn-images-1.medium.com/max/2000/1*PPIp7twJJUknfohZqtL8pQ.png"
    alt=""></p><p>As many of us prepare to go to PyCon, we wanted to share a sampling
    of how Python is used at Netflix. We use Python through the full content lifecycle,
    from deciding which content to fund all the way to operating the CDN that serves
    the final video to 148 million members. We use and contribute to many open-source
    Python packages, some of which are mentioned below. If any of this interests you,
    check out the <a href="https://jobs.netflix.com/search?q=python">jobs site</a>
    or find us at PyCon. We have donated a few Netflix Originals posters to the <a
    href="https://us.pycon.org/2019/events/auction/">PyLadies Auction</a> and look
    forward to seeing you all there.</p><h3>Open Connect</h3><p><a href="https://openconnect.netflix.com/en/">Open
    Connect</a> is Netflix’s content delivery network (CDN). An easy, though imprecise,
    way of thinking about Netflix infrastructure is that everything that happens before
    you press Play on your remote control (e.g., are you logged in? what plan do you
    have? what have you watched so we can recommend new titles to you? what do you
    want to watch?) takes place in Amazon Web Services (AWS), whereas everything that
    happens afterwards (i.e., video streaming) takes place in the Open Connect network.
    Content is placed on the network of servers in the Open Connect CDN as close to
    the end user as possible, improving the streaming experience for our customers
    and reducing costs for both Netflix and our Internet Service Provider (ISP) partners.</p><p>Various
    software systems are needed to design, build, and operate this CDN infrastructure,
    and a significant number of them are written in Python. The network devices that
    underlie a large portion of the CDN are mostly managed by Python applications.
    Such applications track the inventory of our network gear: what devices, of which
    models, with which hardware components, located in which sites. The configuration
    of these devices is controlled by several other systems including source of truth,
    application of configurations to devices, and back up. Device interaction for
    the collection of health and other operational data is yet another Python application.
    Python has long been a popular programming language in the networking space because
    it’s an intuitive language that allows engineers to quickly solve networking problems.
    Subsequently, many useful libraries get developed, making the language even more
    desirable to learn and use.</p><h3>Demand Engineering</h3><p><a href="https://www.linkedin.com/pulse/what-demand-engineering-aaron-blohowiak/">Demand
    Engineering</a> is responsible for <a href="https://opensource.com/article/18/4/how-netflix-does-failovers-7-minutes-flat">Regional
    Failovers</a>, Traffic Distribution, Capacity Operations, and Fleet Efficiency
    of the Netflix cloud. We are proud to say that our team’s tools are built primarily
    in Python. The service that orchestrates failover uses numpy and scipy to perform
    numerical analysis, boto3 to make changes to our AWS infrastructure, rq to run
    asynchronous workloads and we wrap it all up in a thin layer of Flask APIs. The
    ability to drop into a <a href="https://bpython-interpreter.org">bpython</a> shell
    and improvise has saved the day more than once.</p><p>We are heavy users of Jupyter
    Notebooks and <a href="https://nteract.io">nteract</a> to analyze operational
    data and prototype <a href="https://github.com/nteract/nteract/tree/master/packages/data-explorer">visualization
    tools </a>that help us detect capacity regressions.</p><h3>CORE</h3><p>The CORE
    team uses Python in our alerting and statistical analytical work. We lean on many
    of the statistical and mathematical libraries (numpy, scipy, ruptures, pandas)
    to help automate the analysis of 1000s of related signals when our alerting systems
    indicate problems. We’ve developed a time series correlation system used both
    inside and outside the team as well as a distributed worker system to parallelize
    large amounts of analytical work to deliver results quickly.</p><p>Python is also
    a tool we typically use for automation tasks, data exploration and cleaning, and
    as a convenient source for visualization work.</p><h3>Monitoring, alerting and
    auto-remediation</h3><p>The Insight Engineering team is responsible for building
    and operating the tools for operational insight, alerting, diagnostics, and auto-remediation.
    With the increased popularity of Python, the team now supports Python clients
    for most of their services. One example is the <a href="https://github.com/Netflix/spectator-py">Spectator</a>
    Python client library, a library for instrumenting code to record dimensional
    time series metrics. We build Python libraries to interact with other Netflix
    platform level services. In addition to libraries, the <a href="https://medium.com/netflix-techblog/introducing-winston-event-driven-diagnostic-and-remediation-platform-46ce39aa81cc">Winston</a>
    and <a href="https://medium.com/netflix-techblog/introducing-bolt-on-instance-diagnostic-and-remediation-platform-176651b55505">Bolt</a>
    products are also built using Python frameworks (Gunicorn + Flask + Flask-RESTPlus).</p><h3>Information
    Security</h3><p>The information security team uses Python to accomplish a number
    of high leverage goals for Netflix: security automation, risk classification,
    auto-remediation, and vulnerability identification to name a few. We’ve had a
    number of successful Python open sources, including <a href="https://github.com/Netflix/security_monkey">Security
    Monkey</a> (our team’s most active open source project). We leverage Python to
    protect our SSH resources using <a href="https://github.com/Netflix/bless">Bless</a>.
    Our Infrastructure Security team leverages Python to help with IAM permission
    tuning using <a href="https://github.com/Netflix/repokid">Repokid</a>. We use
    Python to help generate TLS certificates using <a href="https://github.com/Netflix/lemur">Lemur</a>.</p><p>Some
    of our more recent projects include Prism: a batch framework to help security
    engineers measure paved road adoption, risk factors, and identify vulnerabilities
    in source code. We currently provide Python and Ruby libraries for Prism. The
    <a href="https://medium.com/netflix-techblog/netflix-sirt-releases-diffy-a-differencing-engine-for-digital-forensics-in-the-cloud-37b71abd2698">Diffy</a>
    forensics triage tool is written entirely in Python. We also use Python to detect
    sensitive data using Lanius.</p><h3>Personalization Algorithms</h3><p>We use Python
    extensively within our broader <a href="https://www.slideshare.net/FaisalZakariaSiddiqi/ml-infra-for-netflix-recommendations-ai-nextcon-talk">Personalization
    Machine Learning Infrastructure</a> to train some of the Machine Learning models
    for key aspects of the Netflix experience: from our <a href="https://research.netflix.com/research-area/recommendations">recommendation
    algorithms</a> to <a href="https://medium.com/netflix-techblog/artwork-personalization-c589f074ad76">artwork
    personalization</a> to <a href="https://medium.com/netflix-techblog/engineering-to-scale-paid-media-campaigns-84ba018fb3fa">marketing
    algorithms</a>. For example, some algorithms use TensorFlow, Keras, and PyTorch
    to learn Deep Neural Networks, XGBoost and LightGBM to learn Gradient Boosted
    Decision Trees or the broader scientific stack in Python (e.g. numpy, scipy, sklearn,
    matplotlib, pandas, cvxpy). Because we’re constantly trying out new approaches,
    we use Jupyter Notebooks to drive many of our experiments. We have also developed
    a number of higher-level libraries to help integrate these with the rest of our
    <a href="https://www.slideshare.net/FaisalZakariaSiddiqi/ml-infra-for-netflix-recommendations-ai-nextcon-talk">ecosystem</a>
    (e.g. data access, fact logging and feature extraction, model evaluation, and
    publishing).</p><h3>Machine Learning Infrastructure</h3><p>Besides personalization,
    Netflix applies machine learning to hundreds of use cases across the company.
    Many of these applications are powered by <a href="https://www.youtube.com/watch?v=XV5VGddmP24">Metaflow</a>,
    a Python framework that makes it easy to execute ML projects from the prototype
    stage to production.</p><p>Metaflow pushes the limits of Python: We leverage well
    parallelized and optimized Python code to fetch data at 10Gbps, handle hundreds
    of millions of data points in memory, and orchestrate computation over tens of
    thousands of CPU cores.</p><h3>Notebooks</h3><p>We are avid users of Jupyter notebooks
    at Netflix, and we’ve written about the <a href="https://medium.com/netflix-techblog/notebook-innovation-591ee3221233">reasons
    and nature of this investment</a> before.</p><p>But Python plays a huge role in
    how we provide those services. Python is a primary language when we need to develop,
    debug, explore, and prototype different interactions with the Jupyter ecosystem.
    We use Python to build custom extensions to the Jupyter server that allows us
    to manage tasks like logging, archiving, publishing, and cloning notebooks on
    behalf of our users.We provide many flavors of Python to our users via different
    Jupyter kernels, and manage the deployment of those kernel specifications using
    Python.</p><h3>Orchestration</h3><p>The Big Data Orchestration team is responsible
    for providing all of the services and tooling to schedule and execute ETL and
    Adhoc pipelines.</p><p>Many of the components of the orchestration service are
    written in Python. Starting with our scheduler, which uses<a href="https://jupyter.org">
    Jupyter Notebooks</a> with <a href="https://papermill.readthedocs.io/en/latest/">papermill</a>
    to provide templatized job types (Spark, Presto, …). This allows our users to
    have a standardized and easy way to express work that needs to be executed. You
    can see some deeper details on the subject <a href="https://medium.com/netflix-techblog/scheduling-notebooks-348e6c14cfd6">here</a>.
    We have been using notebooks as real runbooks for situations where human intervention
    is required — for example: to restart everything that has failed in the last hour.</p><p>Internally,
    we also built an event-driven platform that is fully written in Python. We have
    created streams of events from a number of systems that get unified into a single
    tool. This allows us to define conditions to filter events, and actions to react
    or route them. As a result of this, we have been able to decouple microservices
    and get visibility into everything that happens on the data platform.</p><p>Our
    team also built the <a href="https://github.com/Netflix/pygenie">pygenie</a> client
    which interfaces with <a href="https://netflix.github.io/genie/">Genie</a>, a
    federated job execution service. Internally, we have additional extensions to
    this library that apply business conventions and integrate with the Netflix platform.
    These libraries are the primary way users interface programmatically with work
    in the Big Data platform.</p><p>Finally, it’s been our team’s commitment to contribute
    to <a href="https://papermill.readthedocs.io/en/latest/">papermill</a> and <a
    href="https://nteract-scrapbook.readthedocs.io/en/latest/">scrapbook</a> open
    source projects. Our work there has been both for our own and external use cases.
    These efforts have been gaining a lot of traction in the open source community
    and we’re glad to be able to contribute to these shared projects.</p><h3>Experimentation
    Platform</h3><p>The scientific computing team for experimentation is creating
    a platform for scientists and engineers to analyze AB tests and other experiments.
    Scientists and engineers can contribute new innovations on three fronts, data,
    statistics, and visualizations.</p><p>The Metrics Repo is a Python framework based
    on <a href="https://pypika.readthedocs.io/en/latest/">PyPika</a> that allows contributors
    to write reusable parameterized SQL queries. It serves as an entry point into
    any new analysis.</p><p>The Causal Models library is a Python &amp; R framework
    for scientists to contribute new models for causal inference. It leverages <a
    href="https://arrow.apache.org/docs/python/">PyArrow</a> and <a href="https://rpy2.readthedocs.io/en/version_2.8.x/">RPy2</a>
    so that statistics can be calculated seamlessly in either language.</p><p>The
    Visualizations library is based on <a href="https://plot.ly">Plotly</a>. Since
    Plotly is a widely adopted visualization spec, there are a variety of tools that
    allow contributors to produce an output that is consumable by our platforms.</p><h3>Partner
    Ecosystem</h3><p>The Partner Ecosystem group is expanding its use of Python for
    testing Netflix applications on devices. Python is forming the core of a new CI
    infrastructure, including controlling our orchestration servers, controlling Spinnaker,
    test case querying and filtering, and scheduling test runs on devices and containers.
    Additional post-run analysis is being done in Python using TensorFlow to determine
    which tests are most likely to show problems on which devices.</p><h3>Video Encoding
    and Media Cloud Engineering</h3><p>Our team takes care of encoding (and re-encoding)
    the Netflix catalog, as well as leveraging machine learning for insights into
    that catalog.We use Python for ~50 projects such as <a href="https://github.com/Netflix/vmaf/blob/master/resource/doc/references.md">vmaf</a>
    and <a href="https://medium.com/netflix-techblog/mezzfs-mounting-object-storage-in-netflixs-media-processing-platform-cda01c446ba">mezzfs</a>,
    we build <a href="https://medium.com/netflix-techblog/ava-the-art-and-science-of-image-discovery-at-netflix-a442f163af6">computer
    vision solutions</a> using a media map-reduce platform called <a href="https://medium.com/netflix-techblog/simplifying-media-innovation-at-netflix-with-archer-3f8cbb0e2bcb">Archer</a>,
    and we use Python for many internal projects.We have also open sourced a few tools
    to ease development/distribution of Python projects, like <a href="https://pypi.org/project/setupmeta/">setupmeta</a>
    and <a href="https://pypi.org/project/pickley/">pickley</a>.</p><h3>Netflix Animation
    and NVFX</h3><p>Python is the industry standard for all of the major applications
    we use to create Animated and VFX content, so it goes without saying that we are
    using it very heavily. All of our integrations with Maya and Nuke are in Python,
    and the bulk of our Shotgun tools are also in Python. We’re just getting started
    on getting our tooling in the cloud, and anticipate deploying many of our own
    custom Python AMIs/containers.</p><h3>Content Machine Learning, Science &amp;
    Analytics</h3><p>The Content Machine Learning team uses Python extensively for
    the development of machine learning models that are the core of forecasting audience
    size, viewership, and other demand metrics for all content.</p>'
  :topic_id: 628
- :url: https://netflixtechblog.com/ava-the-art-and-science-of-image-discovery-at-netflix-a442f163af6?source=search_post---------5
  :title: 'AVA: The Art and Science of Image Discovery at Netflix'
  :content: <p>Authored by — Madeline, Lauren, Boris, Tim, Parth, Eugene and Apurva</p><h2>Introduction</h2><p><img
    src="https://cdn-images-1.medium.com/max/5120/1*R-4laTXe2wtJC_cBGUeBSw.jpeg" alt=""></p><p>At
    Netflix, the Content Platform Engineering and Global Product Creative teams know
    that imagery plays an incredibly important role in how viewers find new shows
    and movies to watch. We take pride in surfacing the unique elements of a story
    that connect our audiences to diverse characters and story lines. As our Original
    content slate continues to expand, our technical experts are tasked with finding
    new ways to scale our resources and alleviate our creatives from the tedious and
    ever-increasing demands of digital merchandising. One of the ways in which we
    do this is by harvesting static image frames directly from our source videos to
    provide a more flexible source of raw artwork.</p><h2>The Business Case</h2><p>Merchandising
    stills are static video frames taken directly from the source video content used
    to broaden the reach of a title on the Netflix service. Within a single one-hour
    episode of Stranger Things, there are nearly 86,000 static video frames.</p><p>Traditionally,
    these merchandising stills are selected by human curators or editors, and require
    an in-depth expertise of the source content that they’re intended to represent.
    We know through A/B testing that we can effectively drive increased viewing from
    expected and unexpected audience groups by exploring as many <a href="https://medium.com/netflix-techblog/selecting-the-best-artwork-for-videos-through-a-b-testing-f6155c4595f6">representations
    of a title as possible.</a> When it comes to title key art, we like to test many
    artistic representations of a title in order to find the “right” artwork for the
    right audience. While this presents an exciting opportunity for innovation and
    testing, it simultaneously presents a very challenging expectation to scale this
    experience across every title in our growing global catalog.</p><h2>AVA</h2><p>AVA
    is a collection of tools and algorithms designed to surface high quality imagery
    from the videos on our service. A single season of average TV show (about 10 episodes)
    contains nearly 9 million total frames. Asking creative editors to efficiently
    sift through that many frames of video to identify one frame that will capture
    an audience’s attention is tedious and ineffective. We set out to build a tool
    that quickly and effectively identifies which frames are the best moments to represent
    a title on the Netflix service.</p><p><img src="https://cdn-images-1.medium.com/max/2000/0*hdzpQ8SvoOTcoVOF."
    alt=""></p><p>To achieve this goal, we first came up with objective signals that
    we can measure for each and every frame of the video using Frame Annotations.
    As result, we can collect an effective representation of each frame of the video.
    Subsequently, we created ranking algorithms that allows us to rank a subset of
    frames that meets aesthetic, creative and diversity objectives to represent content
    accurately for various canvases of our product.</p><p><img src="https://cdn-images-1.medium.com/max/2526/0*JX7Jvs3Sw77rWDy2."
    alt="*Image candidates sourced and selected by AVA for ‘Bright’*"><strong>Image
    candidates sourced and selected by AVA for ‘Bright’</strong></p><p><img src="https://cdn-images-1.medium.com/max/2000/0*2XsXoykvK7FG3ty9."
    alt="High level stages from source video to editorial image candidates"><em>High
    level stages from source video to editorial image candidates</em></p><h2>Frame
    Annotation</h2><p>As part of our automation pipeline, we process and annotate
    many different variables on each individual frame of video to best derive what
    the frame contains, and to understand why it is or isn’t important to the story.
    In order to scale horizontally and have predictable SLA for a growing catalog
    of content, we utilized the <a href="https://atscaleconference.com/videos/archer-a-distributed-computing-platform-for-media-processing/">Archer
    </a>framework to process our videos more efficiently. Archer allowed us to split
    the videos into smaller sized chunks that could each be processed in parallel.
    This has enabled us to scale by lending efficiency to our video processing pipelines,
    and allowing us to integrate more and more content intelligence algorithms into
    our tool sets.</p><p><img src="https://cdn-images-1.medium.com/max/2000/0*AuzFOe4Q4znOWVEC."
    alt=""></p><p>Every frame of video in a piece of content is processed through
    a series of computer vision algorithms to gather objective frame metadata, latent
    representation of frame, as well as some of the contextual metadata that those
    frame(s) contain. The annotation properties that we process and apply to our video
    frames can be roughly grouped into 3 main categories:</p><h3>Visual Metadata</h3><p>Typically
    these properties are objective, measurable, and mostly contained at the pixel-level.
    Some examples of visual properties are brightness, color, contrast, and motion
    blur.</p><p><img src="https://cdn-images-1.medium.com/max/3200/0*6JN93Hwllw5SbE7r."
    alt="*Examples of some of the visual properties we capture at frame level.*"><strong>Examples
    of some of the visual properties we capture at frame level.</strong></p><h3>Contextual
    Metadata</h3><p>Contextual metadata is comprised of a combination of elements
    that are aggregated to derive meaning from the actions or movement of the actors,
    objects and camera in the frame. Some examples include;</p><ul><li><p><strong>Face
    detection</strong> with facial landmarks tracking, pose estimation, and sentiment
    analysis — This allows us to estimate posture and sentiment of the subjects in
    the frame.</p></li><li><p><strong>Motion estimation</strong> — This allows us
    to estimate the amount of motion (both camera movement and subject movement) contained
    within a particular shot. This allows us to control for elements such as motion
    blur, as well as to identify camera movement that makes for compelling still imagery.</p></li><li><p><strong>Camera
    shot identification</strong> — (e.g. close up shot vs. dolly shot) This provides
    insight into the intentions of the cinematographer, allowing us to quickly identify
    and surface stylistic camera choices that provide insight into the mood, tone
    and genre of the title.</p></li><li><p><strong>Object detection</strong> — The
    detection of props and animated object segmentation allow us to attribute importance
    to non-human subjects in the frame.</p></li></ul><p><img src="https://cdn-images-1.medium.com/max/2000/0*fRgpOHd60Zs-qE7-."
    alt="*Examples of facial landmarks and pose estimation; some of factors we use
    to detect when characters in frame have compelling facial expressions.*"><strong>Examples
    of facial landmarks and pose estimation; some of factors we use to detect when
    characters in frame have compelling facial expressions.</strong></p><p><img src="https://cdn-images-1.medium.com/max/2000/1*GaNbrMmBBn_8U7ebYr_I1A.gif"
    alt="*Example of optical flow analysis to predict camera motion to estimate the
    shot types (zoom-out and panning shots) of Black Mirror.*"><strong>Example of
    optical flow analysis to predict camera motion to estimate the shot types (zoom-out
    and panning shots) of Black Mirror.</strong></p><h3>Composition Metadata</h3><p>Composition
    metadata refers to a special set of heuristic characteristics that we’ve identified
    and defined based on some of the core principles in photography, cinematography
    and visual aesthetic design. Some examples of composition are rule-of-third, depth-of-field
    and symmetry.</p><p><img src="https://cdn-images-1.medium.com/max/8000/1*30vxgwzO6NiLdECfWC6X9Q.jpeg"
    alt="*Example of object detection and semantic segmentation to identify foreground
    object following rule-of-third aesthetics.*"><strong>Example of object detection
    and semantic segmentation to identify foreground object following rule-of-third
    aesthetics.</strong></p><h2>Image Ranking</h2><p>After we’ve processed and annotated
    every frame in a given video, the next step is to surface “the best” image candidates
    from those frames through an automated artwork pipeline. That way, when our creative
    teams are ready to begin work for a piece of content, they are automatically provided
    with a high quality image set to choose from. Below, we outline some of the key
    elements we use to surface the best images for a given title.</p><p><strong>Actors</strong></p><p>Actors
    play a very important role in artwork. One way we identify the key character for
    a given episode is by utilizing a combination of face clustering and actor recognition
    to prioritize main characters and de-prioritize secondary characters or extras.
    To accomplish this, we trained a deep-learning model to trace facial similarities
    from all qualifying candidate frames tagged with frame annotation to surface and
    rank the main actors of a given title without knowing anything about the cast
    members.</p><p>Beyond cast, we also take into account pose, facial landmarks,
    and the overall position of characters for a given cast member.</p><p><img src="https://cdn-images-1.medium.com/max/2000/0*2hfTvgGuXe15FUUc."
    alt="*Example of actor clusters, frame ranking and optimal selection for Wynona
    Ryder as Joyce Byers.*"><strong>Example of actor clusters, frame ranking and optimal
    selection for Wynona Ryder as Joyce Byers.</strong></p><p><img src="https://cdn-images-1.medium.com/max/2346/0*7Xhx8y_5jfeJ3VjC."
    alt="*Example of imagery that are ranked lower due to sub-optimal facial expression,
    pose and motion blurs*"><strong>Example of imagery that are ranked lower due to
    sub-optimal facial expression, pose and motion blurs</strong></p><p><strong>Frame
    Diversity</strong></p><p>Creative and visual diversity is a highly subjective
    discipline, as there are many different ways to perceive and define diversity
    in imagery. In the context of this solution, image diversity more specifically
    refers to the algorithms ability to capture the heuristic variance that naturally
    occurs within a single movie or episode. In doing so, we hope to provide designers
    and creatives with a scalable mechanism to quickly understand which visual elements
    are most representative of the title, and which elements are misrepresentative
    of the title. Some of the visual heuristic variables that we’ve incorporated into
    AVA to surface a diverse image set for a title include elements such as <strong>camera
    shot types</strong> (long shot vs medium shot), <strong>visual similarity</strong>
    (rule of thirds, brightness, contrast), <strong>color</strong> (colors that are
    most prominent), and <strong>saliency maps</strong> (to identify negative space
    and complexity). By combining these heuristic variables, we can effectively cluster
    image frames based on a custom vector for diversity. Furthermore, by incorporating
    several vectors, we’re able to construct a diversity index against which all candidate
    imagery for a given episode or movie, can be scored.</p><p><img src="https://cdn-images-1.medium.com/max/2604/0*s3H0tZLBWgjv2TUP."
    alt="*Example of AVA’s shot detection variance; (left) medium shot, (center) close-up,
    (right) extreme close-up.*"><strong>Example of AVA’s shot detection variance;
    (left) medium shot, (center) close-up, (right) extreme close-up.</strong></p><p><strong>Filters
    for Maturity</strong></p><p>For content sensitivity and audience maturity reasons,
    we also needed to make sure we excluded frames containing harmful or offensive
    elements. Examples of editorial exclusion criteria are things like; sex/nudity,
    text, logos/unauthorized branding, and violence/gore. In order to de-prioritize
    frames containing these elements, we incorporated the probability of each of these
    variables as vectors, allowing us to quantify and ultimately attribute a lower
    score for these frames.</p><p>We additionally included elements such as title
    genre, content format, maturity rating, etc. as secondary elements or minor features
    and as feedback to the model for ranking prediction.</p><h2>Conclusion</h2><p>In
    this techblog, we’ve provided an overview of our unique approach to surfacing
    meaningful images from video and enabling our creative teams to design stunning
    artwork every single day. AVA is a collection of tools and algorithms encapsulating
    the key intersections of computer vision combined with the core principles of
    filmmaking and photo editing.</p><p>Stay tuned for a follow up blog in which we’ll
    dive into programmatic artwork composition, an exciting new solution that’s responsible
    for much of the artwork you see on the Netflix service today!</p><p>Thank you.</p><p>If
    you have great or innovative ideas come join us on the <a href="https://jobs.netflix.com/jobs/866146">Content
    Platform Engineering team!</a></p>
  :author: <p>Authored by — Madeline, Lauren, Boris, Tim, Parth, Eugene and Apurva</p><h2>Introduction</h2><p><img
    src="https://cdn-images-1.medium.com/max/5120/1*R-4laTXe2wtJC_cBGUeBSw.jpeg" alt=""></p><p>At
    Netflix, the Content Platform Engineering and Global Product Creative teams know
    that imagery plays an incredibly important role in how viewers find new shows
    and movies to watch. We take pride in surfacing the unique elements of a story
    that connect our audiences to diverse characters and story lines. As our Original
    content slate continues to expand, our technical experts are tasked with finding
    new ways to scale our resources and alleviate our creatives from the tedious and
    ever-increasing demands of digital merchandising. One of the ways in which we
    do this is by harvesting static image frames directly from our source videos to
    provide a more flexible source of raw artwork.</p><h2>The Business Case</h2><p>Merchandising
    stills are static video frames taken directly from the source video content used
    to broaden the reach of a title on the Netflix service. Within a single one-hour
    episode of Stranger Things, there are nearly 86,000 static video frames.</p><p>Traditionally,
    these merchandising stills are selected by human curators or editors, and require
    an in-depth expertise of the source content that they’re intended to represent.
    We know through A/B testing that we can effectively drive increased viewing from
    expected and unexpected audience groups by exploring as many <a href="https://medium.com/netflix-techblog/selecting-the-best-artwork-for-videos-through-a-b-testing-f6155c4595f6">representations
    of a title as possible.</a> When it comes to title key art, we like to test many
    artistic representations of a title in order to find the “right” artwork for the
    right audience. While this presents an exciting opportunity for innovation and
    testing, it simultaneously presents a very challenging expectation to scale this
    experience across every title in our growing global catalog.</p><h2>AVA</h2><p>AVA
    is a collection of tools and algorithms designed to surface high quality imagery
    from the videos on our service. A single season of average TV show (about 10 episodes)
    contains nearly 9 million total frames. Asking creative editors to efficiently
    sift through that many frames of video to identify one frame that will capture
    an audience’s attention is tedious and ineffective. We set out to build a tool
    that quickly and effectively identifies which frames are the best moments to represent
    a title on the Netflix service.</p><p><img src="https://cdn-images-1.medium.com/max/2000/0*hdzpQ8SvoOTcoVOF."
    alt=""></p><p>To achieve this goal, we first came up with objective signals that
    we can measure for each and every frame of the video using Frame Annotations.
    As result, we can collect an effective representation of each frame of the video.
    Subsequently, we created ranking algorithms that allows us to rank a subset of
    frames that meets aesthetic, creative and diversity objectives to represent content
    accurately for various canvases of our product.</p><p><img src="https://cdn-images-1.medium.com/max/2526/0*JX7Jvs3Sw77rWDy2."
    alt="*Image candidates sourced and selected by AVA for ‘Bright’*"><strong>Image
    candidates sourced and selected by AVA for ‘Bright’</strong></p><p><img src="https://cdn-images-1.medium.com/max/2000/0*2XsXoykvK7FG3ty9."
    alt="High level stages from source video to editorial image candidates"><em>High
    level stages from source video to editorial image candidates</em></p><h2>Frame
    Annotation</h2><p>As part of our automation pipeline, we process and annotate
    many different variables on each individual frame of video to best derive what
    the frame contains, and to understand why it is or isn’t important to the story.
    In order to scale horizontally and have predictable SLA for a growing catalog
    of content, we utilized the <a href="https://atscaleconference.com/videos/archer-a-distributed-computing-platform-for-media-processing/">Archer
    </a>framework to process our videos more efficiently. Archer allowed us to split
    the videos into smaller sized chunks that could each be processed in parallel.
    This has enabled us to scale by lending efficiency to our video processing pipelines,
    and allowing us to integrate more and more content intelligence algorithms into
    our tool sets.</p><p><img src="https://cdn-images-1.medium.com/max/2000/0*AuzFOe4Q4znOWVEC."
    alt=""></p><p>Every frame of video in a piece of content is processed through
    a series of computer vision algorithms to gather objective frame metadata, latent
    representation of frame, as well as some of the contextual metadata that those
    frame(s) contain. The annotation properties that we process and apply to our video
    frames can be roughly grouped into 3 main categories:</p><h3>Visual Metadata</h3><p>Typically
    these properties are objective, measurable, and mostly contained at the pixel-level.
    Some examples of visual properties are brightness, color, contrast, and motion
    blur.</p><p><img src="https://cdn-images-1.medium.com/max/3200/0*6JN93Hwllw5SbE7r."
    alt="*Examples of some of the visual properties we capture at frame level.*"><strong>Examples
    of some of the visual properties we capture at frame level.</strong></p><h3>Contextual
    Metadata</h3><p>Contextual metadata is comprised of a combination of elements
    that are aggregated to derive meaning from the actions or movement of the actors,
    objects and camera in the frame. Some examples include;</p><ul><li><p><strong>Face
    detection</strong> with facial landmarks tracking, pose estimation, and sentiment
    analysis — This allows us to estimate posture and sentiment of the subjects in
    the frame.</p></li><li><p><strong>Motion estimation</strong> — This allows us
    to estimate the amount of motion (both camera movement and subject movement) contained
    within a particular shot. This allows us to control for elements such as motion
    blur, as well as to identify camera movement that makes for compelling still imagery.</p></li><li><p><strong>Camera
    shot identification</strong> — (e.g. close up shot vs. dolly shot) This provides
    insight into the intentions of the cinematographer, allowing us to quickly identify
    and surface stylistic camera choices that provide insight into the mood, tone
    and genre of the title.</p></li><li><p><strong>Object detection</strong> — The
    detection of props and animated object segmentation allow us to attribute importance
    to non-human subjects in the frame.</p></li></ul><p><img src="https://cdn-images-1.medium.com/max/2000/0*fRgpOHd60Zs-qE7-."
    alt="*Examples of facial landmarks and pose estimation; some of factors we use
    to detect when characters in frame have compelling facial expressions.*"><strong>Examples
    of facial landmarks and pose estimation; some of factors we use to detect when
    characters in frame have compelling facial expressions.</strong></p><p><img src="https://cdn-images-1.medium.com/max/2000/1*GaNbrMmBBn_8U7ebYr_I1A.gif"
    alt="*Example of optical flow analysis to predict camera motion to estimate the
    shot types (zoom-out and panning shots) of Black Mirror.*"><strong>Example of
    optical flow analysis to predict camera motion to estimate the shot types (zoom-out
    and panning shots) of Black Mirror.</strong></p><h3>Composition Metadata</h3><p>Composition
    metadata refers to a special set of heuristic characteristics that we’ve identified
    and defined based on some of the core principles in photography, cinematography
    and visual aesthetic design. Some examples of composition are rule-of-third, depth-of-field
    and symmetry.</p><p><img src="https://cdn-images-1.medium.com/max/8000/1*30vxgwzO6NiLdECfWC6X9Q.jpeg"
    alt="*Example of object detection and semantic segmentation to identify foreground
    object following rule-of-third aesthetics.*"><strong>Example of object detection
    and semantic segmentation to identify foreground object following rule-of-third
    aesthetics.</strong></p><h2>Image Ranking</h2><p>After we’ve processed and annotated
    every frame in a given video, the next step is to surface “the best” image candidates
    from those frames through an automated artwork pipeline. That way, when our creative
    teams are ready to begin work for a piece of content, they are automatically provided
    with a high quality image set to choose from. Below, we outline some of the key
    elements we use to surface the best images for a given title.</p><p><strong>Actors</strong></p><p>Actors
    play a very important role in artwork. One way we identify the key character for
    a given episode is by utilizing a combination of face clustering and actor recognition
    to prioritize main characters and de-prioritize secondary characters or extras.
    To accomplish this, we trained a deep-learning model to trace facial similarities
    from all qualifying candidate frames tagged with frame annotation to surface and
    rank the main actors of a given title without knowing anything about the cast
    members.</p><p>Beyond cast, we also take into account pose, facial landmarks,
    and the overall position of characters for a given cast member.</p><p><img src="https://cdn-images-1.medium.com/max/2000/0*2hfTvgGuXe15FUUc."
    alt="*Example of actor clusters, frame ranking and optimal selection for Wynona
    Ryder as Joyce Byers.*"><strong>Example of actor clusters, frame ranking and optimal
    selection for Wynona Ryder as Joyce Byers.</strong></p><p><img src="https://cdn-images-1.medium.com/max/2346/0*7Xhx8y_5jfeJ3VjC."
    alt="*Example of imagery that are ranked lower due to sub-optimal facial expression,
    pose and motion blurs*"><strong>Example of imagery that are ranked lower due to
    sub-optimal facial expression, pose and motion blurs</strong></p><p><strong>Frame
    Diversity</strong></p><p>Creative and visual diversity is a highly subjective
    discipline, as there are many different ways to perceive and define diversity
    in imagery. In the context of this solution, image diversity more specifically
    refers to the algorithms ability to capture the heuristic variance that naturally
    occurs within a single movie or episode. In doing so, we hope to provide designers
    and creatives with a scalable mechanism to quickly understand which visual elements
    are most representative of the title, and which elements are misrepresentative
    of the title. Some of the visual heuristic variables that we’ve incorporated into
    AVA to surface a diverse image set for a title include elements such as <strong>camera
    shot types</strong> (long shot vs medium shot), <strong>visual similarity</strong>
    (rule of thirds, brightness, contrast), <strong>color</strong> (colors that are
    most prominent), and <strong>saliency maps</strong> (to identify negative space
    and complexity). By combining these heuristic variables, we can effectively cluster
    image frames based on a custom vector for diversity. Furthermore, by incorporating
    several vectors, we’re able to construct a diversity index against which all candidate
    imagery for a given episode or movie, can be scored.</p><p><img src="https://cdn-images-1.medium.com/max/2604/0*s3H0tZLBWgjv2TUP."
    alt="*Example of AVA’s shot detection variance; (left) medium shot, (center) close-up,
    (right) extreme close-up.*"><strong>Example of AVA’s shot detection variance;
    (left) medium shot, (center) close-up, (right) extreme close-up.</strong></p><p><strong>Filters
    for Maturity</strong></p><p>For content sensitivity and audience maturity reasons,
    we also needed to make sure we excluded frames containing harmful or offensive
    elements. Examples of editorial exclusion criteria are things like; sex/nudity,
    text, logos/unauthorized branding, and violence/gore. In order to de-prioritize
    frames containing these elements, we incorporated the probability of each of these
    variables as vectors, allowing us to quantify and ultimately attribute a lower
    score for these frames.</p><p>We additionally included elements such as title
    genre, content format, maturity rating, etc. as secondary elements or minor features
    and as feedback to the model for ranking prediction.</p><h2>Conclusion</h2><p>In
    this techblog, we’ve provided an overview of our unique approach to surfacing
    meaningful images from video and enabling our creative teams to design stunning
    artwork every single day. AVA is a collection of tools and algorithms encapsulating
    the key intersections of computer vision combined with the core principles of
    filmmaking and photo editing.</p><p>Stay tuned for a follow up blog in which we’ll
    dive into programmatic artwork composition, an exciting new solution that’s responsible
    for much of the artwork you see on the Netflix service today!</p><p>Thank you.</p><p>If
    you have great or innovative ideas come join us on the <a href="https://jobs.netflix.com/jobs/866146">Content
    Platform Engineering team!</a></p>
  :topic_id: 628
- :url: https://medium.learningbyshipping.com/nikon-versus-canon-a-story-of-technology-change-45777098038c?source=search_post---------6
  :title: 'Nikon versus Canon: A Story Of Technology Change'
  :content: "<p>Nikon losing the leadership position to Canon in the market for professional
    photography is a story involving technology transitions over a period of 30 or
    more years as told in this annotated twitter thread.</p><p><em>Annotations in
    italics.</em></p><p>The Canon F-1 mechanical film camera from 1971 had over 10,000
    parts. It was designed to compete with the Nikon F released 12 years earlier (with
    fewer parts) and despite the complexity was not really a competitive risk.</p><p><img
    src=\"https://cdn-images-1.medium.com/max/2000/0*W_Dtl2RznCj2QoN_.jpg\" alt=\"Schematic
    of the Canon F-1.\"><em>Schematic of the Canon F-1.</em></p><p><img src=\"https://cdn-images-1.medium.com/max/2000/0*A1kDhBbXAqV2E8vq.jpg\"
    alt=\"This is an artistic schematic of the Nikon F. This is so amazing and can
    be found on shirts and prints on Esty!\"><em>This is an artistic schematic of
    the Nikon F. This is so amazing and can be found on shirts and prints on Esty!</em></p><p><em>I
    was asked in a follow-up where the 10,000 parts number came from. Here’s the 1971
    brochure from Canon Japan with this tagline. Interestingly it is clearly hyperbole
    and also a tricky Japanese translation (does it means the system or the camera?).</em></p><p><img
    src=\"https://cdn-images-1.medium.com/max/2000/1*z95BFXr1KmFjlmpiT4n2Sg.jpeg\"
    alt=\"\"></p><p><img src=\"https://cdn-images-1.medium.com/max/2094/1*Jj2Qo7pT2ZrLz3U9roC7TA.jpeg\"
    alt=\"Canon F-1 brochure from 1971 showing 10,000 parts.\"><em>Canon F-1 brochure
    from 1971 showing 10,000 parts.</em></p><p><img src=\"https://cdn-images-1.medium.com/max/2048/1*E-3mPNVazwK-I8Nrcn_uvg.jpeg\"
    alt=\"Another English-language translation of Canon’s hyperbole/poor translation
    :-)\"><em>Another English-language translation of Canon’s hyperbole/poor translation
    :-)</em></p><p>The Canon “system” (1) was designed to compete with the Nikon F
    system, which had 10 years of ecosystem growth.</p><p>Less than a year after Canon,
    Nikon released the culmination of a 5 year project, the F2 (2), which overwhelmed
    Canon which barely resembled 1962 Nikon System (3).</p><p><img src=\"https://cdn-images-1.medium.com/max/2000/0*_QJLFLIBfb_r9NPQ.jpg\"
    alt=\"(1) This is the Canon F-1 system from 1971 at launch.\"><em>(1) This is
    the Canon F-1 system from 1971 at launch.</em></p><p><img src=\"https://cdn-images-1.medium.com/max/2000/0*Q9F-JB1N_Nwvio0L.jpg\"
    alt=\"(2) Released a few months later this is the Nikon F2 System. It is simply
    overwhelming. Lenses ranged from 6mm (giant round one on the left edge with 220°
    field of view) to 2000mm (upper left telescope looking thing, the longest camera
    lens ever). There were accessories yet to envisioned by Canon.\"><em>(2) Released
    a few months later this is the Nikon F2 System. It is simply overwhelming. Lenses
    ranged from 6mm (giant round one on the left edge with 220° field of view) to
    2000mm (upper left telescope looking thing, the longest camera lens ever). There
    were accessories yet to envisioned by Canon.</em></p><p><img src=\"https://cdn-images-1.medium.com/max/2400/0*ZYV2q3XLCC6U5YKU.jpg\"
    alt=\"(3) This is the 1962 Nikon F System. As you can see the Canon F-1 system
    from 1971 is about the same scope as this system. This was a case of Canon skating
    to where the puck *was*.\"><em>(3) This is the 1962 Nikon F System. As you can
    see the Canon F-1 system from 1971 is about the same scope as this system. This
    was a case of Canon skating to where the puck *was</em>.*</p><p>Nikon took a very
    conservative approach to adding features having secured (and defended) the professional
    market. 8 years later they released the Nikon F3 which <em>required</em> batteries
    for the <em>first time</em>, but even had a backup mechanical shutter release
    that was heavily marketed. <em>As crazy as it sounds, professionals, especially
    National Geographic types on assignment, were terrified of running out of batteries
    so Nikon catered to them. My first camera on my own was an F3 and all the reviews
    (which I memorized) at the time mentioned this manual shutter release.</em></p><p><img
    src=\"https://cdn-images-1.medium.com/max/2000/1*vFy5pN12_U7WFokyNPIp-w.jpeg\"
    alt=\"\"></p><p><img src=\"https://cdn-images-1.medium.com/max/2000/1*ll4hScohN7xQVElOzLSvOg.jpeg\"
    alt=\"Here is a launch review of the F3 from 1980 and you can see how much ink
    was devoted to batteries! This isn’t everything as there is a whole lot more about
    how the motor drive took 8 AA batteries that could power the camera, but they
    don’t work in cold weather and…\"><em>Here is a launch review of the F3 from 1980
    and you can see how much ink was devoted to batteries! This isn’t everything as
    there is a whole lot more about how the motor drive took 8 AA batteries that could
    power the camera, but they don’t work in cold weather and…</em></p><p><img src=\"https://cdn-images-1.medium.com/max/2000/0*oMiGfTBQKBdL9Tdx.jpg\"
    alt=\"Nikon’s broad advertising was relentless but focused almost entirely on
    professionals. The basic view was, it seems, to sell consumer cameras because
    the professionals used them. The little button/lever below the chrome one on the
    left is the emergency manual shutter release.\"><em>Nikon’s broad advertising
    was relentless but focused almost entirely on professionals. The basic view was,
    it seems, to sell consumer cameras because the professionals used them. The little
    button/lever below the chrome one on the left is the emergency manual shutter
    release.</em></p><p><img src=\"https://cdn-images-1.medium.com/max/2106/1*hiuUfWq0OOg4OxjIJDi4gA.png\"
    alt=\"By contrast, Canon used tried and true celebrity endorsement from non-photographers
    (mostly sports) highlighting ease of use and broad appeal.\"><em>By contrast,
    Canon used tried and true celebrity endorsement from non-photographers (mostly
    sports) highlighting ease of use and broad appeal.</em></p><p>All along Nikon
    was incredibly innovative with consumer cameras where they felt they could “experiment”.
    Nikon released dozens of models with automatic exposure, fancy metering, integrated
    motor drives, and eventually even auto focus in 1986. Many more consumer models
    than pro!</p><p><img src=\"https://cdn-images-1.medium.com/max/2000/0*U7N8PP7gkWOJD4g-.jpg\"
    alt=\"Nikon had an incredibly broad range of cameras with a large number of consumer
    models but their focus and business was really about professionals where they
    did not really expand the model lineup rapidly or broadly. The Nikon F was 1959;
    F2 in 1971; F3 in 1980; F4 (autofocus) 1988; and on the heels of the digital revolution
    the F5 in 1996.\"><em>Nikon had an incredibly broad range of cameras with a large
    number of consumer models but their focus and business was really about professionals
    where they did not really expand the model lineup rapidly or broadly. The Nikon
    F was 1959; F2 in 1971; F3 in 1980; F4 (autofocus) 1988; and on the heels of the
    digital revolution the F5 in 1996.</em></p><p><img src=\"https://cdn-images-1.medium.com/max/2000/1*-zeSdsb6An23bKNr_nEBzg.png\"
    alt=\"This is from the Tokyo Olympics (I believe) in 1964. There are only Nikon
    cameras and notice the uniformity. By the way, the lenses with the chrome ring
    are an 85–250mm lens, the equivalent of today’s 80–200 professional zoom. This
    was introduced in 1959 and was the first commercial zoom lens from Japanese makers.\"><em>This
    is from the Tokyo Olympics (I believe) in 1964. There are only Nikon cameras and
    notice the uniformity. By the way, the lenses with the chrome ring are an 85–250mm
    lens, the equivalent of today’s 80–200 professional zoom. This was introduced
    in 1959 and was the first commercial zoom lens from Japanese makers.</em></p><p><img
    src=\"https://cdn-images-1.medium.com/max/2508/1*4EKvQD8PwMU52un1b3ZvGQ.png\"
    alt=\"Nikon was broadly associated with photography and of course was mentioned
    in Simon &amp; Garfunkel’s “Kodachrome”…”I got a Nikon camera/I love to take a
    photograph /So mama don’t take my Kodachrome away”. This is a gallery of 1960s
    and 1970s celebrities just taking pictures with Nikons.\"><em>Nikon was broadly
    associated with photography and of course was mentioned in Simon &amp; Garfunkel’s
    “Kodachrome”…”I got a Nikon camera/I love to take a photograph /So mama don’t
    take my Kodachrome away”. This is a gallery of 1960s and 1970s celebrities just
    taking pictures with Nikons.</em></p><p><img src=\"https://cdn-images-1.medium.com/max/2000/1*lK7IbQL_v3BxM-JGsJwxRQ.png\"
    alt=\"Into the 1980s, Nikon was the only choice for professionals. This is a shot
    of a gallery of sports photographers (at a track of some type).\"><em>Into the
    1980s, Nikon was the only choice for professionals. This is a shot of a gallery
    of sports photographers (at a track of some type).</em></p><p>Also Nikon was very
    busy working with Kodak on pioneering digital cameras. All through the 1990s there
    were incredible advances that relied on Nikon camera bodies and optics and Kodak
    digital sensors.</p><p><img src=\"https://cdn-images-1.medium.com/max/2400/0*iOLUcXLVxZnyR9xn.jpg\"
    alt=\"This is an early prototype of a digital camera based on a Nikon camera body
    with a Kodak sensor in the camera and all the image capture and translation done
    in the off board box connected by cables. This one, I believe had a 1MB monochrome
    image and took about 30 seconds to capture. It was revolutionary in 1995 or so.\"><em>This
    is an early prototype of a digital camera based on a Nikon camera body with a
    Kodak sensor in the camera and all the image capture and translation done in the
    off board box connected by cables. This one, I believe had a 1MB monochrome image
    and took about 30 seconds to capture. It was revolutionary in 1995 or so.</em></p><p>Nikon
    was even first to release professional digital cameras with the much anticipated
    Nikon D-1 in 1999.</p><p><img src=\"https://cdn-images-1.medium.com/max/2176/0*Tw88wefmtKZtweAI.jpg\"
    alt=\"Nikon’s D-1 pioneered the fully integrated and well-designed SLR body that
    is had been evolving since the F3. Nikon resisted digital broadly for products
    because professionals were skeptical of the quality. Digital camera sensors captured
    much less resolution than film at the time and there were endless arguments about
    compression. This is ironic because so much of photography had used film “grain”
    (essentially compression artifacts) as an artistic element. Professionals really
    resisted but this camera was revolutionary for daily news photographers (product-market
    fit!). I can tell lots of stories from news photog friends and my own experiences
    developing film in hotel rooms and closets. Crazy to think of, and they were happy
    to move on! To compromise, this sensor introduced the smaller frame size which
    meant lenses behaved with different fields of view, further confusing professionals
    and because frame size=quality in film it further “amateur-ized” the camera.\"><em>Nikon’s
    D-1 pioneered the fully integrated and well-designed SLR body that is had been
    evolving since the F3. Nikon resisted digital broadly for products because professionals
    were skeptical of the quality. Digital camera sensors captured much less resolution
    than film at the time and there were endless arguments about compression. This
    is ironic because so much of photography had used film “grain” (essentially compression
    artifacts) as an artistic element. Professionals really resisted but this camera
    was revolutionary for daily news photographers (product-market fit!). I can tell
    lots of stories from news photog friends and my own experiences developing film
    in hotel rooms and closets. Crazy to think of, and they were happy to move on!
    To compromise, this sensor introduced the smaller frame size which meant lenses
    behaved with different fields of view, further confusing professionals and because
    frame size=quality in film it further “amateur-ized” the camera.</em></p><p>Canon
    could not win over pros. Nikon was innovating but was still all about Pros being
    in “control” and “conservative”, eg every lens from 1959 forward still worked
    on current cameras. <a href=\"https://threadreaderapp.com/hashtag/professional\">#professional</a></p><p><img
    src=\"https://cdn-images-1.medium.com/max/2000/1*H7MVTa71xaVsy1e-jLb0gw.png\"
    alt=\"*In the photo 5 up of the press gallery with photogs in white caps, this
    is the lens on the left side. It was the first commercial zoom lens from November
    1959 just 3 months after the F launch. Here is that lens mounted on the latest
    Nikon D850 introduced Fall 2017 (59 years later!)*\"><strong>In the photo 5 up
    of the press gallery with photogs in white caps, this is the lens on the left
    side. It was the first commercial zoom lens from November 1959 just 3 months after
    the F launch. Here is that lens mounted on the latest Nikon D850 introduced Fall
    2017 (59 years later!)</strong></p><p>Canon took a bold step and designed a whole
    new lens system around <em>autofocus</em>. <em>It was vastly criticized by professionals
    and the press at the time for breaking compatibility with Canon’s existing FD
    mount (which itself had maintained compatibility over 3 generations of Canon lenses
    going back to the 1960s). This lens mount made driving autofocus much more efficient
    and fast and also easily supported automatic exposure known as “shutter priority”
    and “programmed” where Nikon on professional cameras only supported aperture priority
    until the F4 was released in 1988 (after Canon’s EF mount, but still compatible
    with the F mount) though the 1983 Nikon FA pro-sumer camera supported these automatic
    modes. I think we can draw some fascinating lessons from how professionals react
    to change from this since these patterns repeat and have repeated themselves.</em></p><p><img
    src=\"https://cdn-images-1.medium.com/max/2368/0*pYvdSvh-FUxZEYp_.jpg\" alt=\"This
    is the Canon EF (electronic focus) lens mount introduced in 1987. It broke with
    tradition of maintaining lens compatibility.\"><em>This is the Canon EF (electronic
    focus) lens mount introduced in 1987. It broke with tradition of maintaining lens
    compatibility.</em></p><p>This was 1987! So there was still a long haul. Canon
    focused (ha) on consumers, ease of use, and reliability of electronics and began
    to win with a full range of autofocus lenses and gained expertise with consumer
    cameras.</p><p>They were going after Nikon from the underserved market.</p><p><img
    src=\"https://cdn-images-1.medium.com/max/2400/0*ryZ4GY53dwJAVw_V.jpg\" alt=\"By
    2000’s Canon had built out a massive arsenal of both film and digital cameras.\"><em>By
    2000’s Canon had built out a massive arsenal of both film and digital cameras.</em></p><p>While
    Nikon had invented <em>[productized or commercialized]</em> all the technology
    from electronic metering, to autofocus, to digital, Canon was going to use it
    all to win over professionals while Nikon was concerned about both cannibalization
    and just not serving professionals as well. See where this is going??</p><p>In
    2000, right when Nikon released its Digital Pro SLR, Canon released one as well
    but had their much better autofocus lenses and all that experience, and was also
    willing to sacrifice “quality” for speed. Enter the Canon EOS digital line.</p><p><img
    src=\"https://cdn-images-1.medium.com/max/2400/0*GDiezIYSMhC6keYx.jpg\" alt=\"The
    Canon EOS D30 which was small and compact and not quite up to pro standards. It
    featured Canon’s len’s driven Auto Focus system and a wide range of EF AF lenses.\"><em>The
    Canon EOS D30 which was small and compact and not quite up to pro standards. It
    featured Canon’s len’s driven Auto Focus system and a wide range of EF AF lenses.</em></p><p>Professionals
    photogs, especially sports, were all about speed and saw the advantages of autofocus.
    Canon capitalized with an incredible range of lenses built from the start for
    autofocus and sports, along with these new digital bodies. Also they made their
    lenses white.</p><p><img src=\"https://cdn-images-1.medium.com/max/2400/0*Hvwdj4rTLNcVmJnW.jpg\"
    alt=\"As no doubt everyone is familiar, this is what the sideline and courtside
    and gallery of every press event looks like—it is filled with these iconic white
    barreled Canon lenses. Originally a design statement but being Japanese it was
    justified with a product reason saying that white reflected sunlight and did not
    distort the optics (mostly nonsense).\"><em>As no doubt everyone is familiar,
    this is what the sideline and courtside and gallery of every press event looks
    like—it is filled with these iconic white barreled Canon lenses. Originally a
    design statement but being Japanese it was justified with a product reason saying
    that white reflected sunlight and did not distort the optics (mostly nonsense).</em></p><p><em>From
    1981, when Canon was developing autofocus here is a magazine article about how
    autofocus works. It is telling that they explain how it works from the perspective
    of Canon *</em>even though Nikon productized modern autofocus.***</p><p><img src=\"https://cdn-images-1.medium.com/max/2560/1*DJpPz3YAPuNS04zYYMj1ug.jpeg\"
    alt=\"Autofocus explained in 1980. Note the use of Canon consumer examples in
    lower right.\"><em>Autofocus explained in 1980. Note the use of Canon consumer
    examples in lower right.</em></p><p>Seemingly overnight Canon captured the professional
    market. This is Canon’s system today. While numerically they have about 60% market
    share, they have a much higher mindshare. <em>Disruption happens two ways—first
    slowly and then quickly.</em></p><p><img src=\"https://cdn-images-1.medium.com/max/2400/0*cBoHE2z_-ssI5mtE.jpg\"
    alt=\"Today’s Canon system covers an incredible array. Also worth noting is that
    in the move to digital Canon also became a leader in full motion capture and share
    sensors and lenses with still photography. Today Nikon and Canon are neck and
    neck in lens coverage and quality as well as the specs for bodies and sensors.
    But now Nikon must contend with a UX model baked into professionals.\"><em>Today’s
    Canon system covers an incredible array. Also worth noting is that in the move
    to digital Canon also became a leader in full motion capture and share sensors
    and lenses with still photography. Today Nikon and Canon are neck and neck in
    lens coverage and quality as well as the specs for bodies and sensors. But now
    Nikon must contend with a UX model baked into professionals.</em></p><p>This is
    such a fascinating case of a company, Nikon, literally inventing the whole category,
    then reinventing it, pioneering every technology but falling victim to its own
    success — focus on existing needs, current limitations, and compatibility. Nikon
    still goes to space though!</p><p><img src=\"https://cdn-images-1.medium.com/max/2400/0*mEoOdMEDpAtPmkIV.jpg\"
    alt=\"Nikon in use aboard the International Space Station.\"><em>Nikon in use
    aboard the International Space Station.</em></p><p><img src=\"https://cdn-images-1.medium.com/max/2332/1*IeR9wTitoEPYDCrQezearw.png\"
    alt=\"While Hasselblad went to the moon, Nikon was in space from the late 1960s
    through a close design partnership with NASA that peaked with massive ad campaigns
    in the 80s. Note the similar broad reach advertising focused on extreme professional
    use that would be used to draw in amateurs and consumers!\"><em>While Hasselblad
    went to the moon, Nikon was in space from the late 1960s through a close design
    partnership with NASA that peaked with massive ad campaigns in the 80s. Note the
    similar broad reach advertising focused on extreme professional use that would
    be used to draw in amateurs and consumers!</em></p><p>This kills me because, well,
    I have been using Nikon since my father gave me his 1971 F and I collect them.
    But I also think about this evolution and how technology incumbents can succumb
    to success. // end</p><p>PS/ Nikon did not invent the 35mm SLR! In fact it got
    into the business by making lenses for existing makers while it focused on professional
    rangefinder cameras (below). *Nikon’s original corporate name was Nippon Kogaku,
    which translates as *Japan Optical.</p><p>That said, the 1959 Nikon system was
    very much like the 1964 IBM 360 in scope, reuse, industry impact.</p><p><img src=\"https://cdn-images-1.medium.com/max/2398/0*llX-8Re9rFHzGLuC.jpg\"
    alt=\"This is an early Nikon F system.\"><em>This is an early Nikon F system.</em></p><p><img
    src=\"https://cdn-images-1.medium.com/max/2400/0*xucS9YafSrwZrMmJ.jpg\" alt=\"This
    is IBM’s view of the 701 computer featuring all the peripherals. IBM and Nikon
    both designed systems from the ground up by bringing together components previously
    invented and combining them into a complete platform/system that revolutionized
    the field. Sensing a pattern?\"><em>This is IBM’s view of the 701 computer featuring
    all the peripherals. IBM and Nikon both designed systems from the ground up by
    bringing together components previously invented and combining them into a complete
    platform/system that revolutionized the field. Sensing a pattern?</em></p><p>PPS/
    In 1971 Canon tried to compete head-on w/Nikon, even with some parity, it offered
    nothing substantial that Nikon didn’t offer and had a much smaller “system” and
    ecosystem. Competing head-on rarely works. Reviews at the time “tried” to make
    it a race, but was’t even close.</p><p><img src=\"https://cdn-images-1.medium.com/max/2000/0*1E62iaAT3zbhoX0b.jpg\"
    alt=\"Nikon got its start designing lenses for the leading rangefinder (versus
    single lens reflex) cameras of the time, Leica. Nikon began to develop its own
    cameras earlier but the reputation came from optics. Nikon’ corporate name in
    Japanese was Nippon Kogaku, which means Japan Optical. Nikon today makes leading
    microscopes and imaging tools of all kinds.\"><em>Nikon got its start designing
    lenses for the leading rangefinder (versus single lens reflex) cameras of the
    time, Leica. Nikon began to develop its own cameras earlier but the reputation
    came from optics. Nikon’ corporate name in Japanese was Nippon Kogaku, which means
    Japan Optical. Nikon today makes leading microscopes and imaging tools of all
    kinds.</em></p><p>PPPS/ Was asked…1950’s Nikon made optics for Leica (can you
    believe!) the leader in rangefinder cameras used by Pros. Nikon clearly learned
    from that and much like China mfgs w/their own consumer brands, they went “first
    party” with the F system. FYI, Sony makes Nikon sensors.\U0001F914 <em>Nikon has
    been slowly displacing Sony sensors and the latest pro and pro-sumer cameras no
    longer use Sony sensors. And fwiw, Canon got into cameras by creating cheaper
    copies of other’s rangefinder cameras and using other company’s lenses. They then
    later moved into making lenses as well. See the pattern…</em></p><p><strong><em>I
    will amend this with corrections I receive since posting anything about Nikon
    history or especially Nikon v. Canon will almost certainly get me in trouble.</em></strong></p>"
  :author: "<p>Nikon losing the leadership position to Canon in the market for professional
    photography is a story involving technology transitions over a period of 30 or
    more years as told in this annotated twitter thread.</p><p><em>Annotations in
    italics.</em></p><p>The Canon F-1 mechanical film camera from 1971 had over 10,000
    parts. It was designed to compete with the Nikon F released 12 years earlier (with
    fewer parts) and despite the complexity was not really a competitive risk.</p><p><img
    src=\"https://cdn-images-1.medium.com/max/2000/0*W_Dtl2RznCj2QoN_.jpg\" alt=\"Schematic
    of the Canon F-1.\"><em>Schematic of the Canon F-1.</em></p><p><img src=\"https://cdn-images-1.medium.com/max/2000/0*A1kDhBbXAqV2E8vq.jpg\"
    alt=\"This is an artistic schematic of the Nikon F. This is so amazing and can
    be found on shirts and prints on Esty!\"><em>This is an artistic schematic of
    the Nikon F. This is so amazing and can be found on shirts and prints on Esty!</em></p><p><em>I
    was asked in a follow-up where the 10,000 parts number came from. Here’s the 1971
    brochure from Canon Japan with this tagline. Interestingly it is clearly hyperbole
    and also a tricky Japanese translation (does it means the system or the camera?).</em></p><p><img
    src=\"https://cdn-images-1.medium.com/max/2000/1*z95BFXr1KmFjlmpiT4n2Sg.jpeg\"
    alt=\"\"></p><p><img src=\"https://cdn-images-1.medium.com/max/2094/1*Jj2Qo7pT2ZrLz3U9roC7TA.jpeg\"
    alt=\"Canon F-1 brochure from 1971 showing 10,000 parts.\"><em>Canon F-1 brochure
    from 1971 showing 10,000 parts.</em></p><p><img src=\"https://cdn-images-1.medium.com/max/2048/1*E-3mPNVazwK-I8Nrcn_uvg.jpeg\"
    alt=\"Another English-language translation of Canon’s hyperbole/poor translation
    :-)\"><em>Another English-language translation of Canon’s hyperbole/poor translation
    :-)</em></p><p>The Canon “system” (1) was designed to compete with the Nikon F
    system, which had 10 years of ecosystem growth.</p><p>Less than a year after Canon,
    Nikon released the culmination of a 5 year project, the F2 (2), which overwhelmed
    Canon which barely resembled 1962 Nikon System (3).</p><p><img src=\"https://cdn-images-1.medium.com/max/2000/0*_QJLFLIBfb_r9NPQ.jpg\"
    alt=\"(1) This is the Canon F-1 system from 1971 at launch.\"><em>(1) This is
    the Canon F-1 system from 1971 at launch.</em></p><p><img src=\"https://cdn-images-1.medium.com/max/2000/0*Q9F-JB1N_Nwvio0L.jpg\"
    alt=\"(2) Released a few months later this is the Nikon F2 System. It is simply
    overwhelming. Lenses ranged from 6mm (giant round one on the left edge with 220°
    field of view) to 2000mm (upper left telescope looking thing, the longest camera
    lens ever). There were accessories yet to envisioned by Canon.\"><em>(2) Released
    a few months later this is the Nikon F2 System. It is simply overwhelming. Lenses
    ranged from 6mm (giant round one on the left edge with 220° field of view) to
    2000mm (upper left telescope looking thing, the longest camera lens ever). There
    were accessories yet to envisioned by Canon.</em></p><p><img src=\"https://cdn-images-1.medium.com/max/2400/0*ZYV2q3XLCC6U5YKU.jpg\"
    alt=\"(3) This is the 1962 Nikon F System. As you can see the Canon F-1 system
    from 1971 is about the same scope as this system. This was a case of Canon skating
    to where the puck *was*.\"><em>(3) This is the 1962 Nikon F System. As you can
    see the Canon F-1 system from 1971 is about the same scope as this system. This
    was a case of Canon skating to where the puck *was</em>.*</p><p>Nikon took a very
    conservative approach to adding features having secured (and defended) the professional
    market. 8 years later they released the Nikon F3 which <em>required</em> batteries
    for the <em>first time</em>, but even had a backup mechanical shutter release
    that was heavily marketed. <em>As crazy as it sounds, professionals, especially
    National Geographic types on assignment, were terrified of running out of batteries
    so Nikon catered to them. My first camera on my own was an F3 and all the reviews
    (which I memorized) at the time mentioned this manual shutter release.</em></p><p><img
    src=\"https://cdn-images-1.medium.com/max/2000/1*vFy5pN12_U7WFokyNPIp-w.jpeg\"
    alt=\"\"></p><p><img src=\"https://cdn-images-1.medium.com/max/2000/1*ll4hScohN7xQVElOzLSvOg.jpeg\"
    alt=\"Here is a launch review of the F3 from 1980 and you can see how much ink
    was devoted to batteries! This isn’t everything as there is a whole lot more about
    how the motor drive took 8 AA batteries that could power the camera, but they
    don’t work in cold weather and…\"><em>Here is a launch review of the F3 from 1980
    and you can see how much ink was devoted to batteries! This isn’t everything as
    there is a whole lot more about how the motor drive took 8 AA batteries that could
    power the camera, but they don’t work in cold weather and…</em></p><p><img src=\"https://cdn-images-1.medium.com/max/2000/0*oMiGfTBQKBdL9Tdx.jpg\"
    alt=\"Nikon’s broad advertising was relentless but focused almost entirely on
    professionals. The basic view was, it seems, to sell consumer cameras because
    the professionals used them. The little button/lever below the chrome one on the
    left is the emergency manual shutter release.\"><em>Nikon’s broad advertising
    was relentless but focused almost entirely on professionals. The basic view was,
    it seems, to sell consumer cameras because the professionals used them. The little
    button/lever below the chrome one on the left is the emergency manual shutter
    release.</em></p><p><img src=\"https://cdn-images-1.medium.com/max/2106/1*hiuUfWq0OOg4OxjIJDi4gA.png\"
    alt=\"By contrast, Canon used tried and true celebrity endorsement from non-photographers
    (mostly sports) highlighting ease of use and broad appeal.\"><em>By contrast,
    Canon used tried and true celebrity endorsement from non-photographers (mostly
    sports) highlighting ease of use and broad appeal.</em></p><p>All along Nikon
    was incredibly innovative with consumer cameras where they felt they could “experiment”.
    Nikon released dozens of models with automatic exposure, fancy metering, integrated
    motor drives, and eventually even auto focus in 1986. Many more consumer models
    than pro!</p><p><img src=\"https://cdn-images-1.medium.com/max/2000/0*U7N8PP7gkWOJD4g-.jpg\"
    alt=\"Nikon had an incredibly broad range of cameras with a large number of consumer
    models but their focus and business was really about professionals where they
    did not really expand the model lineup rapidly or broadly. The Nikon F was 1959;
    F2 in 1971; F3 in 1980; F4 (autofocus) 1988; and on the heels of the digital revolution
    the F5 in 1996.\"><em>Nikon had an incredibly broad range of cameras with a large
    number of consumer models but their focus and business was really about professionals
    where they did not really expand the model lineup rapidly or broadly. The Nikon
    F was 1959; F2 in 1971; F3 in 1980; F4 (autofocus) 1988; and on the heels of the
    digital revolution the F5 in 1996.</em></p><p><img src=\"https://cdn-images-1.medium.com/max/2000/1*-zeSdsb6An23bKNr_nEBzg.png\"
    alt=\"This is from the Tokyo Olympics (I believe) in 1964. There are only Nikon
    cameras and notice the uniformity. By the way, the lenses with the chrome ring
    are an 85–250mm lens, the equivalent of today’s 80–200 professional zoom. This
    was introduced in 1959 and was the first commercial zoom lens from Japanese makers.\"><em>This
    is from the Tokyo Olympics (I believe) in 1964. There are only Nikon cameras and
    notice the uniformity. By the way, the lenses with the chrome ring are an 85–250mm
    lens, the equivalent of today’s 80–200 professional zoom. This was introduced
    in 1959 and was the first commercial zoom lens from Japanese makers.</em></p><p><img
    src=\"https://cdn-images-1.medium.com/max/2508/1*4EKvQD8PwMU52un1b3ZvGQ.png\"
    alt=\"Nikon was broadly associated with photography and of course was mentioned
    in Simon &amp; Garfunkel’s “Kodachrome”…”I got a Nikon camera/I love to take a
    photograph /So mama don’t take my Kodachrome away”. This is a gallery of 1960s
    and 1970s celebrities just taking pictures with Nikons.\"><em>Nikon was broadly
    associated with photography and of course was mentioned in Simon &amp; Garfunkel’s
    “Kodachrome”…”I got a Nikon camera/I love to take a photograph /So mama don’t
    take my Kodachrome away”. This is a gallery of 1960s and 1970s celebrities just
    taking pictures with Nikons.</em></p><p><img src=\"https://cdn-images-1.medium.com/max/2000/1*lK7IbQL_v3BxM-JGsJwxRQ.png\"
    alt=\"Into the 1980s, Nikon was the only choice for professionals. This is a shot
    of a gallery of sports photographers (at a track of some type).\"><em>Into the
    1980s, Nikon was the only choice for professionals. This is a shot of a gallery
    of sports photographers (at a track of some type).</em></p><p>Also Nikon was very
    busy working with Kodak on pioneering digital cameras. All through the 1990s there
    were incredible advances that relied on Nikon camera bodies and optics and Kodak
    digital sensors.</p><p><img src=\"https://cdn-images-1.medium.com/max/2400/0*iOLUcXLVxZnyR9xn.jpg\"
    alt=\"This is an early prototype of a digital camera based on a Nikon camera body
    with a Kodak sensor in the camera and all the image capture and translation done
    in the off board box connected by cables. This one, I believe had a 1MB monochrome
    image and took about 30 seconds to capture. It was revolutionary in 1995 or so.\"><em>This
    is an early prototype of a digital camera based on a Nikon camera body with a
    Kodak sensor in the camera and all the image capture and translation done in the
    off board box connected by cables. This one, I believe had a 1MB monochrome image
    and took about 30 seconds to capture. It was revolutionary in 1995 or so.</em></p><p>Nikon
    was even first to release professional digital cameras with the much anticipated
    Nikon D-1 in 1999.</p><p><img src=\"https://cdn-images-1.medium.com/max/2176/0*Tw88wefmtKZtweAI.jpg\"
    alt=\"Nikon’s D-1 pioneered the fully integrated and well-designed SLR body that
    is had been evolving since the F3. Nikon resisted digital broadly for products
    because professionals were skeptical of the quality. Digital camera sensors captured
    much less resolution than film at the time and there were endless arguments about
    compression. This is ironic because so much of photography had used film “grain”
    (essentially compression artifacts) as an artistic element. Professionals really
    resisted but this camera was revolutionary for daily news photographers (product-market
    fit!). I can tell lots of stories from news photog friends and my own experiences
    developing film in hotel rooms and closets. Crazy to think of, and they were happy
    to move on! To compromise, this sensor introduced the smaller frame size which
    meant lenses behaved with different fields of view, further confusing professionals
    and because frame size=quality in film it further “amateur-ized” the camera.\"><em>Nikon’s
    D-1 pioneered the fully integrated and well-designed SLR body that is had been
    evolving since the F3. Nikon resisted digital broadly for products because professionals
    were skeptical of the quality. Digital camera sensors captured much less resolution
    than film at the time and there were endless arguments about compression. This
    is ironic because so much of photography had used film “grain” (essentially compression
    artifacts) as an artistic element. Professionals really resisted but this camera
    was revolutionary for daily news photographers (product-market fit!). I can tell
    lots of stories from news photog friends and my own experiences developing film
    in hotel rooms and closets. Crazy to think of, and they were happy to move on!
    To compromise, this sensor introduced the smaller frame size which meant lenses
    behaved with different fields of view, further confusing professionals and because
    frame size=quality in film it further “amateur-ized” the camera.</em></p><p>Canon
    could not win over pros. Nikon was innovating but was still all about Pros being
    in “control” and “conservative”, eg every lens from 1959 forward still worked
    on current cameras. <a href=\"https://threadreaderapp.com/hashtag/professional\">#professional</a></p><p><img
    src=\"https://cdn-images-1.medium.com/max/2000/1*H7MVTa71xaVsy1e-jLb0gw.png\"
    alt=\"*In the photo 5 up of the press gallery with photogs in white caps, this
    is the lens on the left side. It was the first commercial zoom lens from November
    1959 just 3 months after the F launch. Here is that lens mounted on the latest
    Nikon D850 introduced Fall 2017 (59 years later!)*\"><strong>In the photo 5 up
    of the press gallery with photogs in white caps, this is the lens on the left
    side. It was the first commercial zoom lens from November 1959 just 3 months after
    the F launch. Here is that lens mounted on the latest Nikon D850 introduced Fall
    2017 (59 years later!)</strong></p><p>Canon took a bold step and designed a whole
    new lens system around <em>autofocus</em>. <em>It was vastly criticized by professionals
    and the press at the time for breaking compatibility with Canon’s existing FD
    mount (which itself had maintained compatibility over 3 generations of Canon lenses
    going back to the 1960s). This lens mount made driving autofocus much more efficient
    and fast and also easily supported automatic exposure known as “shutter priority”
    and “programmed” where Nikon on professional cameras only supported aperture priority
    until the F4 was released in 1988 (after Canon’s EF mount, but still compatible
    with the F mount) though the 1983 Nikon FA pro-sumer camera supported these automatic
    modes. I think we can draw some fascinating lessons from how professionals react
    to change from this since these patterns repeat and have repeated themselves.</em></p><p><img
    src=\"https://cdn-images-1.medium.com/max/2368/0*pYvdSvh-FUxZEYp_.jpg\" alt=\"This
    is the Canon EF (electronic focus) lens mount introduced in 1987. It broke with
    tradition of maintaining lens compatibility.\"><em>This is the Canon EF (electronic
    focus) lens mount introduced in 1987. It broke with tradition of maintaining lens
    compatibility.</em></p><p>This was 1987! So there was still a long haul. Canon
    focused (ha) on consumers, ease of use, and reliability of electronics and began
    to win with a full range of autofocus lenses and gained expertise with consumer
    cameras.</p><p>They were going after Nikon from the underserved market.</p><p><img
    src=\"https://cdn-images-1.medium.com/max/2400/0*ryZ4GY53dwJAVw_V.jpg\" alt=\"By
    2000’s Canon had built out a massive arsenal of both film and digital cameras.\"><em>By
    2000’s Canon had built out a massive arsenal of both film and digital cameras.</em></p><p>While
    Nikon had invented <em>[productized or commercialized]</em> all the technology
    from electronic metering, to autofocus, to digital, Canon was going to use it
    all to win over professionals while Nikon was concerned about both cannibalization
    and just not serving professionals as well. See where this is going??</p><p>In
    2000, right when Nikon released its Digital Pro SLR, Canon released one as well
    but had their much better autofocus lenses and all that experience, and was also
    willing to sacrifice “quality” for speed. Enter the Canon EOS digital line.</p><p><img
    src=\"https://cdn-images-1.medium.com/max/2400/0*GDiezIYSMhC6keYx.jpg\" alt=\"The
    Canon EOS D30 which was small and compact and not quite up to pro standards. It
    featured Canon’s len’s driven Auto Focus system and a wide range of EF AF lenses.\"><em>The
    Canon EOS D30 which was small and compact and not quite up to pro standards. It
    featured Canon’s len’s driven Auto Focus system and a wide range of EF AF lenses.</em></p><p>Professionals
    photogs, especially sports, were all about speed and saw the advantages of autofocus.
    Canon capitalized with an incredible range of lenses built from the start for
    autofocus and sports, along with these new digital bodies. Also they made their
    lenses white.</p><p><img src=\"https://cdn-images-1.medium.com/max/2400/0*Hvwdj4rTLNcVmJnW.jpg\"
    alt=\"As no doubt everyone is familiar, this is what the sideline and courtside
    and gallery of every press event looks like—it is filled with these iconic white
    barreled Canon lenses. Originally a design statement but being Japanese it was
    justified with a product reason saying that white reflected sunlight and did not
    distort the optics (mostly nonsense).\"><em>As no doubt everyone is familiar,
    this is what the sideline and courtside and gallery of every press event looks
    like—it is filled with these iconic white barreled Canon lenses. Originally a
    design statement but being Japanese it was justified with a product reason saying
    that white reflected sunlight and did not distort the optics (mostly nonsense).</em></p><p><em>From
    1981, when Canon was developing autofocus here is a magazine article about how
    autofocus works. It is telling that they explain how it works from the perspective
    of Canon *</em>even though Nikon productized modern autofocus.***</p><p><img src=\"https://cdn-images-1.medium.com/max/2560/1*DJpPz3YAPuNS04zYYMj1ug.jpeg\"
    alt=\"Autofocus explained in 1980. Note the use of Canon consumer examples in
    lower right.\"><em>Autofocus explained in 1980. Note the use of Canon consumer
    examples in lower right.</em></p><p>Seemingly overnight Canon captured the professional
    market. This is Canon’s system today. While numerically they have about 60% market
    share, they have a much higher mindshare. <em>Disruption happens two ways—first
    slowly and then quickly.</em></p><p><img src=\"https://cdn-images-1.medium.com/max/2400/0*cBoHE2z_-ssI5mtE.jpg\"
    alt=\"Today’s Canon system covers an incredible array. Also worth noting is that
    in the move to digital Canon also became a leader in full motion capture and share
    sensors and lenses with still photography. Today Nikon and Canon are neck and
    neck in lens coverage and quality as well as the specs for bodies and sensors.
    But now Nikon must contend with a UX model baked into professionals.\"><em>Today’s
    Canon system covers an incredible array. Also worth noting is that in the move
    to digital Canon also became a leader in full motion capture and share sensors
    and lenses with still photography. Today Nikon and Canon are neck and neck in
    lens coverage and quality as well as the specs for bodies and sensors. But now
    Nikon must contend with a UX model baked into professionals.</em></p><p>This is
    such a fascinating case of a company, Nikon, literally inventing the whole category,
    then reinventing it, pioneering every technology but falling victim to its own
    success — focus on existing needs, current limitations, and compatibility. Nikon
    still goes to space though!</p><p><img src=\"https://cdn-images-1.medium.com/max/2400/0*mEoOdMEDpAtPmkIV.jpg\"
    alt=\"Nikon in use aboard the International Space Station.\"><em>Nikon in use
    aboard the International Space Station.</em></p><p><img src=\"https://cdn-images-1.medium.com/max/2332/1*IeR9wTitoEPYDCrQezearw.png\"
    alt=\"While Hasselblad went to the moon, Nikon was in space from the late 1960s
    through a close design partnership with NASA that peaked with massive ad campaigns
    in the 80s. Note the similar broad reach advertising focused on extreme professional
    use that would be used to draw in amateurs and consumers!\"><em>While Hasselblad
    went to the moon, Nikon was in space from the late 1960s through a close design
    partnership with NASA that peaked with massive ad campaigns in the 80s. Note the
    similar broad reach advertising focused on extreme professional use that would
    be used to draw in amateurs and consumers!</em></p><p>This kills me because, well,
    I have been using Nikon since my father gave me his 1971 F and I collect them.
    But I also think about this evolution and how technology incumbents can succumb
    to success. // end</p><p>PS/ Nikon did not invent the 35mm SLR! In fact it got
    into the business by making lenses for existing makers while it focused on professional
    rangefinder cameras (below). *Nikon’s original corporate name was Nippon Kogaku,
    which translates as *Japan Optical.</p><p>That said, the 1959 Nikon system was
    very much like the 1964 IBM 360 in scope, reuse, industry impact.</p><p><img src=\"https://cdn-images-1.medium.com/max/2398/0*llX-8Re9rFHzGLuC.jpg\"
    alt=\"This is an early Nikon F system.\"><em>This is an early Nikon F system.</em></p><p><img
    src=\"https://cdn-images-1.medium.com/max/2400/0*xucS9YafSrwZrMmJ.jpg\" alt=\"This
    is IBM’s view of the 701 computer featuring all the peripherals. IBM and Nikon
    both designed systems from the ground up by bringing together components previously
    invented and combining them into a complete platform/system that revolutionized
    the field. Sensing a pattern?\"><em>This is IBM’s view of the 701 computer featuring
    all the peripherals. IBM and Nikon both designed systems from the ground up by
    bringing together components previously invented and combining them into a complete
    platform/system that revolutionized the field. Sensing a pattern?</em></p><p>PPS/
    In 1971 Canon tried to compete head-on w/Nikon, even with some parity, it offered
    nothing substantial that Nikon didn’t offer and had a much smaller “system” and
    ecosystem. Competing head-on rarely works. Reviews at the time “tried” to make
    it a race, but was’t even close.</p><p><img src=\"https://cdn-images-1.medium.com/max/2000/0*1E62iaAT3zbhoX0b.jpg\"
    alt=\"Nikon got its start designing lenses for the leading rangefinder (versus
    single lens reflex) cameras of the time, Leica. Nikon began to develop its own
    cameras earlier but the reputation came from optics. Nikon’ corporate name in
    Japanese was Nippon Kogaku, which means Japan Optical. Nikon today makes leading
    microscopes and imaging tools of all kinds.\"><em>Nikon got its start designing
    lenses for the leading rangefinder (versus single lens reflex) cameras of the
    time, Leica. Nikon began to develop its own cameras earlier but the reputation
    came from optics. Nikon’ corporate name in Japanese was Nippon Kogaku, which means
    Japan Optical. Nikon today makes leading microscopes and imaging tools of all
    kinds.</em></p><p>PPPS/ Was asked…1950’s Nikon made optics for Leica (can you
    believe!) the leader in rangefinder cameras used by Pros. Nikon clearly learned
    from that and much like China mfgs w/their own consumer brands, they went “first
    party” with the F system. FYI, Sony makes Nikon sensors.\U0001F914 <em>Nikon has
    been slowly displacing Sony sensors and the latest pro and pro-sumer cameras no
    longer use Sony sensors. And fwiw, Canon got into cameras by creating cheaper
    copies of other’s rangefinder cameras and using other company’s lenses. They then
    later moved into making lenses as well. See the pattern…</em></p><p><strong><em>I
    will amend this with corrections I receive since posting anything about Nikon
    history or especially Nikon v. Canon will almost certainly get me in trouble.</em></strong></p>"
  :topic_id: 628
- :url: https://netflixtechblog.com/using-machine-learning-to-improve-streaming-quality-at-netflix-9651263ef09f?source=search_post---------7
  :title: Using Machine Learning to Improve Streaming Quality at Netflix
  :content: '<p>by Chaitanya Ekanadham</p><p>One of the common questions we get asked
    is: “Why do we need machine learning to improve streaming quality?” This is a
    really important question, especially given the recent hype around machine learning
    and AI which can lead to instances where we have a “solution in search of a problem.”
    In this blog post, we describe some of the technical challenges we face for video
    streaming at Netflix and how statistical models and machine learning techniques
    can help overcome these challenges.</p><p>Netflix streams to over 117M members
    worldwide. Well over half of those members live outside the United States, where
    there is a great opportunity to grow and bring Netflix to more consumers. Providing
    a <a href="https://medium.com/netflix-techblog/optimizing-the-netflix-streaming-experience-with-data-science-725f04c3e834">quality
    streaming experience</a> for this global audience is an immense technical challenge.
    A large portion of this is engineering effort required to <a href="https://media.netflix.com/en/company-blog/how-netflix-works-with-isps-around-the-globe-to-deliver-a-great-viewing-experience">install
    and maintain servers throughout the world</a>, as well as algorithms for streaming
    content from those servers to our subscribers’ devices. As we expand rapidly to
    audiences with <a href="https://media.netflix.com/en/company-blog/a-global-approach-to-recommendations">diverse
    viewing behavior,</a> operating on networks and devices with widely varying capabilities,
    a “one size fits all” solution for streaming video becomes increasingly suboptimal.
    For example:</p><ul><li><p>Viewing/browsing behavior on mobile devices is different
    than on Smart TVs</p></li><li><p>Cellular networks may be more volatile and unstable
    than fixed broadband networks</p></li><li><p>Networks in some markets may experience
    higher degrees of congestion</p></li><li><p>Different device groups have different
    capabilities and fidelities of internet connection due to hardware differences</p></li></ul><p>We
    need to adapt our methods for these different, often fluctuating conditions to
    provide a high-quality experience for existing members as well as to expand in
    new markets. At Netflix, we observe network and device conditions as well as aspects
    of the user experience (e.g., video quality) we were able to deliver for every
    session, allowing us to leverage statistical modeling and machine learning in
    this space. A <a href="https://medium.com/netflix-techblog/how-data-science-helps-power-worldwide-delivery-of-netflix-content-bac55800f9a7">previous
    post</a> described how data science is leveraged for distributing content on our
    servers worldwide. In this post we describe some technical challenges we face
    on the device side.</p><h2>Network quality characterization and prediction</h2><p>Network
    quality is difficult to characterize and predict. While the average bandwidth
    and round trip time supported by a network are well-known indicators of network
    quality, other characteristics such as stability and predictability make a big
    difference when it comes to video streaming. A richer characterization of network
    quality would prove useful for analyzing networks (for targeting/analyzing product
    improvements), determining initial video quality and/or adapting video quality
    throughout playback (more on that below).</p><p>Below are a few examples of network
    throughput measured during real viewing sessions. You can see they are quite noisy
    and fluctuate within a wide range. Can we predict what throughput will look like
    in the next 15 minutes given the last 15 minutes of data? How can we incorporate
    longer-term historical information about the network and device? What kind of
    data can we provide from the server that would allow the device to adapt optimally?
    Even if we cannot predict exactly when a network drop will happen (this could
    be due to all kinds of things, e.g. a microwave turning on or going through a
    tunnel while streaming from a vehicle), can we at least characterize the <em>distribution</em>
    of throughput that we expect to see given historical data?</p><p>Since we are
    observing these traces at scale, there is opportunity to bring to bear more complex
    models that combine temporal pattern recognition with various contextual indicators
    to make more accurate predictions of network quality.</p><p><img src="https://cdn-images-1.medium.com/max/3200/0*41Zg-1OWfwp87G-C."
    alt=""></p><p><img src="https://cdn-images-1.medium.com/max/3200/0*zNTp-RsuFnWZMRQi."
    alt="*Examples of network throughput traces measured from real viewing sessions.*"><strong>Examples
    of network throughput traces measured from real viewing sessions.</strong></p><p>One
    useful application of network prediction is to adapt video quality during playback,
    which we describe in the following section.</p><h2>Video quality adaptation during
    playback</h2><p>Movies and shows are often encoded at different video qualities
    to support different network and device capabilities. Adaptive streaming algorithms
    are responsible for adapting which video quality is streamed throughout playback
    based on the current network and device conditions (see <a href="http://yuba.stanford.edu/%7Ehuangty/sigc040-huang.pdf">here</a>
    for an example of our colleagues’ research in this area). The figure below illustrates
    the setup for video quality adaptation. Can we leverage data to determine the
    video quality that will optimize the quality of experience? The quality of experience
    can be measured in several ways, including the initial amount of time spent waiting
    for video to play, the overall video quality experienced by the user, the number
    of times playback paused to load more video into the buffer (“rebuffer”), and
    the amount of perceptible fluctuation in quality during playback.</p><p><img src="https://cdn-images-1.medium.com/max/2000/0*36vFSbJXoll0VTCF."
    alt="*Illustration of the video quality adaptation problem. The video is encoded
    at different qualities (in this case 3 qualities: high in green, medium in yellow,
    low in red). Each quality version of the video is divided up into chunks of a
    fixed duration (grey boxes). A decision is made about which quality to choose
    for each chunk that is downloaded.*"><strong>Illustration of the video quality
    adaptation problem. The video is encoded at different qualities (in this case
    3 qualities: high in green, medium in yellow, low in red). Each quality version
    of the video is divided up into chunks of a fixed duration (grey boxes). A decision
    is made about which quality to choose for each chunk that is downloaded.</strong></p><p>These
    metrics can trade off with one another: we can choose to be aggressive and stream
    very high-quality video but increase the risk of a rebuffer. Or we can choose
    to download more video up front and reduce the rebuffer risk at the cost of increased
    wait time. The feedback signal of a given decision is delayed and sparse. For
    example, an aggressive switch to higher quality may not have immediate repercussions,
    but could gradually deplete the buffer and eventually lead to a rebuffer event
    on some occasions. This <a href="http://www.scholarpedia.org/article/Reinforcement_learning#.28Temporal.29_Credit_Assignment_Problem">“credit
    assignment” problem</a> is a well-known challenge when learning optimal control
    algorithms, and machine learning techniques (e.g., recent advances in reinforcement
    learning) have great potential to tackle these issues.</p><h2>Predictive caching</h2><p>Another
    area in which statistical models can improve the streaming experience is by predicting
    what a user will play in order to cache (part of) it on the device before the
    user hits play, enabling the video to start faster and/or at a higher quality.
    For example, we can exploit the fact that a user who has been watching a particular
    series is very likely to play the next unwatched episode. By combining various
    aspects of their viewing history together with recent user interactions and other
    contextual variables, one can formulate this as a supervised learning problem
    where we want to maximize the model’s likelihood of caching what the user actually
    ended up playing, while respecting constraints around resource usage coming from
    the cache size and available bandwidth. We have seen substantial reductions in
    the time spent waiting for video to start when employing predictive caching models.</p><h2>Device
    anomaly detection</h2><p>Netflix operates on over <a href="https://www.linkedin.com/pulse/managing-reliability-across-thousands-netflix-ready-device-petry/?trk=pulse_spock-articles">a
    thousand different types of devices,</a> ranging from laptops to tablets to Smart
    TVs to mobile phones to streaming sticks. New devices are constantly entering
    into this ecosystem, and existing devices often undergo updates to their firmware
    or interact with changes on our Netflix application. These often go without a
    hitch but at this scale it is not uncommon to cause a problem for the user experience
    — e.g., the app will not start up properly, or playback will be inhibited or degraded
    in some way. In addition, there are gradual trends in device quality that can
    accumulate over time. For example, a chain of successive UI changes may slowly
    degrade performance on a particular device such that it was not immediately noticeable
    after any individual change.</p><p>Detecting these changes is a challenging and
    manually intensive process. Alerting frameworks are a useful tool for surfacing
    potential issues but oftentimes it is tricky to determine the right criteria for
    labeling something as an actual problem. A “liberal” trigger will end up with
    too many false positives, resulting in a large amount of unnecessary manual investigation
    by our device reliability team, whereas a very strict trigger may miss out on
    the real problems. Fortunately, we have history on alerts that were triggered
    as well as the ultimate determination (made by a human) of whether or not the
    issue was in fact real and actionable. We can then use this to train a model that
    can predict the likelihood that a given set of measured conditions constitutes
    a real problem.</p><p>Even when we’re confident we’re observing a problematic
    issue, it is often challenging to determine the root cause. Was it due to a fluctuation
    in network quality on a particular ISP or in a particular region? An internal
    A/B experiment or change that was rolled out? A firmware update issued by the
    device manufacturer? Is the change localized to a particular device group or specific
    models within a group? Statistical modeling can also help us determine root cause
    by controlling for various covariates.</p><p>By employing predictive modeling
    to prioritize device reliability issues, we’ve already seen large reductions in
    overall alert volume while maintaining an acceptably low false negative rate,
    which we expect to drive substantial efficiency gains for Netflix’s device reliability
    team.</p><p>The aforementioned problems are a sampling of the technical challenges
    where we believe statistical modeling and machine learning methods can improve
    the state of the art:</p><ul><li><p>there is sufficient data (over 117M members
    worldwide)</p></li><li><p>the data is high-dimensional and it is difficult to
    hand-craft the minimal set of informative variables for a particular problem</p></li><li><p>there
    is rich structure inherent in the data due to complex underlying phenomena (e.g.,
    collective network usage, human preferences, device hardware capabilities)</p></li></ul><p>Solving
    these problems is central to Netflix’s strategy as we stream video under increasingly
    diverse network and device conditions. If these problems excite you and/or you’re
    interested in bringing machine learning to this exciting new space, please <a
    href="https://www.linkedin.com/in/chaitanya-ekanadham-84622736/">contact me</a>
    or check out these <a href="https://jobs.netflix.com/search?q=%22Senior%20Machine%20Learning%20Scientist%20-%20Video%20Streaming%22%2C%20%22Device%20Performance%22%2C%20%22Streaming%20Security%22&location=Los%20Gatos%2C%20California&organization=Data%2C%20Analytics%2C%20and%20Algorithms">science
    and analytics</a> or <a href="https://jobs.netflix.com/search?q=%22Senior%20Software%20Engineer%2C%20adaptive%20streaming%22&location=Los%20Gatos%2C%20California&organization=Engineering">software
    engineering</a> postings!</p>'
  :author: '<p>by Chaitanya Ekanadham</p><p>One of the common questions we get asked
    is: “Why do we need machine learning to improve streaming quality?” This is a
    really important question, especially given the recent hype around machine learning
    and AI which can lead to instances where we have a “solution in search of a problem.”
    In this blog post, we describe some of the technical challenges we face for video
    streaming at Netflix and how statistical models and machine learning techniques
    can help overcome these challenges.</p><p>Netflix streams to over 117M members
    worldwide. Well over half of those members live outside the United States, where
    there is a great opportunity to grow and bring Netflix to more consumers. Providing
    a <a href="https://medium.com/netflix-techblog/optimizing-the-netflix-streaming-experience-with-data-science-725f04c3e834">quality
    streaming experience</a> for this global audience is an immense technical challenge.
    A large portion of this is engineering effort required to <a href="https://media.netflix.com/en/company-blog/how-netflix-works-with-isps-around-the-globe-to-deliver-a-great-viewing-experience">install
    and maintain servers throughout the world</a>, as well as algorithms for streaming
    content from those servers to our subscribers’ devices. As we expand rapidly to
    audiences with <a href="https://media.netflix.com/en/company-blog/a-global-approach-to-recommendations">diverse
    viewing behavior,</a> operating on networks and devices with widely varying capabilities,
    a “one size fits all” solution for streaming video becomes increasingly suboptimal.
    For example:</p><ul><li><p>Viewing/browsing behavior on mobile devices is different
    than on Smart TVs</p></li><li><p>Cellular networks may be more volatile and unstable
    than fixed broadband networks</p></li><li><p>Networks in some markets may experience
    higher degrees of congestion</p></li><li><p>Different device groups have different
    capabilities and fidelities of internet connection due to hardware differences</p></li></ul><p>We
    need to adapt our methods for these different, often fluctuating conditions to
    provide a high-quality experience for existing members as well as to expand in
    new markets. At Netflix, we observe network and device conditions as well as aspects
    of the user experience (e.g., video quality) we were able to deliver for every
    session, allowing us to leverage statistical modeling and machine learning in
    this space. A <a href="https://medium.com/netflix-techblog/how-data-science-helps-power-worldwide-delivery-of-netflix-content-bac55800f9a7">previous
    post</a> described how data science is leveraged for distributing content on our
    servers worldwide. In this post we describe some technical challenges we face
    on the device side.</p><h2>Network quality characterization and prediction</h2><p>Network
    quality is difficult to characterize and predict. While the average bandwidth
    and round trip time supported by a network are well-known indicators of network
    quality, other characteristics such as stability and predictability make a big
    difference when it comes to video streaming. A richer characterization of network
    quality would prove useful for analyzing networks (for targeting/analyzing product
    improvements), determining initial video quality and/or adapting video quality
    throughout playback (more on that below).</p><p>Below are a few examples of network
    throughput measured during real viewing sessions. You can see they are quite noisy
    and fluctuate within a wide range. Can we predict what throughput will look like
    in the next 15 minutes given the last 15 minutes of data? How can we incorporate
    longer-term historical information about the network and device? What kind of
    data can we provide from the server that would allow the device to adapt optimally?
    Even if we cannot predict exactly when a network drop will happen (this could
    be due to all kinds of things, e.g. a microwave turning on or going through a
    tunnel while streaming from a vehicle), can we at least characterize the <em>distribution</em>
    of throughput that we expect to see given historical data?</p><p>Since we are
    observing these traces at scale, there is opportunity to bring to bear more complex
    models that combine temporal pattern recognition with various contextual indicators
    to make more accurate predictions of network quality.</p><p><img src="https://cdn-images-1.medium.com/max/3200/0*41Zg-1OWfwp87G-C."
    alt=""></p><p><img src="https://cdn-images-1.medium.com/max/3200/0*zNTp-RsuFnWZMRQi."
    alt="*Examples of network throughput traces measured from real viewing sessions.*"><strong>Examples
    of network throughput traces measured from real viewing sessions.</strong></p><p>One
    useful application of network prediction is to adapt video quality during playback,
    which we describe in the following section.</p><h2>Video quality adaptation during
    playback</h2><p>Movies and shows are often encoded at different video qualities
    to support different network and device capabilities. Adaptive streaming algorithms
    are responsible for adapting which video quality is streamed throughout playback
    based on the current network and device conditions (see <a href="http://yuba.stanford.edu/%7Ehuangty/sigc040-huang.pdf">here</a>
    for an example of our colleagues’ research in this area). The figure below illustrates
    the setup for video quality adaptation. Can we leverage data to determine the
    video quality that will optimize the quality of experience? The quality of experience
    can be measured in several ways, including the initial amount of time spent waiting
    for video to play, the overall video quality experienced by the user, the number
    of times playback paused to load more video into the buffer (“rebuffer”), and
    the amount of perceptible fluctuation in quality during playback.</p><p><img src="https://cdn-images-1.medium.com/max/2000/0*36vFSbJXoll0VTCF."
    alt="*Illustration of the video quality adaptation problem. The video is encoded
    at different qualities (in this case 3 qualities: high in green, medium in yellow,
    low in red). Each quality version of the video is divided up into chunks of a
    fixed duration (grey boxes). A decision is made about which quality to choose
    for each chunk that is downloaded.*"><strong>Illustration of the video quality
    adaptation problem. The video is encoded at different qualities (in this case
    3 qualities: high in green, medium in yellow, low in red). Each quality version
    of the video is divided up into chunks of a fixed duration (grey boxes). A decision
    is made about which quality to choose for each chunk that is downloaded.</strong></p><p>These
    metrics can trade off with one another: we can choose to be aggressive and stream
    very high-quality video but increase the risk of a rebuffer. Or we can choose
    to download more video up front and reduce the rebuffer risk at the cost of increased
    wait time. The feedback signal of a given decision is delayed and sparse. For
    example, an aggressive switch to higher quality may not have immediate repercussions,
    but could gradually deplete the buffer and eventually lead to a rebuffer event
    on some occasions. This <a href="http://www.scholarpedia.org/article/Reinforcement_learning#.28Temporal.29_Credit_Assignment_Problem">“credit
    assignment” problem</a> is a well-known challenge when learning optimal control
    algorithms, and machine learning techniques (e.g., recent advances in reinforcement
    learning) have great potential to tackle these issues.</p><h2>Predictive caching</h2><p>Another
    area in which statistical models can improve the streaming experience is by predicting
    what a user will play in order to cache (part of) it on the device before the
    user hits play, enabling the video to start faster and/or at a higher quality.
    For example, we can exploit the fact that a user who has been watching a particular
    series is very likely to play the next unwatched episode. By combining various
    aspects of their viewing history together with recent user interactions and other
    contextual variables, one can formulate this as a supervised learning problem
    where we want to maximize the model’s likelihood of caching what the user actually
    ended up playing, while respecting constraints around resource usage coming from
    the cache size and available bandwidth. We have seen substantial reductions in
    the time spent waiting for video to start when employing predictive caching models.</p><h2>Device
    anomaly detection</h2><p>Netflix operates on over <a href="https://www.linkedin.com/pulse/managing-reliability-across-thousands-netflix-ready-device-petry/?trk=pulse_spock-articles">a
    thousand different types of devices,</a> ranging from laptops to tablets to Smart
    TVs to mobile phones to streaming sticks. New devices are constantly entering
    into this ecosystem, and existing devices often undergo updates to their firmware
    or interact with changes on our Netflix application. These often go without a
    hitch but at this scale it is not uncommon to cause a problem for the user experience
    — e.g., the app will not start up properly, or playback will be inhibited or degraded
    in some way. In addition, there are gradual trends in device quality that can
    accumulate over time. For example, a chain of successive UI changes may slowly
    degrade performance on a particular device such that it was not immediately noticeable
    after any individual change.</p><p>Detecting these changes is a challenging and
    manually intensive process. Alerting frameworks are a useful tool for surfacing
    potential issues but oftentimes it is tricky to determine the right criteria for
    labeling something as an actual problem. A “liberal” trigger will end up with
    too many false positives, resulting in a large amount of unnecessary manual investigation
    by our device reliability team, whereas a very strict trigger may miss out on
    the real problems. Fortunately, we have history on alerts that were triggered
    as well as the ultimate determination (made by a human) of whether or not the
    issue was in fact real and actionable. We can then use this to train a model that
    can predict the likelihood that a given set of measured conditions constitutes
    a real problem.</p><p>Even when we’re confident we’re observing a problematic
    issue, it is often challenging to determine the root cause. Was it due to a fluctuation
    in network quality on a particular ISP or in a particular region? An internal
    A/B experiment or change that was rolled out? A firmware update issued by the
    device manufacturer? Is the change localized to a particular device group or specific
    models within a group? Statistical modeling can also help us determine root cause
    by controlling for various covariates.</p><p>By employing predictive modeling
    to prioritize device reliability issues, we’ve already seen large reductions in
    overall alert volume while maintaining an acceptably low false negative rate,
    which we expect to drive substantial efficiency gains for Netflix’s device reliability
    team.</p><p>The aforementioned problems are a sampling of the technical challenges
    where we believe statistical modeling and machine learning methods can improve
    the state of the art:</p><ul><li><p>there is sufficient data (over 117M members
    worldwide)</p></li><li><p>the data is high-dimensional and it is difficult to
    hand-craft the minimal set of informative variables for a particular problem</p></li><li><p>there
    is rich structure inherent in the data due to complex underlying phenomena (e.g.,
    collective network usage, human preferences, device hardware capabilities)</p></li></ul><p>Solving
    these problems is central to Netflix’s strategy as we stream video under increasingly
    diverse network and device conditions. If these problems excite you and/or you’re
    interested in bringing machine learning to this exciting new space, please <a
    href="https://www.linkedin.com/in/chaitanya-ekanadham-84622736/">contact me</a>
    or check out these <a href="https://jobs.netflix.com/search?q=%22Senior%20Machine%20Learning%20Scientist%20-%20Video%20Streaming%22%2C%20%22Device%20Performance%22%2C%20%22Streaming%20Security%22&location=Los%20Gatos%2C%20California&organization=Data%2C%20Analytics%2C%20and%20Algorithms">science
    and analytics</a> or <a href="https://jobs.netflix.com/search?q=%22Senior%20Software%20Engineer%2C%20adaptive%20streaming%22&location=Los%20Gatos%2C%20California&organization=Engineering">software
    engineering</a> postings!</p>'
  :topic_id: 628
- :url: https://netflixtechblog.com/fast-json-api-serialization-with-ruby-on-rails-7c06578ad17f?source=search_post---------8
  :title: Fast JSON API serialization with Ruby on Rails
  :content: '<p>by Shishir Kakaraddi, Srinivas Raghunathan, Adam Gross and Ryan Johnston</p><p>We
    are pleased to announce the open source release of the <a href="https://github.com/Netflix/fast_jsonapi">Fast
    JSON API</a> gem geared towards Ruby on Rails applications.</p><p><strong>Introduction</strong></p><p>Fast
    JSONAPI is aimed at providing all the major functionality that Active Model Serializer
    (AMS) provides, along with an emphasis on speed and performance by meeting a benchmark
    requirement of being 25 times faster than AMS. The gem also enforces<a href="https://github.com/Netflix/fast_jsonapi/blob/master/spec/lib/object_serializer_performance_spec.rb">
    performance testing</a> as a discipline.</p><p>AMS is a great gem and<a href="https://github.com/Netflix/fast_jsonapi">
    fast_jsonapi</a> was inspired by it when it comes to declaration syntax and features.
    AMS begins to slow down, however, when a model has one or more relationships.
    Compound documents, AKA sideloading, on those models makes AMS slow down further.
    Throw in a need for infinite scroll on the UI, and AMS’s slowness starts becoming
    visible to users.</p><p><strong>Why optimize serialization?</strong></p><p>JSON
    API serialization is often one of the slowest parts of many well implemented Rails
    APIs. Why not provide all the major functionality that AMS provides with greater
    speed?</p><p><strong>Features:</strong></p><ul><li><p>Declaration syntax similar
    to Active Model Serializer</p></li><li><p>Support for belongs<em>to, has</em>many
    and has_one</p></li><li><p>Support for compound documents (included)</p></li><li><p>Optimized
    serialization of compound documents</p></li><li><p>Caching</p></li><li><p>Instrumentation
    with Skylight integration (optional)</p></li></ul><p><strong>How do you write
    a serializer using Fast JSONAPI?</strong></p><p>We like the familiar way Active
    Model Serializers lets us declare our serializers. Declaration syntax of fast_jsonapi
    is similar to AMS.</p><pre><code>class MovieSerializer include FastJsonapi::ObjectSerializer
    attributes :name, :year has_many :actors belongs_to :owner, record_type: :user
    belongs_to :movie_typeend</code></pre><p><strong>How fast is it compared to Active
    Model Serializers?</strong></p><p>Performance tests indicate a 25–40x speed gain
    over AMS, essentially making serialization time negligible on even fairly complex
    models. Performance gain is significant when the number of serialized records
    increases.</p><p><img src="https://cdn-images-1.medium.com/max/2532/1*3ZLHzQ0cSJxwa3gyviq53A.png"
    alt="Difference in performance"><em>Difference in performance</em></p><p><img
    src="https://cdn-images-1.medium.com/max/2000/1*hqNFZ8-oqseea0kidZWyhg.png" alt=""></p><p>Don’t
    believe us? You can run the benchmark tests for yourself. Refer to<a href="https://github.com/Netflix/fast_jsonapi/blob/master/README.md">
    readme</a>.</p><p><strong>Dependency</strong></p><p>JSON API is the anti-bikeshedding
    tool.</p><p><strong>Future Work</strong></p><p>We plan to add more features to
    the gem. We welcome suggestions, improvements, corrections and additional tests.</p>'
  :author: '<p>by Shishir Kakaraddi, Srinivas Raghunathan, Adam Gross and Ryan Johnston</p><p>We
    are pleased to announce the open source release of the <a href="https://github.com/Netflix/fast_jsonapi">Fast
    JSON API</a> gem geared towards Ruby on Rails applications.</p><p><strong>Introduction</strong></p><p>Fast
    JSONAPI is aimed at providing all the major functionality that Active Model Serializer
    (AMS) provides, along with an emphasis on speed and performance by meeting a benchmark
    requirement of being 25 times faster than AMS. The gem also enforces<a href="https://github.com/Netflix/fast_jsonapi/blob/master/spec/lib/object_serializer_performance_spec.rb">
    performance testing</a> as a discipline.</p><p>AMS is a great gem and<a href="https://github.com/Netflix/fast_jsonapi">
    fast_jsonapi</a> was inspired by it when it comes to declaration syntax and features.
    AMS begins to slow down, however, when a model has one or more relationships.
    Compound documents, AKA sideloading, on those models makes AMS slow down further.
    Throw in a need for infinite scroll on the UI, and AMS’s slowness starts becoming
    visible to users.</p><p><strong>Why optimize serialization?</strong></p><p>JSON
    API serialization is often one of the slowest parts of many well implemented Rails
    APIs. Why not provide all the major functionality that AMS provides with greater
    speed?</p><p><strong>Features:</strong></p><ul><li><p>Declaration syntax similar
    to Active Model Serializer</p></li><li><p>Support for belongs<em>to, has</em>many
    and has_one</p></li><li><p>Support for compound documents (included)</p></li><li><p>Optimized
    serialization of compound documents</p></li><li><p>Caching</p></li><li><p>Instrumentation
    with Skylight integration (optional)</p></li></ul><p><strong>How do you write
    a serializer using Fast JSONAPI?</strong></p><p>We like the familiar way Active
    Model Serializers lets us declare our serializers. Declaration syntax of fast_jsonapi
    is similar to AMS.</p><pre><code>class MovieSerializer include FastJsonapi::ObjectSerializer
    attributes :name, :year has_many :actors belongs_to :owner, record_type: :user
    belongs_to :movie_typeend</code></pre><p><strong>How fast is it compared to Active
    Model Serializers?</strong></p><p>Performance tests indicate a 25–40x speed gain
    over AMS, essentially making serialization time negligible on even fairly complex
    models. Performance gain is significant when the number of serialized records
    increases.</p><p><img src="https://cdn-images-1.medium.com/max/2532/1*3ZLHzQ0cSJxwa3gyviq53A.png"
    alt="Difference in performance"><em>Difference in performance</em></p><p><img
    src="https://cdn-images-1.medium.com/max/2000/1*hqNFZ8-oqseea0kidZWyhg.png" alt=""></p><p>Don’t
    believe us? You can run the benchmark tests for yourself. Refer to<a href="https://github.com/Netflix/fast_jsonapi/blob/master/README.md">
    readme</a>.</p><p><strong>Dependency</strong></p><p>JSON API is the anti-bikeshedding
    tool.</p><p><strong>Future Work</strong></p><p>We plan to add more features to
    the gem. We welcome suggestions, improvements, corrections and additional tests.</p>'
  :topic_id: 628
- :url: https://netflixtechblog.com/netflix-flamescope-a57ca19d47bb?source=search_post---------9
  :title: Netflix FlameScope
  :content: '<p>We’re excited to release FlameScope: a new performance visualization
    tool for analyzing variance, perturbations, single-threaded execution, application
    startup, and other time-based issues. It has been created by the Netflix cloud
    performance engineering team and just released as open source, and we welcome
    help from others to develop the project further. (If it especially interests you,
    you might be interested in joining Netflix to work on it and other projects.)</p><p>FlameScope
    combines a subsecond-offset heatmap for navigating a profile with flame graphs.
    This profile can be of CPU samples or other events. Since it’s visual, it’s best
    demonstrated by the following one minute video:</p><p><center><iframe width="560"
    height="315" src="https://www.youtube.com/embed/cFuI8SAAvJg" frameborder="0" allowfullscreen></iframe></center></p><p>There
    is also a longer video of examples <a href="https://www.youtube.com/watch?v=gRawd7CO-Q8">here</a>.</p><h2>Subsecond-Offset
    Heat Maps</h2><p>If you’re familiar with <a href="https://medium.com/netflix-techblog/java-in-flames-e763b3d32166">flame
    graphs</a>, you’ll know they show an entire profile at once, which can span one
    minute. That’s good for analyzing steady workloads, but often there are small
    perturbations or variation during that minute that you want to know about, which
    become a needle-in-a-haystack search when shown with the full profile. FlameScope
    solves this by starting with a subsecond-offset heat map to visualize these perturbations,
    then lets you select them for study with a flame graph. In other words, you can
    select an arbitrary continuous time-slice of the captured profile, and visualize
    it as a flame graph.</p><p><img src="https://cdn-images-1.medium.com/max/2780/1*v9ooXXqmObcOOWYo87NxjQ.png"
    alt=""></p><p>You might not be familiar with subsecond-offset heat maps. They
    work as shown in figure 1, which has a mock ten row heat map, where:</p><ul><li><p><strong>x-axis</strong>:
    the passage of time, where each column represents one second</p></li><li><p><strong>y-axis</strong>:
    this is also time, showing the fraction within the second: its subsecond offset</p></li><li><p><strong>color</strong>:
    shows how many events or samples that fell in that time range: darker for more</p></li></ul><p>Imagine
    you have an event timestamp of 11.25 seconds. The x coordinate will be the 11th
    column, and the y coordinate will be the row that’s one quarter from the bottom.
    The more events that occurred around 11.25 seconds, the darker that block will
    be drawn.</p><h2>Example</h2><p>Here’s an example, with annotations showing the
    steps for selecting a range:</p><p><img src="https://cdn-images-1.medium.com/max/2394/0*cs-q2P3dFtFgcyS9."
    alt="Figure 2. FlameScope selecting a time range"><em>Figure 2. FlameScope selecting
    a time range</em></p><p>There’s a number of interesting things from this production
    CPU profile. The CPUs are busier between 0 and 5 seconds, shown as darker colors.
    Around the 34 and 94 second mark (sounds like a 60 second periodic task), the
    CPUs also become busier, but for a shorter duration. And there are occasional
    bursts of heavy CPU activity for about 80 milliseconds, shown as short dark red
    stripes.</p><p>All of these details can be selected in FlameScope, which will
    then draw a flame graph just for that range. Here’s one of the short red stripes:</p><p><img
    src="https://cdn-images-1.medium.com/max/3200/0*9i8D14au3K88Luid." alt="Figure
    3. Flame graph for a selected time range"><em>Figure 3. Flame graph for a selected
    time range</em></p><p>Ah, that’s Java garbage collection.</p><h2>Instructions</h2><p>Getting
    started instructions are listed (and will be updated) on the github repository
    <a href="https://github.com/Netflix/flamescope#installation--instructions">here</a>.
    The quickest way to get started is:</p><pre><code>$ git clone [https://github.com/Netflix/flamescope](https://github.com/Netflix/flamescope)$
    cd flamescope$ pip install -r requirements.txt$ python run.py</code></pre><p>FlameScope
    comes with a sample profile to browse (where application code has been redacted
    with ‘x’ characters). Here’s how to create new profiles on Linux, which can be
    added to the examples directory of FlameScope for browsing:</p><pre><code>$ sudo
    perf record -F 49 -a -g -- sleep 120$ sudo perf script --header &gt; stacks.myproductionapp.2018_03_30_01$
    gzip stacks.myproductionapp.2018_03_30_01    # optional</code></pre><p>That example
    shows a two minute CPU profile, sampling at 49 Hertz on all CPUs. Any perf output
    with stack traces can be browsed with FlameScope, including tracing events such
    as block I/O, context switches, page faults, etc. Since the profile output can
    get large, it can also be compressed with gzip (flamescope can read .gz).</p><p>Why
    sample at 49 Hertz? Because 50 Hertz may sample in lock-step with a timed activity,
    and over- or under-count. Why roughly 50 Hertz in the first place? It’s not too
    slow and not too fast. Too slow and we don’t have enough samples to paint across
    FlameScope’s 50 row heatmap (the row count can be changed). Too fast and the overhead
    of sampling can slow down the application.</p><p>Runtimes like Java can require
    extra steps to profile using perf correctly, which have been documented in the
    past for generating flame graphs (including <a href="https://medium.com/netflix-techblog/java-in-flames-e763b3d32166">here</a>).
    Since you may have already been running these steps, you might have a library
    of old profiles (perf script output) that you can now explore using FlameScope.</p><h2>Screenshots</h2><p>Since
    FlameScope reads Linux perf profiles, I already have a collection from prior investigations.
    Here are some screenshots, showing variation that I did not know about at the
    time.</p><p><img src="https://cdn-images-1.medium.com/max/2464/1*MwNHcOCAELCyizqwTDpcDQ.png"
    alt=""></p><p><img src="https://cdn-images-1.medium.com/max/2460/1*hxEeLyGdjk6ymlNZaOKuQA.png"
    alt=""></p><p><img src="https://cdn-images-1.medium.com/max/2452/1*jkSquR5B3YOTHn3jrYLoiw.png"
    alt=""></p><p><img src="https://cdn-images-1.medium.com/max/4516/1*EypUnkPtayeKr9bJuvgNEg.png"
    alt=""></p><p><img src="https://cdn-images-1.medium.com/max/2452/1*ia0FTPByjAPtshcdDxOPMQ.png"
    alt=""></p><p><img src="https://cdn-images-1.medium.com/max/2460/1*-jj6gnWNqnj7bzvHjqbjSw.png"
    alt=""></p><p><img src="https://cdn-images-1.medium.com/max/4512/1*sAVyFVpC8mjgD_8WQbYYGQ.png"
    alt=""></p><p>Not all profiles are this interesting. Some do just look like TV
    static: a steady workload of random request arrivals and consistent latency. You
    can find out with FlameScope.</p><h2>Origin</h2><p>FlameScope was created by the
    Netflix cloud performance team, so far involving Vadim Filanovsky, myself, Martin
    Spier, and our manager, Ed Hunter, who has supported the project. The original
    issue was a microservice that was suffering latency spikes every 15 minutes or
    so, cause unknown. Vadim found it corresponded to an increase in CPU utilization
    that lasted only a few seconds. He had tried collecting CPU flame graphs to explain
    this further, but a) could not reliably capture a one minute flame graph covering
    the issue, as the onset of it kept fluctuating; and b) capturing a two or three
    minute flamegraph didn’t help either, and the issue was “drowned” in the normal
    workload profile. Vadim asked me for help.</p><p>Since I had a two minute profile
    to begin with, I began by slicing it into ten second ranges, and creating a flame
    graph for each. This approach looked promising as it revealed variation, so I
    sliced it even further down to one second windows. Browsing these short windows
    solved the problem and found the issue, however, it had become a laborious task.
    I wanted a quicker way.</p><p>Subsecond-offset heat maps were something I invented
    many years ago, but they haven’t seen much adoption or use so far. I realized
    they would be a great way to navigate a profile, allowing variance to be visualized
    not just for whole seconds but also for fractions within a second. I did a quick
    prototype which proved the idea worked, and discussed turning it into a real tool
    with Martin. The annotated heat map in this post shows Vadim’s original profile,
    and the issue was the CPU activity in the first few seconds.</p><p>Martin has
    done most of the architecture design and coding for FlameScope, which includes
    his newer d3-based version of FlameGraphs: <a href="https://github.com/spiermar/d3-flame-graph">d3-flame-graph</a>,
    and his d3-based heat maps: <a href="https://github.com/spiermar/d3-heatmap2">d3-heatmap2</a>.
    It’s great to see them come together in FlameScope.</p><h2>Future Work</h2><p>There’s
    more features we have planned, and we’ll add tickets to github in case others
    would like to help. They include more interactive features, such as palette selection
    and data transformations. There should be a button to save the final flame graph
    as a stand alone SVG. Other profile sources can be supported, not just Linux perf.
    And there should be a way to show the difference between the selected range and
    the baseline (the whole profile).</p><p>If you’re reading this months in the future,
    some of these extra features may already exist: check out the latest on <a href="https://github.com/Netflix/flamescope">https://github.com/Netflix/flamescope</a>.</p><p><em>FlameScope
    was developed by Martin Spier and Brendan Gregg, Netflix cloud performance engineering
    team. Blog post by Brendan Gregg.</em></p>'
  :author: '<p>We’re excited to release FlameScope: a new performance visualization
    tool for analyzing variance, perturbations, single-threaded execution, application
    startup, and other time-based issues. It has been created by the Netflix cloud
    performance engineering team and just released as open source, and we welcome
    help from others to develop the project further. (If it especially interests you,
    you might be interested in joining Netflix to work on it and other projects.)</p><p>FlameScope
    combines a subsecond-offset heatmap for navigating a profile with flame graphs.
    This profile can be of CPU samples or other events. Since it’s visual, it’s best
    demonstrated by the following one minute video:</p><p><center><iframe width="560"
    height="315" src="https://www.youtube.com/embed/cFuI8SAAvJg" frameborder="0" allowfullscreen></iframe></center></p><p>There
    is also a longer video of examples <a href="https://www.youtube.com/watch?v=gRawd7CO-Q8">here</a>.</p><h2>Subsecond-Offset
    Heat Maps</h2><p>If you’re familiar with <a href="https://medium.com/netflix-techblog/java-in-flames-e763b3d32166">flame
    graphs</a>, you’ll know they show an entire profile at once, which can span one
    minute. That’s good for analyzing steady workloads, but often there are small
    perturbations or variation during that minute that you want to know about, which
    become a needle-in-a-haystack search when shown with the full profile. FlameScope
    solves this by starting with a subsecond-offset heat map to visualize these perturbations,
    then lets you select them for study with a flame graph. In other words, you can
    select an arbitrary continuous time-slice of the captured profile, and visualize
    it as a flame graph.</p><p><img src="https://cdn-images-1.medium.com/max/2780/1*v9ooXXqmObcOOWYo87NxjQ.png"
    alt=""></p><p>You might not be familiar with subsecond-offset heat maps. They
    work as shown in figure 1, which has a mock ten row heat map, where:</p><ul><li><p><strong>x-axis</strong>:
    the passage of time, where each column represents one second</p></li><li><p><strong>y-axis</strong>:
    this is also time, showing the fraction within the second: its subsecond offset</p></li><li><p><strong>color</strong>:
    shows how many events or samples that fell in that time range: darker for more</p></li></ul><p>Imagine
    you have an event timestamp of 11.25 seconds. The x coordinate will be the 11th
    column, and the y coordinate will be the row that’s one quarter from the bottom.
    The more events that occurred around 11.25 seconds, the darker that block will
    be drawn.</p><h2>Example</h2><p>Here’s an example, with annotations showing the
    steps for selecting a range:</p><p><img src="https://cdn-images-1.medium.com/max/2394/0*cs-q2P3dFtFgcyS9."
    alt="Figure 2. FlameScope selecting a time range"><em>Figure 2. FlameScope selecting
    a time range</em></p><p>There’s a number of interesting things from this production
    CPU profile. The CPUs are busier between 0 and 5 seconds, shown as darker colors.
    Around the 34 and 94 second mark (sounds like a 60 second periodic task), the
    CPUs also become busier, but for a shorter duration. And there are occasional
    bursts of heavy CPU activity for about 80 milliseconds, shown as short dark red
    stripes.</p><p>All of these details can be selected in FlameScope, which will
    then draw a flame graph just for that range. Here’s one of the short red stripes:</p><p><img
    src="https://cdn-images-1.medium.com/max/3200/0*9i8D14au3K88Luid." alt="Figure
    3. Flame graph for a selected time range"><em>Figure 3. Flame graph for a selected
    time range</em></p><p>Ah, that’s Java garbage collection.</p><h2>Instructions</h2><p>Getting
    started instructions are listed (and will be updated) on the github repository
    <a href="https://github.com/Netflix/flamescope#installation--instructions">here</a>.
    The quickest way to get started is:</p><pre><code>$ git clone [https://github.com/Netflix/flamescope](https://github.com/Netflix/flamescope)$
    cd flamescope$ pip install -r requirements.txt$ python run.py</code></pre><p>FlameScope
    comes with a sample profile to browse (where application code has been redacted
    with ‘x’ characters). Here’s how to create new profiles on Linux, which can be
    added to the examples directory of FlameScope for browsing:</p><pre><code>$ sudo
    perf record -F 49 -a -g -- sleep 120$ sudo perf script --header &gt; stacks.myproductionapp.2018_03_30_01$
    gzip stacks.myproductionapp.2018_03_30_01    # optional</code></pre><p>That example
    shows a two minute CPU profile, sampling at 49 Hertz on all CPUs. Any perf output
    with stack traces can be browsed with FlameScope, including tracing events such
    as block I/O, context switches, page faults, etc. Since the profile output can
    get large, it can also be compressed with gzip (flamescope can read .gz).</p><p>Why
    sample at 49 Hertz? Because 50 Hertz may sample in lock-step with a timed activity,
    and over- or under-count. Why roughly 50 Hertz in the first place? It’s not too
    slow and not too fast. Too slow and we don’t have enough samples to paint across
    FlameScope’s 50 row heatmap (the row count can be changed). Too fast and the overhead
    of sampling can slow down the application.</p><p>Runtimes like Java can require
    extra steps to profile using perf correctly, which have been documented in the
    past for generating flame graphs (including <a href="https://medium.com/netflix-techblog/java-in-flames-e763b3d32166">here</a>).
    Since you may have already been running these steps, you might have a library
    of old profiles (perf script output) that you can now explore using FlameScope.</p><h2>Screenshots</h2><p>Since
    FlameScope reads Linux perf profiles, I already have a collection from prior investigations.
    Here are some screenshots, showing variation that I did not know about at the
    time.</p><p><img src="https://cdn-images-1.medium.com/max/2464/1*MwNHcOCAELCyizqwTDpcDQ.png"
    alt=""></p><p><img src="https://cdn-images-1.medium.com/max/2460/1*hxEeLyGdjk6ymlNZaOKuQA.png"
    alt=""></p><p><img src="https://cdn-images-1.medium.com/max/2452/1*jkSquR5B3YOTHn3jrYLoiw.png"
    alt=""></p><p><img src="https://cdn-images-1.medium.com/max/4516/1*EypUnkPtayeKr9bJuvgNEg.png"
    alt=""></p><p><img src="https://cdn-images-1.medium.com/max/2452/1*ia0FTPByjAPtshcdDxOPMQ.png"
    alt=""></p><p><img src="https://cdn-images-1.medium.com/max/2460/1*-jj6gnWNqnj7bzvHjqbjSw.png"
    alt=""></p><p><img src="https://cdn-images-1.medium.com/max/4512/1*sAVyFVpC8mjgD_8WQbYYGQ.png"
    alt=""></p><p>Not all profiles are this interesting. Some do just look like TV
    static: a steady workload of random request arrivals and consistent latency. You
    can find out with FlameScope.</p><h2>Origin</h2><p>FlameScope was created by the
    Netflix cloud performance team, so far involving Vadim Filanovsky, myself, Martin
    Spier, and our manager, Ed Hunter, who has supported the project. The original
    issue was a microservice that was suffering latency spikes every 15 minutes or
    so, cause unknown. Vadim found it corresponded to an increase in CPU utilization
    that lasted only a few seconds. He had tried collecting CPU flame graphs to explain
    this further, but a) could not reliably capture a one minute flame graph covering
    the issue, as the onset of it kept fluctuating; and b) capturing a two or three
    minute flamegraph didn’t help either, and the issue was “drowned” in the normal
    workload profile. Vadim asked me for help.</p><p>Since I had a two minute profile
    to begin with, I began by slicing it into ten second ranges, and creating a flame
    graph for each. This approach looked promising as it revealed variation, so I
    sliced it even further down to one second windows. Browsing these short windows
    solved the problem and found the issue, however, it had become a laborious task.
    I wanted a quicker way.</p><p>Subsecond-offset heat maps were something I invented
    many years ago, but they haven’t seen much adoption or use so far. I realized
    they would be a great way to navigate a profile, allowing variance to be visualized
    not just for whole seconds but also for fractions within a second. I did a quick
    prototype which proved the idea worked, and discussed turning it into a real tool
    with Martin. The annotated heat map in this post shows Vadim’s original profile,
    and the issue was the CPU activity in the first few seconds.</p><p>Martin has
    done most of the architecture design and coding for FlameScope, which includes
    his newer d3-based version of FlameGraphs: <a href="https://github.com/spiermar/d3-flame-graph">d3-flame-graph</a>,
    and his d3-based heat maps: <a href="https://github.com/spiermar/d3-heatmap2">d3-heatmap2</a>.
    It’s great to see them come together in FlameScope.</p><h2>Future Work</h2><p>There’s
    more features we have planned, and we’ll add tickets to github in case others
    would like to help. They include more interactive features, such as palette selection
    and data transformations. There should be a button to save the final flame graph
    as a stand alone SVG. Other profile sources can be supported, not just Linux perf.
    And there should be a way to show the difference between the selected range and
    the baseline (the whole profile).</p><p>If you’re reading this months in the future,
    some of these extra features may already exist: check out the latest on <a href="https://github.com/Netflix/flamescope">https://github.com/Netflix/flamescope</a>.</p><p><em>FlameScope
    was developed by Martin Spier and Brendan Gregg, Netflix cloud performance engineering
    team. Blog post by Brendan Gregg.</em></p>'
  :topic_id: 628
